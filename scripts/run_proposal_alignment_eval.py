#!/usr/bin/env python3
"""Run proposal-aligned NPC dialogue evaluation and publish reproducible artifacts."""

from __future__ import annotations

import argparse
import csv
import hashlib
import json
import math
import os
import platform
import random
import re
import statistics
import subprocess
import sys
from dataclasses import dataclass
from datetime import datetime, timezone
from itertools import combinations
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple

import requests

ROOT_DIR = Path(__file__).resolve().parents[1]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from core.response_controller import ControlConfig, control_response, sanitize_response

TOKEN_RE = re.compile(r"[a-z0-9']+")
SENTENCE_SPLIT_RE = re.compile(r"(?<=[.!?])\s+")
MODEL_ID_RE = re.compile(r"[^a-z0-9]+")

BASE_METRICS: Tuple[str, ...] = (
    "context_relevance",
    "persona_consistency",
    "naturalness",
)
DIAGNOSTIC_METRICS: Tuple[str, ...] = (
    "context_keyword_coverage",
    "context_overlap",
    "persona_keyword_coverage",
    "persona_style",
    "distinct1",
    "length_score",
    "sentence_score",
)


@dataclass(frozen=True)
class EvaluationArm:
    arm_id: str
    model: str
    include_dynamic_context: bool
    use_response_control: bool = False


def parse_list_arg(raw: str) -> List[str]:
    items = [x.strip() for x in re.split(r"[,\n;]+", raw) if x.strip()]
    out: List[str] = []
    seen = set()
    for item in items:
        lowered = item.lower()
        if lowered in seen:
            continue
        seen.add(lowered)
        out.append(item)
    return out


def sanitize_model_id(model: str) -> str:
    lowered = model.lower().strip()
    lowered = MODEL_ID_RE.sub("_", lowered).strip("_")
    if not lowered:
        return "model"
    return lowered


def metric_names(include_bertscore: bool) -> List[str]:
    names = list(BASE_METRICS) + list(DIAGNOSTIC_METRICS)
    if include_bertscore:
        names.append("bertscore_f1")
    names.append("overall_quality")
    return names


def quality_weights(has_bertscore: bool) -> Dict[str, float]:
    if has_bertscore:
        return {
            "context_relevance": 0.30,
            "persona_consistency": 0.27,
            "naturalness": 0.18,
            "bertscore_f1": 0.12,
            "context_keyword_coverage": 0.08,
            "persona_keyword_coverage": 0.05,
        }
    return {
        "context_relevance": 0.36,
        "persona_consistency": 0.30,
        "naturalness": 0.20,
        "context_keyword_coverage": 0.08,
        "persona_keyword_coverage": 0.06,
    }


def row_overall_quality(row: Dict[str, Any]) -> float:
    has_bertscore = "bertscore_f1" in row and not math.isnan(float(row.get("bertscore_f1", float("nan"))))
    weights = quality_weights(has_bertscore=has_bertscore)
    total = 0.0
    denom = 0.0
    for metric, weight in weights.items():
        if metric not in row:
            continue
        val = float(row.get(metric, 0.0))
        if math.isnan(val):
            continue
        total += weight * val
        denom += weight
    return (total / denom) if denom > 0.0 else 0.0


def utc_stamp() -> str:
    return datetime.now(timezone.utc).strftime("%Y%m%dT%H%M%SZ")


def utc_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def tokenize(text: str) -> List[str]:
    return TOKEN_RE.findall(text.lower())


def read_jsonl(path: Path) -> List[Dict[str, Any]]:
    rows: List[Dict[str, Any]] = []
    # Use utf-8-sig to tolerate BOM-prefixed files generated by some editors/shells.
    with path.open("r", encoding="utf-8-sig") as handle:
        for line in handle:
            line = line.strip()
            if not line:
                continue
            rows.append(json.loads(line))
    return rows


def write_jsonl(path: Path, rows: Iterable[Dict[str, Any]]) -> None:
    with path.open("w", encoding="utf-8") as handle:
        for row in rows:
            handle.write(json.dumps(row, ensure_ascii=False) + "\n")


def write_json(path: Path, payload: Any) -> None:
    with path.open("w", encoding="utf-8") as handle:
        json.dump(payload, handle, indent=2, ensure_ascii=False)


def sha256_file(path: Path) -> str:
    digest = hashlib.sha256()
    with path.open("rb") as handle:
        while True:
            chunk = handle.read(1024 * 1024)
            if not chunk:
                break
            digest.update(chunk)
    return digest.hexdigest()


def percentile(values: Sequence[float], p: float) -> float:
    if not values:
        return float("nan")
    sorted_vals = sorted(values)
    if len(sorted_vals) == 1:
        return sorted_vals[0]
    idx = (len(sorted_vals) - 1) * p
    lo = math.floor(idx)
    hi = math.ceil(idx)
    if lo == hi:
        return sorted_vals[lo]
    frac = idx - lo
    return sorted_vals[lo] * (1.0 - frac) + sorted_vals[hi] * frac


def bootstrap_mean_ci(values: Sequence[float], seed: int, iters: int = 2000) -> Dict[str, float]:
    if not values:
        return {
            "n": 0,
            "mean": float("nan"),
            "median": float("nan"),
            "p95": float("nan"),
            "ci95_low": float("nan"),
            "ci95_high": float("nan"),
        }

    vals = [float(v) for v in values]
    mean_val = statistics.fmean(vals)
    median_val = statistics.median(vals)
    p95_val = percentile(vals, 0.95)
    n = len(vals)
    if n == 1:
        low = high = mean_val
    else:
        rng = random.Random(seed)
        samples: List[float] = []
        for _ in range(iters):
            draw = [vals[rng.randrange(n)] for __ in range(n)]
            samples.append(statistics.fmean(draw))
        low = percentile(samples, 0.025)
        high = percentile(samples, 0.975)

    return {
        "n": n,
        "mean": mean_val,
        "median": median_val,
        "p95": p95_val,
        "ci95_low": low,
        "ci95_high": high,
    }


def paired_bootstrap_delta_ci(
    target_values: Sequence[float],
    baseline_values: Sequence[float],
    seed: int,
    iters: int = 3000,
) -> Dict[str, float]:
    n = min(len(target_values), len(baseline_values))
    if n == 0:
        return {
            "n": 0,
            "mean_delta": float("nan"),
            "ci95_low": float("nan"),
            "ci95_high": float("nan"),
            "p_delta_le_0": float("nan"),
        }

    deltas = [float(target_values[i]) - float(baseline_values[i]) for i in range(n)]
    mean_delta = statistics.fmean(deltas)
    if n == 1:
        return {
            "n": 1,
            "mean_delta": mean_delta,
            "ci95_low": mean_delta,
            "ci95_high": mean_delta,
            "p_delta_le_0": 1.0 if mean_delta <= 0 else 0.0,
        }

    rng = random.Random(seed)
    sampled_means: List[float] = []
    for _ in range(iters):
        draw = [deltas[rng.randrange(n)] for __ in range(n)]
        sampled_means.append(statistics.fmean(draw))
    ci95_low = percentile(sampled_means, 0.025)
    ci95_high = percentile(sampled_means, 0.975)
    p_delta_le_0 = sum(1 for x in sampled_means if x <= 0.0) / float(len(sampled_means))
    return {
        "n": n,
        "mean_delta": mean_delta,
        "ci95_low": ci95_low,
        "ci95_high": ci95_high,
        "p_delta_le_0": p_delta_le_0,
    }


def safe_shell(command: Sequence[str], timeout_s: int = 15) -> str:
    try:
        result = subprocess.run(
            list(command),
            check=False,
            capture_output=True,
            text=True,
            timeout=timeout_s,
        )
    except Exception:
        return ""
    if result.returncode != 0:
        return ""
    return result.stdout.strip()


def gather_hardware_metadata() -> Dict[str, Any]:
    metadata: Dict[str, Any] = {
        "timestamp_utc": utc_iso(),
        "platform": {
            "system": platform.system(),
            "release": platform.release(),
            "version": platform.version(),
            "machine": platform.machine(),
            "python": platform.python_version(),
        },
        "cpu_count_logical": os.cpu_count(),
    }

    if platform.system().lower().startswith("win"):
        cpu_name = safe_shell(
            [
                "powershell",
                "-NoProfile",
                "-Command",
                "(Get-CimInstance Win32_Processor | Select-Object -First 1 -ExpandProperty Name).Trim()",
            ]
        )
        ram_bytes = safe_shell(
            [
                "powershell",
                "-NoProfile",
                "-Command",
                "(Get-CimInstance Win32_ComputerSystem).TotalPhysicalMemory",
            ]
        )
        if cpu_name:
            metadata["cpu_name"] = cpu_name
        if ram_bytes.isdigit():
            metadata["ram_bytes"] = int(ram_bytes)
            metadata["ram_gb"] = round(int(ram_bytes) / (1024**3), 2)
    else:
        cpu_name = safe_shell(["sh", "-lc", "cat /proc/cpuinfo | grep 'model name' | head -n 1 | cut -d: -f2"])
        mem_total = safe_shell(["sh", "-lc", "cat /proc/meminfo | grep MemTotal | awk '{print $2}'"])
        if cpu_name:
            metadata["cpu_name"] = cpu_name.strip()
        if mem_total.isdigit():
            metadata["ram_kb"] = int(mem_total)
            metadata["ram_gb"] = round(int(mem_total) / (1024**2), 2)

    gpu_csv = safe_shell(
        [
            "nvidia-smi",
            "--query-gpu=name,memory.total,driver_version",
            "--format=csv,noheader",
        ],
        timeout_s=10,
    )
    metadata["gpu"] = []
    if gpu_csv:
        for line in gpu_csv.splitlines():
            parts = [p.strip() for p in line.split(",")]
            if len(parts) >= 3:
                metadata["gpu"].append(
                    {
                        "name": parts[0],
                        "memory_total": parts[1],
                        "driver_version": parts[2],
                    }
                )
    return metadata


def query_ollama_model(host: str, model: str) -> Dict[str, Any]:
    for payload in ({"name": model}, {"model": model}):
        try:
            resp = requests.post(f"{host}/api/show", json=payload, timeout=30)
            if resp.status_code == 200:
                data = resp.json()
                tensors = data.get("tensors")
                if isinstance(tensors, list):
                    data["tensor_count"] = len(tensors)
                    data["tensor_preview"] = tensors[:8]
                    data.pop("tensors", None)
                data["model_name"] = model
                data["host"] = host
                return data
        except Exception as exc:
            return {"model_name": model, "host": host, "error": str(exc)}
    return {"model_name": model, "host": host, "error": "Failed to query /api/show"}


def build_prompt(row: Dict[str, Any], include_dynamic_context: bool) -> str:
    persona = str(row.get("persona", "")).strip()
    context = str(row.get("dynamic_context", "")).strip()
    player_input = str(row.get("player_input", "")).strip()

    lines: List[str] = []
    lines.append("You are roleplaying one NPC in a narrative game.")
    lines.append(f"Persona: {persona}")
    lines.append("Rules:")
    lines.append("- Stay in persona.")
    lines.append("- Write 2-3 natural sentences.")
    lines.append("- Return only spoken dialogue, no labels, no metadata, no templates.")
    lines.append("- Never output hidden fields such as Temporal Memories or BehaviorTreeState.")
    if include_dynamic_context:
        lines.append("Current dynamic game state:")
        for chunk in [c.strip() for c in context.split(";") if c.strip()]:
            lines.append(f"- {chunk.replace('=', ': ')}")
        lines.append("Use at least two concrete details from current dynamic game state.")
    else:
        lines.append("Current dynamic game state is unavailable.")
    lines.append(f"Player says: {player_input}")
    lines.append("NPC reply:")
    return "\n".join(lines)


def sanitize_npc_response(text: str) -> str:
    return sanitize_response(text)


def generate_ollama_response(
    host: str,
    model: str,
    prompt: str,
    temperature: float,
    max_tokens: int,
    timeout_s: int,
) -> Dict[str, Any]:
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
        "options": {
            "temperature": temperature,
            "num_predict": max_tokens,
        },
    }
    try:
        resp = requests.post(f"{host}/api/generate", json=payload, timeout=timeout_s)
    except Exception as exc:
        return {"ok": False, "error": f"request_failed: {exc}"}
    if resp.status_code != 200:
        return {
            "ok": False,
            "error": f"http_{resp.status_code}",
            "body": resp.text[:220],
        }
    try:
        data = resp.json()
    except Exception as exc:
        return {"ok": False, "error": f"json_decode_failed: {exc}"}

    response_text = str(data.get("response", "")).strip()
    return {
        "ok": True,
        "response": response_text,
        "eval_count": int(data.get("eval_count", 0) or 0),
        "prompt_eval_count": int(data.get("prompt_eval_count", 0) or 0),
        "eval_duration_ns": int(data.get("eval_duration", 0) or 0),
        "total_duration_ns": int(data.get("total_duration", 0) or 0),
    }


def keyword_coverage(text: str, keywords: Sequence[str]) -> float:
    kws = [str(k).strip().lower() for k in keywords if str(k).strip()]
    if not kws:
        return 0.0
    lowered = text.lower()
    text_tokens = set(tokenize(text))
    hits = 0
    for kw in kws:
        if kw in lowered:
            hits += 1
            continue
        kw_tokens = tokenize(kw)
        if kw_tokens and all(token in text_tokens for token in kw_tokens):
            hits += 1
    return hits / len(kws)


def jaccard_overlap(a: str, b: str) -> float:
    ta = set(tokenize(a))
    tb = set(tokenize(b))
    if not ta or not tb:
        return 0.0
    return len(ta.intersection(tb)) / len(ta.union(tb))


def repetition_ratio(text: str, n: int = 3) -> float:
    tokens = tokenize(text)
    if len(tokens) < n:
        return 0.0
    grams = [tuple(tokens[idx : idx + n]) for idx in range(len(tokens) - n + 1)]
    if not grams:
        return 0.0
    counts: Dict[tuple[str, ...], int] = {}
    for gram in grams:
        counts[gram] = counts.get(gram, 0) + 1
    repeats = sum(count - 1 for count in counts.values() if count > 1)
    return repeats / len(grams)


def sentence_count(text: str) -> int:
    rough = [x.strip() for x in re.split(r"[.!?]+", text) if x.strip()]
    return len(rough)


def style_score(persona: str, response: str) -> float:
    persona_l = persona.lower()
    words = tokenize(response)
    wc = len(words)
    scores: List[float] = []

    if "brief" in persona_l or "short" in persona_l:
        if wc <= 55:
            scores.append(1.0)
        else:
            scores.append(max(0.0, 1.0 - (wc - 55) / 80.0))
    if "talkative" in persona_l:
        scores.append(min(1.0, wc / 32.0))
    if "formal" in persona_l:
        long_words = sum(1 for w in words if len(w) >= 7)
        scores.append(min(1.0, long_words / max(1.0, wc * 0.25)))
    if "mysterious" in persona_l or "indirect" in persona_l:
        marker_hit = keyword_coverage(response, ["perhaps", "price", "ritual", "shadow", "moon", "curse"])
        scores.append(marker_hit)
    if "procedural" in persona_l:
        marker_hit = keyword_coverage(response, ["statement", "verify", "evidence", "process"])
        scores.append(marker_hit)

    if not scores:
        return 0.5
    return float(sum(scores) / len(scores))


def context_relevance_score(response: str, context: str, context_keywords: Sequence[str]) -> float:
    kw = keyword_coverage(response, context_keywords)
    overlap = jaccard_overlap(response, context)
    return 0.7 * kw + 0.3 * overlap


def persona_consistency_score(response: str, persona: str, persona_keywords: Sequence[str]) -> float:
    kw = keyword_coverage(response, persona_keywords)
    sty = style_score(persona, response)
    return 0.8 * kw + 0.2 * sty


def naturalness_components(response: str) -> Dict[str, float]:
    words = tokenize(response)
    wc = len(words)
    if wc == 0:
        return {
            "naturalness": 0.0,
            "distinct1": 0.0,
            "length_score": 0.0,
            "sentence_score": 0.0,
        }

    distinct_1 = len(set(words)) / wc
    rep = repetition_ratio(response, n=3)

    target = 38.0
    spread = 30.0
    length_score = max(0.0, 1.0 - abs(wc - target) / spread)

    sc = sentence_count(response)
    if 2 <= sc <= 3:
        sentence_score = 1.0
    elif sc == 1 or sc == 4:
        sentence_score = 0.65
    else:
        sentence_score = 0.35

    score = 0.4 * distinct_1 + 0.3 * (1.0 - rep) + 0.2 * length_score + 0.1 * sentence_score
    return {
        "naturalness": max(0.0, min(1.0, score)),
        "distinct1": max(0.0, min(1.0, distinct_1)),
        "length_score": max(0.0, min(1.0, length_score)),
        "sentence_score": max(0.0, min(1.0, sentence_score)),
    }


def naturalness_score(response: str) -> float:
    return float(naturalness_components(response).get("naturalness", 0.0))


def compute_optional_bertscore(
    predictions: List[str],
    references: List[str],
    lang: str = "en",
    model_type: str = "",
    batch_size: int = 16,
    cache_dir: str = "",
) -> Dict[str, Any]:
    try:
        import transformers.utils.import_utils as iu  # type: ignore
        iu._torchao_available = False
        iu._torchao_version = "0.0.0"
    except Exception:
        pass

    try:
        from bert_score import score as bert_score  # type: ignore
    except Exception as exc:
        return {"available": False, "reason": str(exc)}

    env_backup: Dict[str, Optional[str]] = {}
    resolved_cache_dir = str(cache_dir).strip()
    if resolved_cache_dir:
        cache_path = Path(resolved_cache_dir).expanduser()
        cache_path.mkdir(parents=True, exist_ok=True)
        resolved_cache_dir = str(cache_path)
        for key in ("HF_HOME", "TRANSFORMERS_CACHE", "HUGGINGFACE_HUB_CACHE"):
            env_backup[key] = os.environ.get(key)
            os.environ[key] = resolved_cache_dir

    kwargs: Dict[str, Any] = {
        "lang": lang,
        "rescale_with_baseline": True,
        "batch_size": max(1, int(batch_size)),
    }
    chosen_model = str(model_type).strip()
    if chosen_model:
        kwargs["model_type"] = chosen_model

    try:
        _, _, f1 = bert_score(predictions, references, **kwargs)
        values = [float(x) for x in f1]
        return {
            "available": True,
            "values": values,
            "lang": lang,
            "model_type": chosen_model or "auto",
            "batch_size": int(kwargs["batch_size"]),
            "cache_dir": resolved_cache_dir,
        }
    except Exception as exc:
        return {"available": False, "reason": str(exc)}
    finally:
        if env_backup:
            for key, value in env_backup.items():
                if value is None:
                    os.environ.pop(key, None)
                else:
                    os.environ[key] = value


def evaluate_responses_for_arm(
    responses: List[Dict[str, Any]],
    scenarios_by_id: Dict[str, Dict[str, Any]],
) -> List[Dict[str, Any]]:
    scored: List[Dict[str, Any]] = []
    for row in responses:
        sid = str(row.get("scenario_id", ""))
        scenario = scenarios_by_id.get(sid, {})
        response = str(row.get("response", ""))
        persona = str(scenario.get("persona", ""))
        context = str(scenario.get("dynamic_context", ""))
        context_keywords = [str(x) for x in scenario.get("context_keywords", [])]
        persona_keywords = [str(x) for x in scenario.get("persona_keywords", [])]

        context_kw = keyword_coverage(response, context_keywords)
        context_ov = jaccard_overlap(response, context)
        persona_kw = keyword_coverage(response, persona_keywords)
        persona_sty = style_score(persona, response)
        nat = naturalness_components(response)

        context_rel = context_relevance_score(response, context, context_keywords)
        persona_cons = persona_consistency_score(response, persona, persona_keywords)
        natural = float(nat.get("naturalness", 0.0))

        scored.append(
            {
                **row,
                "context_relevance": context_rel,
                "persona_consistency": persona_cons,
                "naturalness": natural,
                "context_keyword_coverage": context_kw,
                "context_overlap": context_ov,
                "persona_keyword_coverage": persona_kw,
                "persona_style": persona_sty,
                "distinct1": float(nat.get("distinct1", 0.0)),
                "length_score": float(nat.get("length_score", 0.0)),
                "sentence_score": float(nat.get("sentence_score", 0.0)),
            }
        )
    return scored


def summarize_arm_scores(
    scored_rows: List[Dict[str, Any]],
    seed: int,
    include_bertscore: bool,
) -> Dict[str, Any]:
    metrics = list(BASE_METRICS) + list(DIAGNOSTIC_METRICS)
    if include_bertscore:
        metrics.append("bertscore_f1")

    out: Dict[str, Any] = {"sample_count": len(scored_rows)}
    for idx, metric in enumerate(metrics):
        vals = [float(row.get(metric, float("nan"))) for row in scored_rows]
        vals = [x for x in vals if not math.isnan(x)]
        out[metric] = bootstrap_mean_ci(vals, seed=seed + 17 * (idx + 1))

    overall_vals: List[float] = []
    for row in scored_rows:
        if "overall_quality" in row:
            overall_vals.append(float(row.get("overall_quality", 0.0)))
        else:
            overall_vals.append(row_overall_quality(row))
    out["overall_quality"] = bootstrap_mean_ci(overall_vals, seed=seed + 211)
    out["metrics"] = metrics + ["overall_quality"]

    return out


def diff_against_reference(
    summary: Dict[str, Dict[str, Any]],
    target_arm: str,
    baseline_arm: str,
    metrics: Optional[Sequence[str]] = None,
) -> Dict[str, Any]:
    metric_list = list(metrics) if metrics else metric_names(include_bertscore=True)
    out: Dict[str, Any] = {}
    target = summary.get(target_arm, {})
    baseline = summary.get(baseline_arm, {})
    for metric in metric_list:
        target_mean = target.get(metric, {}).get("mean")
        base_mean = baseline.get(metric, {}).get("mean")
        if target_mean is None or base_mean is None:
            continue
        if math.isnan(float(target_mean)) or math.isnan(float(base_mean)):
            continue
        abs_delta = float(target_mean) - float(base_mean)
        rel_delta = (abs_delta / float(base_mean)) if float(base_mean) != 0 else float("nan")
        out[metric] = {
            "target_mean": float(target_mean),
            "baseline_mean": float(base_mean),
            "absolute_delta": abs_delta,
            "relative_delta": rel_delta,
        }
    return out


def paired_metric_deltas(
    scored_by_arm: Dict[str, List[Dict[str, Any]]],
    target_arm: str,
    baseline_arm: str,
    seed: int,
    metrics: Optional[Sequence[str]] = None,
) -> Dict[str, Any]:
    metric_list = list(metrics) if metrics else metric_names(include_bertscore=True)
    target_rows = scored_by_arm.get(target_arm, [])
    baseline_rows = scored_by_arm.get(baseline_arm, [])
    if not target_rows or not baseline_rows:
        return {}

    target_map: Dict[tuple[str, int], Dict[str, Any]] = {}
    baseline_map: Dict[tuple[str, int], Dict[str, Any]] = {}
    for row in target_rows:
        key = (str(row.get("scenario_id", "")), int(row.get("repeat_index", 0) or 0))
        target_map[key] = row
    for row in baseline_rows:
        key = (str(row.get("scenario_id", "")), int(row.get("repeat_index", 0) or 0))
        baseline_map[key] = row

    common_keys = sorted(set(target_map.keys()).intersection(set(baseline_map.keys())))
    if not common_keys:
        return {}

    out: Dict[str, Any] = {}
    for metric_idx, metric in enumerate(metric_list):
        target_vals: List[float] = []
        baseline_vals: List[float] = []
        for key in common_keys:
            t_row = target_map[key]
            b_row = baseline_map[key]
            if metric == "overall_quality":
                t_val = float(t_row.get("overall_quality", row_overall_quality(t_row)))
                b_val = float(b_row.get("overall_quality", row_overall_quality(b_row)))
            else:
                if metric not in t_row or metric not in b_row:
                    continue
                t_val = float(t_row.get(metric, 0.0))
                b_val = float(b_row.get(metric, 0.0))
            if math.isnan(t_val) or math.isnan(b_val):
                continue
            target_vals.append(t_val)
            baseline_vals.append(b_val)

        if not target_vals or not baseline_vals:
            continue
        out[metric] = paired_bootstrap_delta_ci(
            target_values=target_vals,
            baseline_values=baseline_vals,
            seed=seed + 101 * (metric_idx + 1),
        )
    return out


def paired_metric_win_rates(
    scored_by_arm: Dict[str, List[Dict[str, Any]]],
    target_arm: str,
    baseline_arm: str,
    seed: int,
    metrics: Optional[Sequence[str]] = None,
) -> Dict[str, Any]:
    metric_list = list(metrics) if metrics else metric_names(include_bertscore=True)
    target_rows = scored_by_arm.get(target_arm, [])
    baseline_rows = scored_by_arm.get(baseline_arm, [])
    if not target_rows or not baseline_rows:
        return {}

    target_map: Dict[tuple[str, int], Dict[str, Any]] = {}
    baseline_map: Dict[tuple[str, int], Dict[str, Any]] = {}
    for row in target_rows:
        key = (str(row.get("scenario_id", "")), int(row.get("repeat_index", 0) or 0))
        target_map[key] = row
    for row in baseline_rows:
        key = (str(row.get("scenario_id", "")), int(row.get("repeat_index", 0) or 0))
        baseline_map[key] = row
    common_keys = sorted(set(target_map.keys()).intersection(set(baseline_map.keys())))
    if not common_keys:
        return {}

    out: Dict[str, Any] = {}
    for metric_idx, metric in enumerate(metric_list):
        wins = 0
        losses = 0
        ties = 0
        soft_outcomes: List[float] = []
        strict_outcomes: List[float] = []
        for key in common_keys:
            t_row = target_map[key]
            b_row = baseline_map[key]
            if metric == "overall_quality":
                t_val = float(t_row.get("overall_quality", row_overall_quality(t_row)))
                b_val = float(b_row.get("overall_quality", row_overall_quality(b_row)))
            else:
                if metric not in t_row or metric not in b_row:
                    continue
                t_val = float(t_row.get(metric, float("nan")))
                b_val = float(b_row.get(metric, float("nan")))
            if math.isnan(t_val) or math.isnan(b_val):
                continue

            if t_val > b_val:
                wins += 1
                soft_outcomes.append(1.0)
                strict_outcomes.append(1.0)
            elif t_val < b_val:
                losses += 1
                soft_outcomes.append(0.0)
                strict_outcomes.append(0.0)
            else:
                ties += 1
                soft_outcomes.append(0.5)

        compared_n = wins + losses + ties
        if compared_n == 0:
            continue

        soft_summary = bootstrap_mean_ci(soft_outcomes, seed=seed + 131 * (metric_idx + 1))
        non_tie_n = wins + losses
        if non_tie_n > 0:
            strict_summary = bootstrap_mean_ci(strict_outcomes, seed=seed + 151 * (metric_idx + 1))
        else:
            strict_summary = {
                "n": 0,
                "mean": float("nan"),
                "median": float("nan"),
                "p95": float("nan"),
                "ci95_low": float("nan"),
                "ci95_high": float("nan"),
            }

        out[metric] = {
            "n": compared_n,
            "wins": wins,
            "losses": losses,
            "ties": ties,
            "soft_win_rate": soft_summary,
            "strict_non_tie_win_rate": strict_summary,
        }
    return out


def parse_dynamic_context_map(dynamic_context: str) -> Dict[str, str]:
    out: Dict[str, str] = {}
    for chunk in [x.strip() for x in str(dynamic_context).split(";") if x.strip()]:
        if "=" not in chunk:
            continue
        key, value = chunk.split("=", 1)
        out[str(key).strip().lower()] = str(value).strip()
    return out


def infer_location_type(location: str) -> str:
    lowered = location.lower()
    if any(k in lowered for k in ("gate", "wall", "checkpoint")):
        return "security"
    if any(k in lowered for k in ("market", "shop", "stall")):
        return "commerce"
    if any(k in lowered for k in ("library", "archive", "school")):
        return "knowledge"
    if any(k in lowered for k in ("forge", "smith", "workshop")):
        return "craft"
    if any(k in lowered for k in ("dungeon", "cell", "prison")):
        return "detention"
    if any(k in lowered for k in ("forest", "grove", "wild")):
        return "wilderness"
    if any(k in lowered for k in ("clinic", "hut", "healer", "hospital")):
        return "medical"
    return "other"


def infer_conflict_type(player_input: str) -> str:
    lowered = player_input.lower()
    if any(k in lowered for k in ("discount", "price", "cheap", "bargain")):
        return "economic_negotiation"
    if any(k in lowered for k in ("innocent", "trust", "prove", "why are you")):
        return "credibility_dispute"
    if any(k in lowered for k in ("help", "cure", "heal")):
        return "assistance_request"
    if any(k in lowered for k in ("weapon", "how should", "what should")):
        return "advisory_request"
    if any(k in lowered for k in ("let me", "entry", "open", "allow")):
        return "access_control"
    return "general_dialogue"


def infer_persona_archetype(persona: str) -> str:
    lowered = persona.lower()
    candidates = (
        "gatekeeper",
        "merchant",
        "healer",
        "blacksmith",
        "scholar",
        "captain",
        "prisoner",
        "witch",
    )
    for name in candidates:
        if name in lowered:
            return name
    return "other"


def get_scenario_tags(scenario: Dict[str, Any]) -> Dict[str, str]:
    tags: Dict[str, str] = {}
    raw_tags = scenario.get("scenario_tags")
    if isinstance(raw_tags, dict):
        for key, value in raw_tags.items():
            key_s = str(key).strip()
            val_s = str(value).strip()
            if key_s and val_s:
                tags[key_s] = val_s

    ctx = parse_dynamic_context_map(str(scenario.get("dynamic_context", "")))
    if "behavior_state" not in tags and ctx.get("behaviortreestate"):
        tags["behavior_state"] = str(ctx.get("behaviortreestate", "")).strip()
    if "location_type" not in tags:
        loc = str(ctx.get("location", ""))
        if loc:
            tags["location_type"] = infer_location_type(loc)
    if "conflict_type" not in tags:
        tags["conflict_type"] = infer_conflict_type(str(scenario.get("player_input", "")))
    if "persona_archetype" not in tags:
        tags["persona_archetype"] = infer_persona_archetype(str(scenario.get("persona", "")))
    return tags


def summarize_scores_by_slice(
    scored_by_arm: Dict[str, List[Dict[str, Any]]],
    scenarios_by_id: Dict[str, Dict[str, Any]],
    slice_keys: Sequence[str],
    seed: int,
    include_bertscore: bool,
) -> Dict[str, Any]:
    chosen_metrics = [*BASE_METRICS, "overall_quality"]
    chosen_metrics.extend(["context_keyword_coverage", "persona_keyword_coverage"])
    if include_bertscore:
        chosen_metrics.append("bertscore_f1")

    output: Dict[str, Any] = {"slice_keys": list(slice_keys), "arms": {}}
    for arm_id, rows in scored_by_arm.items():
        arm_payload: Dict[str, Any] = {}
        for slice_idx, slice_key in enumerate(slice_keys):
            buckets: Dict[str, List[Dict[str, Any]]] = {}
            for row in rows:
                sid = str(row.get("scenario_id", ""))
                scenario = scenarios_by_id.get(sid, {})
                tags = get_scenario_tags(scenario)
                value = str(tags.get(slice_key, "")).strip()
                if not value:
                    continue
                buckets.setdefault(value, []).append(row)

            slice_payload: Dict[str, Any] = {}
            for bucket_idx, (bucket_value, bucket_rows) in enumerate(sorted(buckets.items())):
                metric_payload: Dict[str, Any] = {"sample_count": len(bucket_rows)}
                for metric_idx, metric_name in enumerate(chosen_metrics):
                    vals = [float(r.get(metric_name, float("nan"))) for r in bucket_rows if metric_name in r]
                    vals = [x for x in vals if not math.isnan(x)]
                    metric_payload[metric_name] = bootstrap_mean_ci(
                        vals,
                        seed=seed + 211 * (slice_idx + 1) + 41 * (bucket_idx + 1) + metric_idx,
                    )
                slice_payload[bucket_value] = metric_payload
            arm_payload[slice_key] = slice_payload
        output["arms"][arm_id] = arm_payload
    return output


def cohen_kappa_from_labels(labels_a: Sequence[int], labels_b: Sequence[int]) -> float:
    n = min(len(labels_a), len(labels_b))
    if n == 0:
        return float("nan")
    a = [int(x) for x in labels_a[:n]]
    b = [int(x) for x in labels_b[:n]]
    labels = sorted(set(a) | set(b))
    if not labels:
        return float("nan")

    observed = sum(1 for x, y in zip(a, b) if x == y) / float(n)
    pe = 0.0
    for label in labels:
        pa = sum(1 for x in a if x == label) / float(n)
        pb = sum(1 for y in b if y == label) / float(n)
        pe += pa * pb
    if pe >= 1.0:
        return float("nan")
    return (observed - pe) / (1.0 - pe)


def read_human_eval_rows(path: Path) -> List[Dict[str, Any]]:
    if path.suffix.lower() == ".jsonl":
        return read_jsonl(path)
    if path.suffix.lower() in (".csv", ".tsv"):
        delimiter = "," if path.suffix.lower() == ".csv" else "\t"
        with path.open("r", encoding="utf-8-sig", newline="") as handle:
            reader = csv.DictReader(handle, delimiter=delimiter)
            return [dict(row) for row in reader]
    raise ValueError(f"Unsupported human-eval format: {path}")


def normalize_human_score(value: Any, scale_max: float) -> float:
    try:
        raw = float(value)
    except Exception:
        return float("nan")
    if math.isnan(raw):
        return float("nan")
    if raw < 0.0:
        return float("nan")
    if raw <= 1.0:
        return max(0.0, min(1.0, raw))
    denom = max(1.0, float(scale_max))
    return max(0.0, min(1.0, raw / denom))


def analyze_human_eval(
    rows: List[Dict[str, Any]],
    metrics: Sequence[str],
    scale_max: float,
    target_arm: str,
    baseline_arms: Sequence[str],
    seed: int,
) -> Dict[str, Any]:
    normalized: List[Dict[str, Any]] = []
    for row in rows:
        scenario_id = str(row.get("scenario_id", "")).strip()
        arm_id = str(row.get("arm_id", "")).strip()
        annotator = str(row.get("annotator_id", row.get("annotator", row.get("user", "")))).strip()
        if not scenario_id or not arm_id or not annotator:
            continue
        item: Dict[str, Any] = {
            "scenario_id": scenario_id,
            "arm_id": arm_id,
            "annotator_id": annotator,
        }
        has_metric = False
        for metric in metrics:
            val = normalize_human_score(row.get(metric, float("nan")), scale_max=scale_max)
            if not math.isnan(val):
                item[metric] = val
                has_metric = True
        if has_metric:
            normalized.append(item)

    out: Dict[str, Any] = {
        "row_count": len(normalized),
        "metric_scale_max": scale_max,
        "metrics": list(metrics),
        "arms": {},
        "agreement": {},
        "paired_deltas": {},
        "preferences": {},
    }
    if not normalized:
        return out

    by_arm: Dict[str, List[Dict[str, Any]]] = {}
    for row in normalized:
        by_arm.setdefault(str(row.get("arm_id", "")), []).append(row)

    for arm_id, arm_rows in sorted(by_arm.items()):
        metric_summary: Dict[str, Any] = {"sample_count": len(arm_rows)}
        for metric_idx, metric in enumerate(metrics):
            vals = [float(r.get(metric, float("nan"))) for r in arm_rows if metric in r]
            vals = [x for x in vals if not math.isnan(x)]
            metric_summary[metric] = bootstrap_mean_ci(vals, seed=seed + 17 * (metric_idx + 1) + len(arm_id))
        out["arms"][arm_id] = metric_summary

    # Inter-rater agreement: pairwise kappa over shared (scenario, arm) items per metric.
    annotators = sorted({str(r.get("annotator_id", "")) for r in normalized})
    for metric_idx, metric in enumerate(metrics):
        ratings: Dict[Tuple[str, str], Dict[str, int]] = {}
        for row in normalized:
            if metric not in row:
                continue
            key = (str(row.get("scenario_id", "")), str(row.get("arm_id", "")))
            score_bin = int(round(float(row.get(metric, 0.0)) * 4.0))
            ratings.setdefault(key, {})[str(row.get("annotator_id", ""))] = score_bin

        pairwise: Dict[str, float] = {}
        pair_vals: List[float] = []
        for a1, a2 in combinations(annotators, 2):
            labs1: List[int] = []
            labs2: List[int] = []
            for key, mapping in ratings.items():
                if a1 in mapping and a2 in mapping:
                    labs1.append(int(mapping[a1]))
                    labs2.append(int(mapping[a2]))
            if not labs1:
                continue
            kappa = cohen_kappa_from_labels(labs1, labs2)
            pairwise[f"{a1}__{a2}"] = kappa
            if not math.isnan(kappa):
                pair_vals.append(kappa)

        out["agreement"][metric] = {
            "pairwise_kappa": pairwise,
            "mean_pairwise_kappa": (statistics.fmean(pair_vals) if pair_vals else float("nan")),
            "pair_count": len(pairwise),
        }

    # Paired deltas by scenario-level mean score across annotators.
    scenario_arm_mean: Dict[Tuple[str, str], Dict[str, float]] = {}
    for metric in metrics:
        grouped: Dict[Tuple[str, str], List[float]] = {}
        for row in normalized:
            if metric not in row:
                continue
            key = (str(row.get("scenario_id", "")), str(row.get("arm_id", "")))
            grouped.setdefault(key, []).append(float(row.get(metric, 0.0)))
        for key, values in grouped.items():
            payload = scenario_arm_mean.setdefault(key, {})
            payload[metric] = statistics.fmean(values)

    for baseline_idx, baseline_arm in enumerate(baseline_arms):
        comparison_key = f"{target_arm}_vs_{baseline_arm}"
        metric_deltas: Dict[str, Any] = {}
        for metric_idx, metric in enumerate(metrics):
            target_vals: List[float] = []
            baseline_vals: List[float] = []
            scenario_ids = sorted({sid for sid, _ in scenario_arm_mean.keys()})
            for sid in scenario_ids:
                target_key = (sid, target_arm)
                base_key = (sid, baseline_arm)
                if target_key not in scenario_arm_mean or base_key not in scenario_arm_mean:
                    continue
                if metric not in scenario_arm_mean[target_key] or metric not in scenario_arm_mean[base_key]:
                    continue
                target_vals.append(float(scenario_arm_mean[target_key][metric]))
                baseline_vals.append(float(scenario_arm_mean[base_key][metric]))
            if not target_vals:
                continue
            metric_deltas[metric] = paired_bootstrap_delta_ci(
                target_vals,
                baseline_vals,
                seed=seed + 601 * (baseline_idx + 1) + 29 * (metric_idx + 1),
            )
        out["paired_deltas"][comparison_key] = metric_deltas

    # Derived preferences: per (scenario, annotator), compare overall_quality-like metric.
    pref_metric = "overall_quality" if "overall_quality" in metrics else metrics[-1]
    score_lookup: Dict[Tuple[str, str, str], float] = {}
    for row in normalized:
        if pref_metric not in row:
            continue
        key = (
            str(row.get("scenario_id", "")),
            str(row.get("annotator_id", "")),
            str(row.get("arm_id", "")),
        )
        score_lookup[key] = float(row.get(pref_metric, 0.0))

    for baseline_idx, baseline_arm in enumerate(baseline_arms):
        comparison_key = f"{target_arm}_vs_{baseline_arm}"
        outcomes: List[float] = []
        wins = 0
        losses = 0
        ties = 0
        keys = sorted({(sid, ann) for sid, ann, _ in score_lookup.keys()})
        for sid, ann in keys:
            tk = (sid, ann, target_arm)
            bk = (sid, ann, baseline_arm)
            if tk not in score_lookup or bk not in score_lookup:
                continue
            tv = score_lookup[tk]
            bv = score_lookup[bk]
            if tv > bv:
                wins += 1
                outcomes.append(1.0)
            elif tv < bv:
                losses += 1
                outcomes.append(0.0)
            else:
                ties += 1
                outcomes.append(0.5)

        strict = [x for x in outcomes if x in (0.0, 1.0)]
        out["preferences"][comparison_key] = {
            "n": len(outcomes),
            "wins": wins,
            "losses": losses,
            "ties": ties,
            "soft_win_rate": bootstrap_mean_ci(outcomes, seed=seed + 701 * (baseline_idx + 1)),
            "strict_non_tie_win_rate": bootstrap_mean_ci(strict, seed=seed + 709 * (baseline_idx + 1))
            if strict
            else {
                "n": 0,
                "mean": float("nan"),
                "median": float("nan"),
                "p95": float("nan"),
                "ci95_low": float("nan"),
                "ci95_high": float("nan"),
            },
        }

    return out


def render_human_eval_report(path: Path, payload: Dict[str, Any]) -> None:
    lines: List[str] = []
    lines.append("# Human Evaluation Report")
    lines.append("")
    lines.append(f"- Total normalized rows: `{payload.get('row_count', 0)}`")
    lines.append(f"- Metrics: `{', '.join(payload.get('metrics', []))}`")
    lines.append("")

    lines.append("## Arm Summary")
    lines.append("| Arm | Metric | Mean | 95% CI |")
    lines.append("|---|---|---:|---:|")
    for arm_id, arm_data in payload.get("arms", {}).items():
        for metric in payload.get("metrics", []):
            m = arm_data.get(metric, {})
            lines.append(
                f"| {arm_id} | {metric} | {m.get('mean', float('nan')):.4f} | "
                f"({m.get('ci95_low', float('nan')):.4f}, {m.get('ci95_high', float('nan')):.4f}) |"
            )
    lines.append("")

    lines.append("## Agreement")
    lines.append("| Metric | Mean Pairwise Kappa | Pair Count |")
    lines.append("|---|---:|---:|")
    for metric, row in payload.get("agreement", {}).items():
        lines.append(
            f"| {metric} | {row.get('mean_pairwise_kappa', float('nan')):.4f} | {row.get('pair_count', 0)} |"
        )
    lines.append("")

    lines.append("## Preference Wins")
    lines.append("| Comparison | Soft Win Rate | Strict Non-tie Win Rate |")
    lines.append("|---|---:|---:|")
    for comp, row in payload.get("preferences", {}).items():
        soft = row.get("soft_win_rate", {})
        strict = row.get("strict_non_tie_win_rate", {})
        lines.append(
            f"| {comp} | {soft.get('mean', float('nan')):.4f} | {strict.get('mean', float('nan')):.4f} |"
        )

    path.write_text("\n".join(lines), encoding="utf-8")


def analyze_errors(scored_rows: List[Dict[str, Any]]) -> Dict[str, Any]:
    categories = {
        "context_miss": 0,
        "persona_drift": 0,
        "low_naturalness": 0,
        "too_short": 0,
    }
    per_case: List[Dict[str, Any]] = []
    for row in scored_rows:
        response = str(row.get("response", ""))
        labels: List[str] = []
        if float(row.get("context_relevance", 0.0)) < 0.35:
            labels.append("context_miss")
        if float(row.get("persona_consistency", 0.0)) < 0.35:
            labels.append("persona_drift")
        if float(row.get("naturalness", 0.0)) < 0.40:
            labels.append("low_naturalness")
        if len(tokenize(response)) < 10:
            labels.append("too_short")

        for label in labels:
            categories[label] += 1
        per_case.append(
            {
                "scenario_id": row.get("scenario_id"),
                "labels": labels,
            }
        )

    return {
        "counts": categories,
        "per_case": per_case,
    }


def render_report(
    output_path: Path,
    run_id: str,
    scenarios_path: Path,
    arms: List[EvaluationArm],
    summary: Dict[str, Any],
    deltas: Dict[str, Any],
    paired_deltas: Dict[str, Any],
    bertscore_meta: Dict[str, Any],
    win_rates: Optional[Dict[str, Any]] = None,
    slice_summary: Optional[Dict[str, Any]] = None,
    human_eval: Optional[Dict[str, Any]] = None,
) -> None:
    lines: List[str] = []
    lines.append("# Proposal Alignment Evaluation Report")
    lines.append("")
    lines.append(f"- Run ID: `{run_id}`")
    lines.append(f"- Generated: `{utc_iso()}`")
    lines.append(f"- Scenarios: `{scenarios_path}`")
    lines.append(f"- Scenario count: `{len(read_jsonl(scenarios_path))}`")
    lines.append("")
    lines.append("## Evaluation Arms")
    for arm in arms:
        lines.append(
            f"- `{arm.arm_id}`: model `{arm.model}`, dynamic_context={'on' if arm.include_dynamic_context else 'off'}, "
            f"response_control={'on' if arm.use_response_control else 'off'}"
        )
    lines.append("")
    lines.append("## Metric Summary (mean, 95% CI)")
    lines.append("| Arm | Context Relevance | Persona Consistency | Naturalness | Overall | BERTScore F1 |")
    lines.append("|---|---:|---:|---:|---:|---:|")
    for arm in arms:
        arm_summary = summary.get(arm.arm_id, {})
        c = arm_summary.get("context_relevance", {})
        p = arm_summary.get("persona_consistency", {})
        n = arm_summary.get("naturalness", {})
        o = arm_summary.get("overall_quality", {})
        b = arm_summary.get("bertscore_f1", {})
        if b:
            b_text = f"{b.get('mean', float('nan')):.4f}"
        else:
            b_text = "n/a"
        lines.append(
            "| "
            + arm.arm_id
            + " | "
            + f"{c.get('mean', float('nan')):.4f} ({c.get('ci95_low', float('nan')):.4f}, {c.get('ci95_high', float('nan')):.4f})"
            + " | "
            + f"{p.get('mean', float('nan')):.4f} ({p.get('ci95_low', float('nan')):.4f}, {p.get('ci95_high', float('nan')):.4f})"
            + " | "
            + f"{n.get('mean', float('nan')):.4f} ({n.get('ci95_low', float('nan')):.4f}, {n.get('ci95_high', float('nan')):.4f})"
            + " | "
            + f"{o.get('mean', float('nan')):.4f} ({o.get('ci95_low', float('nan')):.4f}, {o.get('ci95_high', float('nan')):.4f})"
            + " | "
            + b_text
            + " |"
        )
    lines.append("")

    lines.append("## Deltas vs Baselines")
    lines.append("| Comparison | Metric | Absolute Delta | Relative Delta |")
    lines.append("|---|---|---:|---:|")
    for comp_name, comp_metrics in deltas.items():
        for metric_name, vals in comp_metrics.items():
            lines.append(
                f"| {comp_name} | {metric_name} | {vals.get('absolute_delta', float('nan')):.4f} | {vals.get('relative_delta', float('nan')):.4f} |"
            )
    lines.append("")

    if paired_deltas:
        lines.append("## Paired Bootstrap Delta Significance")
        lines.append("| Comparison | Metric | Mean Delta | 95% CI | p(delta<=0) |")
        lines.append("|---|---|---:|---:|---:|")
        for comp_name, comp_metrics in paired_deltas.items():
            for metric_name, vals in comp_metrics.items():
                lines.append(
                    f"| {comp_name} | {metric_name} | {vals.get('mean_delta', float('nan')):.4f} | "
                    f"({vals.get('ci95_low', float('nan')):.4f}, {vals.get('ci95_high', float('nan')):.4f}) | "
                    f"{vals.get('p_delta_le_0', float('nan')):.4f} |"
                )
        lines.append("")

    if win_rates:
        lines.append("## Pairwise Win Rates")
        lines.append("| Comparison | Metric | Wins | Losses | Ties | Soft Win Rate | Strict Non-tie Win Rate |")
        lines.append("|---|---|---:|---:|---:|---:|---:|")
        for comp_name, comp_metrics in win_rates.items():
            for metric_name, vals in comp_metrics.items():
                soft = vals.get("soft_win_rate", {})
                strict = vals.get("strict_non_tie_win_rate", {})
                lines.append(
                    f"| {comp_name} | {metric_name} | {vals.get('wins', 0)} | {vals.get('losses', 0)} | "
                    f"{vals.get('ties', 0)} | {soft.get('mean', float('nan')):.4f} | {strict.get('mean', float('nan')):.4f} |"
                )
        lines.append("")

    if slice_summary:
        lines.append("## Scenario Slice Coverage")
        lines.append(f"- Slice keys: `{', '.join(slice_summary.get('slice_keys', []))}`")
        lines.append("- Detailed slice metrics are published in `slice_summary.json`.")
        lines.append("")

    if human_eval:
        lines.append("## Human Evaluation")
        lines.append(f"- Normalized rows: `{human_eval.get('row_count', 0)}`")
        lines.append("- Agreement and preference analysis is published in `human_eval_summary.json`.")
        lines.append("- A readable version is published in `human_eval_report.md`.")
        lines.append("")

    if bertscore_meta.get("available"):
        lines.append("- BERTScore status: enabled.")
    else:
        lines.append(f"- BERTScore status: unavailable ({bertscore_meta.get('reason', 'unknown')}).")
    lines.append("")
    lines.append("This report directly covers proposal RO5 metrics: context relevance, persona consistency, naturalness, and baseline deltas.")
    output_path.write_text("\n".join(lines), encoding="utf-8")


def main() -> None:
    parser = argparse.ArgumentParser(description="Run proposal-aligned evaluation and export artifacts.")
    parser.add_argument("--host", default="http://127.0.0.1:11434", help="Ollama host URL")
    parser.add_argument("--candidate-model", default="elara-npc:latest", help="Candidate fine-tuned model")
    parser.add_argument("--baseline-model", default="phi3:mini", help="Baseline generic model")
    parser.add_argument(
        "--baseline-models",
        default="",
        help="Optional comma-separated additional external baseline models (all no-context).",
    )
    parser.add_argument("--scenarios", default="data/proposal_eval_scenarios_large.jsonl", help="Scenario JSONL path")
    parser.add_argument("--temperature", type=float, default=0.2, help="Generation temperature")
    parser.add_argument("--max-tokens", type=int, default=96, help="Max generated tokens")
    parser.add_argument("--repeats", type=int, default=1, help="Runs per scenario per arm")
    parser.add_argument(
        "--max-scenarios",
        type=int,
        default=0,
        help="Optional cap on scenario count (0 means use all scenarios). Uses deterministic shuffled sampling.",
    )
    parser.add_argument("--seed", type=int, default=19, help="Random seed")
    parser.add_argument("--timeout-s", type=int, default=180, help="Generation timeout seconds")
    parser.add_argument("--bertscore-lang", default="en", help="Language for BERTScore")
    parser.add_argument(
        "--bertscore-model-type",
        default="",
        help="Optional HF model id for BERTScore (default: bert-score auto model for language).",
    )
    parser.add_argument(
        "--bertscore-batch-size",
        type=int,
        default=16,
        help="BERTScore batch size.",
    )
    parser.add_argument(
        "--bertscore-cache-dir",
        default="",
        help="Optional local cache dir for deterministic BERTScore model downloads.",
    )
    parser.add_argument(
        "--control-min-context-coverage",
        type=float,
        default=0.35,
        help="Minimum context keyword coverage for response control.",
    )
    parser.add_argument(
        "--control-min-persona-coverage",
        type=float,
        default=0.20,
        help="Minimum persona keyword coverage for response control.",
    )
    parser.add_argument(
        "--control-rewrite-max-tokens",
        type=int,
        default=96,
        help="Max tokens for response-control rewrite pass.",
    )
    parser.add_argument(
        "--control-rewrite-candidates",
        type=int,
        default=3,
        help="Number of rewrite candidates to sample for response control.",
    )
    parser.add_argument(
        "--control-rewrite-temperature-step",
        type=float,
        default=0.15,
        help="Temperature step used to diversify rewrite candidates.",
    )
    parser.add_argument(
        "--control-rewrite-temperature",
        type=float,
        default=0.2,
        help="Temperature for response-control rewrite pass.",
    )
    parser.add_argument(
        "--disable-control-rewrite",
        action="store_true",
        help="Disable rewrite pass in response control and use fallback directly.",
    )
    parser.add_argument(
        "--disable-control-best-effort-rewrite",
        action="store_true",
        help="Disable best-effort rewrite acceptance when strict thresholds are missed.",
    )
    parser.add_argument(
        "--target-arm",
        default="proposed_contextual_controlled",
        help="Arm used as target for external-baseline win-rate and human-eval comparisons.",
    )
    parser.add_argument(
        "--slice-keys",
        default="persona_archetype,conflict_type,location_type,behavior_state",
        help="Comma-separated scenario slice keys for per-slice reporting.",
    )
    parser.add_argument(
        "--human-eval-file",
        default="",
        help="Optional CSV/TSV/JSONL with human ratings (scenario_id, arm_id, annotator_id, metric columns).",
    )
    parser.add_argument(
        "--human-eval-metrics",
        default="context_relevance,persona_consistency,naturalness,overall_quality",
        help="Comma-separated metric columns to read from human-eval file.",
    )
    parser.add_argument(
        "--human-eval-scale-max",
        type=float,
        default=5.0,
        help="Maximum rating scale used in human-eval file when values are >1.",
    )
    parser.add_argument("--output-root", default="artifacts/proposal", help="Output root folder")
    args = parser.parse_args()

    scenarios_path = Path(args.scenarios)
    if not scenarios_path.exists():
        raise FileNotFoundError(f"Scenario file not found: {scenarios_path}")

    run_id = utc_stamp()
    run_dir = Path(args.output_root) / run_id
    metadata_dir = run_dir / "metadata"
    responses_dir = run_dir / "responses"
    scores_dir = run_dir / "scores"
    metadata_dir.mkdir(parents=True, exist_ok=True)
    responses_dir.mkdir(parents=True, exist_ok=True)
    scores_dir.mkdir(parents=True, exist_ok=True)

    scenarios = read_jsonl(scenarios_path)
    if int(args.max_scenarios) > 0 and int(args.max_scenarios) < len(scenarios):
        sampled = list(scenarios)
        rng = random.Random(args.seed + 17)
        rng.shuffle(sampled)
        scenarios = sampled[: int(args.max_scenarios)]
    scenarios_by_id = {str(row.get("scenario_id")): row for row in scenarios}
    extra_baselines = parse_list_arg(str(args.baseline_models))
    baseline_models: List[str] = []
    for model_name in [str(args.baseline_model)] + extra_baselines:
        if model_name.lower() == str(args.candidate_model).lower():
            continue
        if model_name.lower() in {m.lower() for m in baseline_models}:
            continue
        baseline_models.append(model_name)
    if not baseline_models:
        baseline_models = [str(args.baseline_model)]

    baseline_arm_ids: List[str] = []
    for idx, model_name in enumerate(baseline_models):
        if idx == 0:
            baseline_arm_ids.append("baseline_no_context")
        else:
            baseline_arm_ids.append(f"baseline_no_context_{sanitize_model_id(model_name)}")

    base_control_config = ControlConfig(
        min_context_coverage=float(args.control_min_context_coverage),
        min_persona_coverage=float(args.control_min_persona_coverage),
        rewrite_temperature=float(args.control_rewrite_temperature),
        rewrite_max_tokens=int(args.control_rewrite_max_tokens),
        rewrite_candidates=max(1, int(args.control_rewrite_candidates)),
        rewrite_temperature_step=float(args.control_rewrite_temperature_step),
        enable_rewrite=not bool(args.disable_control_rewrite),
        allow_best_effort_rewrite=not bool(args.disable_control_best_effort_rewrite),
    )

    arms = [
        EvaluationArm(
            arm_id="proposed_contextual_controlled",
            model=args.candidate_model,
            include_dynamic_context=True,
            use_response_control=True,
        ),
        EvaluationArm(
            arm_id="proposed_contextual",
            model=args.candidate_model,
            include_dynamic_context=True,
        ),
        EvaluationArm(
            arm_id="candidate_no_context",
            model=args.candidate_model,
            include_dynamic_context=False,
        ),
    ]
    for arm_id, model_name in zip(baseline_arm_ids, baseline_models):
        arms.append(
            EvaluationArm(
                arm_id=arm_id,
                model=model_name,
                include_dynamic_context=False,
            )
        )

    run_config = {
        "run_id": run_id,
        "generated_utc": utc_iso(),
        "host": args.host,
        "candidate_model": args.candidate_model,
        "baseline_model": args.baseline_model,
        "baseline_models": baseline_models,
        "baseline_arm_ids": baseline_arm_ids,
        "temperature": args.temperature,
        "max_tokens": args.max_tokens,
        "repeats": args.repeats,
        "max_scenarios": int(args.max_scenarios),
        "seed": args.seed,
        "bertscore_lang": args.bertscore_lang,
        "bertscore_model_type": str(args.bertscore_model_type),
        "bertscore_batch_size": int(args.bertscore_batch_size),
        "bertscore_cache_dir": str(args.bertscore_cache_dir),
        "target_arm": args.target_arm,
        "slice_keys": parse_list_arg(str(args.slice_keys)),
        "human_eval_file": str(args.human_eval_file),
        "human_eval_metrics": parse_list_arg(str(args.human_eval_metrics)),
        "human_eval_scale_max": float(args.human_eval_scale_max),
        "scenario_path": str(scenarios_path),
        "scenario_sha256": sha256_file(scenarios_path),
        "arms": [
            {
                "arm_id": arm.arm_id,
                "model": arm.model,
                "include_dynamic_context": arm.include_dynamic_context,
                "use_response_control": arm.use_response_control,
            }
            for arm in arms
        ],
        "response_control_config": {
            "min_context_coverage": base_control_config.min_context_coverage,
            "min_persona_coverage": base_control_config.min_persona_coverage,
            "rewrite_temperature": base_control_config.rewrite_temperature,
            "rewrite_max_tokens": base_control_config.rewrite_max_tokens,
            "rewrite_candidates": base_control_config.rewrite_candidates,
            "rewrite_temperature_step": base_control_config.rewrite_temperature_step,
            "enable_rewrite": base_control_config.enable_rewrite,
            "allow_best_effort_rewrite": base_control_config.allow_best_effort_rewrite,
        },
    }
    write_json(run_dir / "run_config.json", run_config)
    write_jsonl(run_dir / "scenarios.jsonl", scenarios)

    hardware = gather_hardware_metadata()
    write_json(metadata_dir / "hardware.json", hardware)

    model_meta: Dict[str, Any] = {}
    for model_name in sorted({arm.model for arm in arms}):
        model_meta[model_name] = query_ollama_model(args.host, model_name)
    write_json(metadata_dir / "models.json", model_meta)

    all_responses: Dict[str, List[Dict[str, Any]]] = {arm.arm_id: [] for arm in arms}
    request_index = 0

    for arm in arms:
        print(
            f"[arm] {arm.arm_id} model={arm.model} dynamic_context={arm.include_dynamic_context} "
            f"response_control={arm.use_response_control}"
        )
        for repeat_idx in range(max(1, args.repeats)):
            for scenario in scenarios:
                scenario_id = str(scenario.get("scenario_id", ""))
                request_index += 1
                prompt = build_prompt(scenario, include_dynamic_context=arm.include_dynamic_context)
                gen = generate_ollama_response(
                    host=args.host,
                    model=arm.model,
                    prompt=prompt,
                    temperature=args.temperature,
                    max_tokens=args.max_tokens,
                    timeout_s=max(3, args.timeout_s),
                )

                record = {
                    "request_index": request_index,
                    "repeat_index": repeat_idx,
                    "arm_id": arm.arm_id,
                    "model": arm.model,
                    "scenario_id": scenario_id,
                    "include_dynamic_context": arm.include_dynamic_context,
                    "use_response_control": arm.use_response_control,
                    "timestamp_utc": utc_iso(),
                    "prompt": prompt,
                    "prompt_chars": len(prompt),
                    "reference_response": scenario.get("reference_response", ""),
                }
                record.update(gen)

                raw_response = str(record.get("response", ""))
                record["raw_response"] = raw_response

                if arm.use_response_control:
                    scenario_persona = str(scenario.get("persona", ""))
                    scenario_context = str(scenario.get("dynamic_context", "")) if arm.include_dynamic_context else ""
                    scenario_input = str(scenario.get("player_input", ""))
                    context_keywords = [str(x) for x in scenario.get("context_keywords", [])] if arm.include_dynamic_context else []
                    persona_keywords = [str(x) for x in scenario.get("persona_keywords", [])]
                    control_config = ControlConfig(
                        min_context_coverage=(
                            base_control_config.min_context_coverage if arm.include_dynamic_context else 0.0
                        ),
                        min_persona_coverage=base_control_config.min_persona_coverage,
                        rewrite_temperature=base_control_config.rewrite_temperature,
                        rewrite_max_tokens=base_control_config.rewrite_max_tokens,
                        rewrite_candidates=base_control_config.rewrite_candidates,
                        rewrite_temperature_step=base_control_config.rewrite_temperature_step,
                        enable_rewrite=base_control_config.enable_rewrite,
                        allow_best_effort_rewrite=base_control_config.allow_best_effort_rewrite,
                    )

                    rewrite_meta: Dict[str, Any] = {
                        "attempts": 0,
                        "successful_attempts": 0,
                        "last_error": "",
                        "total_eval_count": 0,
                        "total_duration_ns": 0,
                    }

                    def rewrite_fn(rewrite_prompt: str, rewrite_max_tokens: int, rewrite_temperature: float) -> str:
                        rewrite_meta["attempts"] = int(rewrite_meta.get("attempts", 0) or 0) + 1
                        rewrite_gen = generate_ollama_response(
                            host=args.host,
                            model=arm.model,
                            prompt=rewrite_prompt,
                            temperature=rewrite_temperature,
                            max_tokens=rewrite_max_tokens,
                            timeout_s=max(3, args.timeout_s),
                        )
                        ok = bool(rewrite_gen.get("ok"))
                        if ok:
                            rewrite_meta["successful_attempts"] = int(
                                rewrite_meta.get("successful_attempts", 0) or 0
                            ) + 1
                        else:
                            rewrite_meta["last_error"] = str(rewrite_gen.get("error", ""))
                        rewrite_meta["total_eval_count"] = int(rewrite_meta.get("total_eval_count", 0) or 0) + int(
                            rewrite_gen.get("eval_count", 0) or 0
                        )
                        rewrite_meta["total_duration_ns"] = int(
                            rewrite_meta.get("total_duration_ns", 0) or 0
                        ) + int(rewrite_gen.get("total_duration_ns", 0) or 0)
                        return str(rewrite_gen.get("response", ""))

                    control_result = control_response(
                        raw_response=raw_response,
                        persona=scenario_persona,
                        dynamic_context=scenario_context,
                        player_input=scenario_input,
                        context_keywords=context_keywords,
                        persona_keywords=persona_keywords,
                        rewrite_fn=rewrite_fn,
                        config=control_config,
                    )

                    record["response"] = control_result.response
                    record["response_sanitized"] = (sanitize_npc_response(raw_response) != control_result.response)
                    record["response_control_source"] = control_result.source
                    record["response_repaired"] = control_result.repaired
                    record["response_repair_reason"] = control_result.repair_reason
                    record["response_context_coverage"] = control_result.context_coverage
                    record["response_persona_coverage"] = control_result.persona_coverage
                    if rewrite_meta:
                        record["rewrite_attempted"] = True
                        record["rewrite_attempts"] = int(rewrite_meta.get("attempts", 0) or 0)
                        record["rewrite_successful_attempts"] = int(
                            rewrite_meta.get("successful_attempts", 0) or 0
                        )
                        record["rewrite_ok"] = bool(record["rewrite_successful_attempts"] > 0)
                        record["rewrite_error"] = str(rewrite_meta.get("last_error", ""))
                        record["rewrite_eval_count"] = int(rewrite_meta.get("total_eval_count", 0) or 0)
                        record["rewrite_total_duration_ns"] = int(rewrite_meta.get("total_duration_ns", 0) or 0)
                    if control_result.source == "fallback":
                        record["response_fallback"] = True
                else:
                    sanitized = sanitize_npc_response(raw_response)
                    if sanitized:
                        record["response"] = sanitized
                        record["response_sanitized"] = (sanitized != raw_response)
                    else:
                        record["response"] = "I need a clearer request before I can respond in character."
                        record["response_sanitized"] = True
                        record["response_fallback"] = True

                all_responses[arm.arm_id].append(record)

                state = "ok" if record.get("ok") else "error"
                preview = str(record.get("response", ""))[:70].replace("\n", " ")
                print(f"  - scenario={scenario_id} repeat={repeat_idx + 1}/{args.repeats} status={state} preview={preview}")

    for arm in arms:
        write_jsonl(responses_dir / f"{arm.arm_id}.jsonl", all_responses[arm.arm_id])

    all_scores: Dict[str, List[Dict[str, Any]]] = {}
    summary: Dict[str, Dict[str, Any]] = {}

    bertscore_meta: Dict[str, Any] = {"available": False}
    bertscore_available_any = False

    for idx, arm in enumerate(arms):
        rows = [r for r in all_responses[arm.arm_id] if r.get("ok")]
        scored = evaluate_responses_for_arm(rows, scenarios_by_id=scenarios_by_id)

        refs = [str(row.get("reference_response", "")) for row in scored]
        hyps = [str(row.get("response", "")) for row in scored]
        bert = compute_optional_bertscore(
            hyps,
            refs,
            lang=args.bertscore_lang,
            model_type=str(args.bertscore_model_type),
            batch_size=int(args.bertscore_batch_size),
            cache_dir=str(args.bertscore_cache_dir),
        )
        if bert.get("available"):
            bertscore_meta = {
                "available": True,
                "lang": bert.get("lang", args.bertscore_lang),
                "model_type": bert.get("model_type", str(args.bertscore_model_type).strip() or "auto"),
                "batch_size": int(bert.get("batch_size", int(args.bertscore_batch_size))),
                "cache_dir": str(bert.get("cache_dir", str(args.bertscore_cache_dir))),
            }
            bertscore_available_any = True
            vals = bert.get("values", [])
            for row, f1 in zip(scored, vals):
                row["bertscore_f1"] = float(f1)
        else:
            if "reason" in bert and not bertscore_available_any:
                bertscore_meta = {"available": False, "reason": bert["reason"]}

        for row in scored:
            row["overall_quality"] = row_overall_quality(row)

        all_scores[arm.arm_id] = scored
        write_jsonl(scores_dir / f"{arm.arm_id}.jsonl", scored)
        summary[arm.arm_id] = summarize_arm_scores(
            scored_rows=scored,
            seed=args.seed + 401 * (idx + 1),
            include_bertscore=any("bertscore_f1" in row for row in scored),
        )

    write_json(run_dir / "summary.json", summary)

    available_arm_ids = {arm.arm_id for arm in arms}
    metrics_for_compare = metric_names(include_bertscore=bertscore_available_any)

    def baseline_comp_name(prefix: str, baseline_arm: str, legacy_name: str) -> str:
        if baseline_arm == "baseline_no_context":
            return legacy_name
        return f"{prefix}_vs_{baseline_arm}"

    default_target_arm = "proposed_contextual_controlled" if "proposed_contextual_controlled" in available_arm_ids else "proposed_contextual"
    target_arm_for_external = str(args.target_arm).strip()
    if target_arm_for_external not in available_arm_ids:
        target_arm_for_external = default_target_arm

    comparison_plan: List[Tuple[str, str, str]] = []
    comparison_plan.append(("proposed_vs_candidate_no_context", "proposed_contextual", "candidate_no_context"))
    for baseline_arm in baseline_arm_ids:
        comparison_plan.append(
            (
                baseline_comp_name("proposed", baseline_arm, "proposed_vs_baseline_no_context"),
                "proposed_contextual",
                baseline_arm,
            )
        )

    if "proposed_contextual_controlled" in available_arm_ids:
        comparison_plan.append(("controlled_vs_proposed_raw", "proposed_contextual_controlled", "proposed_contextual"))
        comparison_plan.append(
            ("controlled_vs_candidate_no_context", "proposed_contextual_controlled", "candidate_no_context")
        )
        for baseline_arm in baseline_arm_ids:
            comparison_plan.append(
                (
                    baseline_comp_name("controlled", baseline_arm, "controlled_vs_baseline_no_context"),
                    "proposed_contextual_controlled",
                    baseline_arm,
                )
            )

    if target_arm_for_external in available_arm_ids:
        for baseline_arm in baseline_arm_ids:
            custom_name = f"{target_arm_for_external}_vs_{baseline_arm}"
            if all(custom_name != existing[0] for existing in comparison_plan):
                comparison_plan.append((custom_name, target_arm_for_external, baseline_arm))

    deltas: Dict[str, Any] = {}
    for comp_idx, (comp_name, target_arm, baseline_arm) in enumerate(comparison_plan):
        deltas[comp_name] = diff_against_reference(
            summary,
            target_arm,
            baseline_arm,
            metrics=metrics_for_compare,
        )
    write_json(run_dir / "delta_vs_baselines.json", deltas)

    scored_by_arm = {arm.arm_id: all_scores.get(arm.arm_id, []) for arm in arms}
    paired_deltas: Dict[str, Any] = {}
    win_rates: Dict[str, Any] = {}
    for comp_idx, (comp_name, target_arm, baseline_arm) in enumerate(comparison_plan):
        paired_deltas[comp_name] = paired_metric_deltas(
            scored_by_arm,
            target_arm,
            baseline_arm,
            seed=args.seed + 5001 + 997 * (comp_idx + 1),
            metrics=metrics_for_compare,
        )
        win_rates[comp_name] = paired_metric_win_rates(
            scored_by_arm,
            target_arm,
            baseline_arm,
            seed=args.seed + 8001 + 991 * (comp_idx + 1),
            metrics=metrics_for_compare,
        )
    write_json(run_dir / "paired_delta_significance.json", paired_deltas)
    write_json(run_dir / "win_rates.json", win_rates)

    slice_keys = parse_list_arg(str(args.slice_keys))
    slice_summary = summarize_scores_by_slice(
        scored_by_arm=scored_by_arm,
        scenarios_by_id=scenarios_by_id,
        slice_keys=slice_keys,
        seed=args.seed + 12001,
        include_bertscore=bertscore_available_any,
    )
    write_json(run_dir / "slice_summary.json", slice_summary)

    human_eval_summary: Optional[Dict[str, Any]] = None
    human_eval_path = Path(str(args.human_eval_file).strip()) if str(args.human_eval_file).strip() else None
    if human_eval_path:
        if not human_eval_path.exists():
            raise FileNotFoundError(f"Human evaluation file not found: {human_eval_path}")
        human_rows = read_human_eval_rows(human_eval_path)
        human_metrics = parse_list_arg(str(args.human_eval_metrics))
        human_eval_summary = analyze_human_eval(
            rows=human_rows,
            metrics=human_metrics,
            scale_max=float(args.human_eval_scale_max),
            target_arm=target_arm_for_external,
            baseline_arms=baseline_arm_ids,
            seed=args.seed + 14001,
        )
        write_json(run_dir / "human_eval_summary.json", human_eval_summary)
        render_human_eval_report(run_dir / "human_eval_report.md", human_eval_summary)

    error_analysis: Dict[str, Any] = {}
    for arm in arms:
        error_analysis[arm.arm_id] = analyze_errors(all_scores.get(arm.arm_id, []))
    write_json(run_dir / "error_analysis.json", error_analysis)

    write_json(
        run_dir / "comparison_plan.json",
        {
            "target_arm_for_external": target_arm_for_external,
            "baseline_arm_ids": baseline_arm_ids,
            "comparisons": [
                {"comparison_id": comp_name, "target_arm": target_arm, "baseline_arm": baseline_arm}
                for comp_name, target_arm, baseline_arm in comparison_plan
            ],
            "metrics": metrics_for_compare,
        },
    )

    render_report(
        output_path=run_dir / "report.md",
        run_id=run_id,
        scenarios_path=Path(run_dir / "scenarios.jsonl"),
        arms=arms,
        summary=summary,
        deltas=deltas,
        paired_deltas=paired_deltas,
        bertscore_meta=bertscore_meta,
        win_rates=win_rates,
        slice_summary=slice_summary,
        human_eval=human_eval_summary,
    )

    print(f"\nPublished proposal artifact bundle: {run_dir}")
    print("Generated files:")
    print(f"  - {run_dir / 'summary.json'}")
    print(f"  - {run_dir / 'delta_vs_baselines.json'}")
    print(f"  - {run_dir / 'paired_delta_significance.json'}")
    print(f"  - {run_dir / 'win_rates.json'}")
    print(f"  - {run_dir / 'slice_summary.json'}")
    print(f"  - {run_dir / 'error_analysis.json'}")
    if human_eval_summary is not None:
        print(f"  - {run_dir / 'human_eval_summary.json'}")
        print(f"  - {run_dir / 'human_eval_report.md'}")
    print(f"  - {run_dir / 'report.md'}")


if __name__ == "__main__":
    main()
