#pragma once

#include "CoreMinimal.h"
#include "Subsystems/GameInstanceSubsystem.h"
#include "NPCInference.h" // C++ Engine Header
#include "NPCInferenceSubsystem.generated.h"

DECLARE_DYNAMIC_DELEGATE_OneParam(FOnDialogueGenerated, const FString&, Response);

/**
 * Subsystem to manage the lifecycle of the NPC Inference Engine
 */
UCLASS()
class NPCINFERENCE_API UNPCInferenceSubsystem : public UGameInstanceSubsystem
{
	GENERATED_BODY()

public:
	// Begin USubsystem
	virtual void Initialize(FSubsystemCollectionBase& Collection) override;
	virtual void Deinitialize() override;
	// End USubsystem

	/**
	 * Initialize the engine with a specific model path
	 * @param ModelPath Absolute path to the ONNX model directory or tokenizer file location
	 * @return true if successful
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	bool InitializeEngine(const FString& ModelPath);

	/**
	 * Generate a response for an NPC
	 * @param SystemPrompt The persona/system instruction
	 * @param Name NPC Name
	 * @param Context The scenario or context
	 * @param PlayerInput What the player said
	 * @return The generated response
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	FString GenerateDialogue(const FString& SystemPrompt, const FString& Name, const FString& Context, const FString& PlayerInput);

	/**
	 * Native generation wrapper
	 */
	FString GenerateFromPrompt(const FString& FullPrompt);

	/** Check if engine is ready */
	UFUNCTION(BlueprintPure, Category = "NPC Inference")
	bool IsEngineReady() const;

	/**
	 * Inject a gossip memory into the system
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	void ReceiveGossip(const FString& GossipText, const FString& SourceNPC);

	/**
	 * Extract interesting gossip/memory from the vector store
	 */
	UFUNCTION(BlueprintPure, Category = "NPC Inference")
	FString ExtractGossip();

	/**
	 * Trigger sleep cycle to consolidate memories
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	void TriggerSleepMode();
    
	/**
	 * Generate a structured (JSON) response using grammar constraints
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	FString GenerateStructuredDialogue(const FString& SystemPrompt, const FString& Name, const FString& Context, const FString& PlayerInput);

	/**
	 * Async version of GenerateDialogue to prevent Game Thread freeze
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	void GenerateDialogueAsync(const FString& SystemPrompt, const FString& Name, const FString& Context, const FString& PlayerInput, FOnDialogueGenerated OnComplete);

	/**
	 * Streaming version of GenerateDialogue that fires callbacks when actions are generated
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	void GenerateDialogueStreamAsync(const FString& SystemPrompt, const FString& Name, const FString& Context, const FString& PlayerInput, FOnDialogueGenerated OnActionChunk, FOnDialogueGenerated OnComplete);

	/**
	 * Phase 7: Zero-Latency Local Streaming version. Bypasses Ollama HTTP sockets entirely.
	 * Shares memory with C++ Engine directly for instant Time-To-First-Token.
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	void GenerateDialogueLocalStreamAsync(const FString& SystemPrompt, const FString& Name, const FString& Context, const FString& PlayerInput, FOnDialogueGenerated OnTokenStream, FOnDialogueGenerated OnActionChunk, FOnDialogueGenerated OnComplete);

	/**
	 * Immediately abort any ongoing generation processes (Native or API)
	 * @param ConversationID Optimal phase 8 mult-agent target ID. Leave empty to cancel all.
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	void CancelGeneration(const FString& ConversationID = TEXT(""));

	/**
	 * Execute a tool call generated by the NPC
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	FString ExecuteNPCTool(const FString& ToolCallJSON);

	/**
	 * Analyze visual scene data
	 */
	UFUNCTION(BlueprintCallable, Category = "NPC Inference")
	FString AnalyzeScene(const TArray<uint8>& ImageData, int32 Width, int32 Height);

private:
	// Pimpl idiom or direct member if header is available
	// Since we include NPCInference.h, we can use the class directly or via pointer
	// Using pointer to avoid strict dependency in header if possible, but here we included it.
	std::unique_ptr<NPCInference::NPCInferenceEngine> InferenceEngine;

	// Helper to convert FString to std::string
	std::string ToString(const FString& InStr);
	// Helper to convert std::string to FString
	FString ToFString(const std::string& InStr);
};
