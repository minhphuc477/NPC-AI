{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BD-NSCA Multi-Turn Training on Kaggle\n",
                "\n",
                "This notebook trains the Phi-3-mini model with QLoRA on Kaggle's free GPU (P100/T4).\n",
                "\n",
                "## Setup Instructions:\n",
                "1. Upload `train_multiturn.jsonl` to Kaggle as a dataset\n",
                "2. Enable GPU: Settings → Accelerator → GPU P100 or T4 x2\n",
                "3. Enable Internet: Settings → Internet → On\n",
                "4. Run all cells"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers accelerate peft trl bitsandbytes datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration - adjust these as needed\n",
                "CONFIG = {\n",
                "    # Data path - UPDATE THIS to match your Kaggle dataset path\n",
                "    \"data_path\": \"/kaggle/input/npc-ai-training-data/train_multiturn.jsonl\",\n",
                "    \n",
                "    # Model\n",
                "    \"base_model\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
                "    \n",
                "    # Training\n",
                "    \"num_epochs\": 3,\n",
                "    \"batch_size\": 4,  # Can use larger batch on P100 (16GB VRAM)\n",
                "    \"grad_accum\": 4,\n",
                "    \"learning_rate\": 2e-4,\n",
                "    \"max_seq_length\": 2048,\n",
                "    \n",
                "    # LoRA\n",
                "    \"lora_r\": 16,\n",
                "    \"lora_alpha\": 32,\n",
                "    \"lora_dropout\": 0.05,\n",
                "    \n",
                "    # Output\n",
                "    \"output_dir\": \"/kaggle/working/adapter_multiturn\",\n",
                "}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import LoraConfig, prepare_model_for_kbit_training\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from datasets import Dataset\n",
                "\n",
                "# Load dataset\n",
                "def load_dataset(path):\n",
                "    samples = []\n",
                "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
                "        for line in f:\n",
                "            if line.strip():\n",
                "                samples.append(json.loads(line))\n",
                "    print(f\"Loaded {len(samples)} samples\")\n",
                "    return samples\n",
                "\n",
                "raw_samples = load_dataset(CONFIG[\"data_path\"])\n",
                "formatted = [{\"text\": s.get(\"text\", s.get(\"prompt\", \"\") + s.get(\"completion\", \"\"))} for s in raw_samples]\n",
                "dataset = Dataset.from_list(formatted)\n",
                "print(dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model with 4-bit quantization\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=False,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    CONFIG[\"base_model\"],\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                "    attn_implementation=\"eager\",\n",
                ")\n",
                "model.config.use_cache = False\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"base_model\"], trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "print(\"Model loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configure LoRA\n",
                "peft_config = LoraConfig(\n",
                "    lora_alpha=CONFIG[\"lora_alpha\"],\n",
                "    lora_dropout=CONFIG[\"lora_dropout\"],\n",
                "    r=CONFIG[\"lora_r\"],\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                ")\n",
                "\n",
                "# Configure training\n",
                "sft_config = SFTConfig(\n",
                "    output_dir=CONFIG[\"output_dir\"],\n",
                "    num_train_epochs=CONFIG[\"num_epochs\"],\n",
                "    per_device_train_batch_size=CONFIG[\"batch_size\"],\n",
                "    gradient_accumulation_steps=CONFIG[\"grad_accum\"],\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    save_steps=50,\n",
                "    logging_steps=10,\n",
                "    learning_rate=CONFIG[\"learning_rate\"],\n",
                "    fp16=True,  # Use fp16 on Kaggle GPUs\n",
                "    max_grad_norm=0.3,\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=True,\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    report_to=\"none\",\n",
                "    max_length=CONFIG[\"max_seq_length\"],\n",
                "    dataset_text_field=\"text\",\n",
                ")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=peft_config,\n",
                "    processing_class=tokenizer,\n",
                "    args=sft_config,\n",
                "    formatting_func=lambda x: x[\"text\"],\n",
                ")\n",
                "\n",
                "print(\"Trainer configured!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train!\n",
                "print(\"Starting training...\")\n",
                "trainer.train()\n",
                "print(\"Training complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the adapter\n",
                "trainer.model.save_pretrained(CONFIG[\"output_dir\"])\n",
                "tokenizer.save_pretrained(CONFIG[\"output_dir\"])\n",
                "print(f\"Adapter saved to {CONFIG['output_dir']}\")\n",
                "\n",
                "# List saved files\n",
                "import os\n",
                "for f in os.listdir(CONFIG[\"output_dir\"]):\n",
                "    size = os.path.getsize(os.path.join(CONFIG[\"output_dir\"], f)) / 1e6\n",
                "    print(f\"  {f}: {size:.2f} MB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download the adapter (creates a zip file)\n",
                "import shutil\n",
                "shutil.make_archive(\"/kaggle/working/adapter_multiturn\", \"zip\", CONFIG[\"output_dir\"])\n",
                "print(\"Download adapter_multiturn.zip from the Output tab!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Quick Test (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the trained model\n",
                "from peft import PeftModel\n",
                "\n",
                "test_prompt = \"\"\"System: You are friendly NPC villager. You share rumors and help adventurers.\n",
                "Question: Have you heard any rumors lately?\n",
                "Answer:\"\"\"\n",
                "\n",
                "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(model.device)\n",
                "outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, temperature=0.7)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}