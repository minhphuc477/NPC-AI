{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† NPC AI ‚Äî Complete Training & Integration Pipeline\n",
    "\n",
    "**BD-NSCA: Behavior-Driven Neuro-Symbolic Cognitive Architecture**\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| 1 | Environment Setup |\n",
    "| 2 | Training Data Generation |\n",
    "| 3 | QLoRA Fine-Tuning (checkpoint/resume) |\n",
    "| 4 | GGUF Export |\n",
    "| 5 | Ollama Serving |\n",
    "| 6 | Integrated Demo |\n",
    "| 7 | Quality Evaluation |\n",
    "| 8 | C++ Engine Compilation |\n",
    "\n",
    "> **Checkpoint/Resume**: Training auto-detects and resumes from existing checkpoints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. üîß Environment Setup & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d0674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Environment Setup (Auto-Clone & Install)\n",
    "# ============================================================\n",
    "import os, sys, subprocess, shutil\n",
    "\n",
    "IN_KAGGLE = os.path.exists('/kaggle')\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "ENV_NAME = 'Kaggle' if IN_KAGGLE else ('Colab' if IN_COLAB else 'Local')\n",
    "print(f'üåç Environment: {ENV_NAME}')\n",
    "\n",
    "if IN_KAGGLE:\n",
    "    if not os.path.exists('NPC-AI'):\n",
    "        print('üì• Cloning NPC-AI repository...')\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/minhphuc477/NPC-AI.git'], check=True)\n",
    "    \n",
    "    for folder in ['cpp', 'data']:\n",
    "        src = f'NPC-AI/{folder}'\n",
    "        if os.path.exists(src):\n",
    "            if not os.path.exists(folder):\n",
    "                print(f'üìÇ Cloning {folder} to root...')\n",
    "                shutil.copytree(src, folder)\n",
    "            else:\n",
    "                for item in os.listdir(src):\n",
    "                    s, d = os.path.join(src, item), os.path.join(folder, item)\n",
    "                    if not os.path.exists(d):\n",
    "                        if os.path.isdir(s): shutil.copytree(s, d)\n",
    "                        else: shutil.copy2(s, d)\n",
    "\n",
    "if IN_KAGGLE or IN_COLAB:\n",
    "    print('üì¶ Installing Unsloth and dependencies...')\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git'])\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '--no-deps', 'trl>=0.12.0', 'peft>=0.7.1', 'accelerate>=0.26.0', 'bitsandbytes>=0.40.0'])\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'transformers>=4.45.0', 'datasets', 'sentencepiece', 'protobuf'])\n",
    "    \n",
    "    print('üì¶ Installing Ollama...')\n",
    "    try:\n",
    "        subprocess.run(['apt-get', 'update'], check=True, capture_output=True)\n",
    "        subprocess.run(['apt-get', 'install', '-y', 'zstd'], check=True, capture_output=True)\n",
    "        os.system('curl -fsSL https://ollama.com/install.sh | sh')\n",
    "        if shutil.which('ollama'): print('‚úÖ Ollama installed successfully!')\n",
    "    except Exception as e: print(f'‚ùå Failed to install Ollama: {e}')\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f'üéÆ GPU: {torch.cuda.get_device_name(0)}')\n",
    "else: print('‚ö†Ô∏è  No GPU detected!')\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "cpp_patch_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Step 1.5: Patching C++ Engine for Linux compatibility\n",
    "# ============================================================\n",
    "import os, textwrap, re\n",
    "print('üõ†Ô∏è Patching C++ engine code for Linux...')\n",
    "\n",
    "def apply_patch(filepath, old_str, new_str):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f'‚ö†Ô∏è File not found: {filepath}')\n",
    "        return\n",
    "    with open(filepath, 'r', encoding='utf-8') as f: content = f.read()\n",
    "    if old_str in content:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f: f.write(content.replace(old_str, new_str))\n",
    "        print(f'‚úÖ Patched {filepath}')\n",
    "    else: print(f'‚òëÔ∏è No patch needed for {filepath}')\n",
    "\n",
    "def prepend_to_file(filepath, prepend_str, guard=None):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f'‚ö†Ô∏è File not found: {filepath}')\n",
    "        return\n",
    "    with open(filepath, 'r', encoding='utf-8') as f: content = f.read()\n",
    "    if guard and guard in content:\n",
    "        print(f'‚òëÔ∏è Already has headers: {filepath}')\n",
    "        return\n",
    "    with open(filepath, 'w', encoding='utf-8') as f: f.write(prepend_str + content)\n",
    "    print(f'‚úÖ Prepended to {filepath}')\n",
    "\n",
    "# Fix 1: PythonBridge.cpp ‚Äî Linux process management headers\n",
    "prepend_to_file('cpp/src/PythonBridge.cpp', \n",
    "    '#include <thread>\\n#include <chrono>\\n#ifdef _WIN32\\n#include <windows.h>\\n#else\\n#include <unistd.h>\\n#include <sys/types.h>\\n#include <sys/wait.h>\\n#include <fcntl.h>\\n#include <signal.h>\\n#endif\\n', \n",
    "    guard='unistd.h')\n",
    "\n",
    "# Fix 2: NPCInference.cpp ‚Äî missing thread/chrono\n",
    "prepend_to_file('cpp/src/NPCInference.cpp', '#include <thread>\\n#include <chrono>\\n', guard='<thread>')\n",
    "\n",
    "# Fix 3: HybridRetriever.h ‚Äî signature fix\n",
    "apply_patch('cpp/include/HybridRetriever.h', \n",
    "    'Search(const std::string& query, const RetrievalConfig& config = {});', \n",
    "    'Search(const std::string& query);\\n    std::vector<RetrievalResult> Search(const std::string& query, const RetrievalConfig& config);')\n",
    "\n",
    "# Fix 4: PromptBuilder.cpp ‚Äî Align with [NPC]/[PLAYER] format\n",
    "pb_new_source = textwrap.dedent(\"\"\"\n",
    "    std::string PromptBuilder::BuildAdvanced(const json& npcData, const json& gameState, const std::string& playerInput, const std::string& language, const json& tools) {\n",
    "        bool isVi = (language == \"vi\");\n",
    "        std::stringstream ss;\n",
    "        std::string persona = npcData.value(isVi ? \"persona_vi\" : \"persona_en\", npcData.value(\"persona\", isVi ? \"B·∫°n l√† m·ªôt NPC.\" : \"You are an NPC.\"));\n",
    "        ss << \"[INSTRUCTION] \" << (isVi ? \"Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát. \" : \"Respond strictly in English. \") << persona << \"\\\\n\";\n",
    "        if (!tools.is_null() && !tools.empty()) ss << (isVi ? \"C√¥ng c·ª• kh·∫£ d·ª•ng: \" : \"Available tools: \") << tools.dump() << \"\\\\n\";\n",
    "        ss << \"\\\\n[CONTEXT]\\\\n\";\n",
    "        json cog; cog[\"npc_info\"] = {{\"name\", npcData.value(\"name\", \"NPC\")}, {\"mood\", gameState.value(\"mood_state\", \"Neutral\")}, {\"health\", gameState.value(\"health_state\", \"Healthy\")}};\n",
    "        if (gameState.contains(\"current_emotion\")) cog[\"current_emotion\"] = gameState[\"current_emotion\"];\n",
    "        if (gameState.contains(\"memories\")) cog[\"memories\"] = gameState[\"memories\"];\n",
    "        if (gameState.contains(\"relationships\")) cog[\"relationships\"] = gameState[\"relationships\"];\n",
    "        if (gameState.contains(\"knowledge\")) cog[\"knowledge\"] = gameState[\"knowledge\"];\n",
    "        if (gameState.contains(\"recent_history\")) cog[\"recent_dialogue\"] = gameState[\"recent_history\"];\n",
    "        if (gameState.contains(\"memory_context\") && !gameState[\"memory_context\"].get<std::string>().empty()) cog[\"historical_memories\"] = gameState[\"memory_context\"];\n",
    "        ss << cog.dump() << \"\\\\n\\\\n[PLAYER] \" << playerInput << \"\\\\n[NPC] \";\n",
    "        return ss.str();\n",
    "    }\n",
    "\"\"\")\n",
    "with open('cpp/src/PromptBuilder.cpp', 'r', encoding='utf-8') as f: pb_content = f.read()\n",
    "pb_content = re.sub(r'std::string PromptBuilder::BuildAdvanced.*?\\n\\s+\\}', pb_new_source, pb_content, flags=re.DOTALL)\n",
    "with open('cpp/src/PromptBuilder.cpp', 'w', encoding='utf-8') as f: f.write(pb_content)\n",
    "print('‚úÖ Overwrote PromptBuilder::BuildAdvanced')\n",
    "\n",
    "# Fix 5: NPCInference.cpp ‚Äî Wire BuildAdvancedContext into Chat() with shared_ptr\n",
    "chat_new_source = textwrap.dedent(\"\"\"\n",
    "    std::string NPCInferenceEngine::Chat(const std::string& session_id, const std::string& user_message) {\n",
    "        if (!conversation_manager_) return \"Error: No conversation manager\";\n",
    "        \n",
    "        auto ctx = conversation_manager_->GetSession(session_id); // Now returns shared_ptr\n",
    "        if (!ctx) return \"Error: Invalid session ID\";\n",
    "        \n",
    "        conversation_manager_->AddMessage(session_id, \"user\", user_message);\n",
    "        json advanced_context = BuildAdvancedContext(ctx->npc_name, user_message);\n",
    "        \n",
    "        std::string history_str = \"\";\n",
    "        auto history = conversation_manager_->GetHistory(session_id, 6);\n",
    "        for (const auto& msg : history) {\n",
    "            history_str += (msg.role == \"user\" ? ctx->player_name : ctx->npc_name) + \": \" + msg.content + \"\\\\n\";\n",
    "        }\n",
    "        \n",
    "        advanced_context[\"recent_history\"] = history_str;\n",
    "        advanced_context[\"npc_id\"] = ctx->npc_name;\n",
    "        advanced_context[\"player_id\"] = ctx->player_name;\n",
    "        advanced_context[\"conversation_id\"] = session_id;\n",
    "        \n",
    "        std::string response = GenerateWithState(user_message, advanced_context, false);\n",
    "        conversation_manager_->AddMessage(session_id, \"assistant\", response);\n",
    "        \n",
    "        if (config_.enable_graph) Learn(user_message);\n",
    "        return response;\n",
    "    }\n",
    "\"\"\")\n",
    "with open('cpp/src/NPCInference.cpp', 'r', encoding='utf-8') as f: ni_content = f.read()\n",
    "ni_content = re.sub(r'std::string NPCInferenceEngine::Chat.*?\\n\\s+\\}', chat_new_source, ni_content, flags=re.DOTALL)\n",
    "with open('cpp/src/NPCInference.cpp', 'w', encoding='utf-8') as f: f.write(ni_content)\n",
    "print('‚úÖ Overwrote NPCInferenceEngine::Chat')\n",
    "\n",
    "print('üéâ C++ patching complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97efb198",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. üìù Training Data Generation (Enhanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd29124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Training Data Generation (Refined English)\n",
    "# ============================================================\n",
    "import json, random, os\n",
    "os.makedirs('data', exist_ok=True)\n",
    "PERSONAS_PATH = 'data/personas.json'\n",
    "UTTERANCES_PATH = 'data/player_utterances.json'\n",
    "OUTPUT_PATH = 'data/npc_training_v2.json'\n",
    "\n",
    "if os.path.exists(PERSONAS_PATH):\n",
    "    with open(PERSONAS_PATH, 'r', encoding='utf-8') as f: personas = json.load(f)\n",
    "else: personas = {'merchant': {'persona_en': 'You are a Merchant.', 'traits': ['friendly'], 'id': 'merchant'}}\n",
    "\n",
    "if os.path.exists(UTTERANCES_PATH):\n",
    "    with open(UTTERANCES_PATH, 'r', encoding='utf-8') as f: utterances = json.load(f)\n",
    "else: utterances = {'greetings': {'en': ['Hello!']}}\n",
    "\n",
    "def generate_heuristic_response(persona, category, player_input):\n",
    "    name = persona.get('id', 'NPC').replace('npc_', '').capitalize()\n",
    "    traits = persona.get('traits', [])\n",
    "    trait_str = random.choice(traits) if traits else 'friendly'\n",
    "    templates = [\n",
    "        lambda: f\"{name} looks at you with a {trait_str} expression. 'Welcome, traveler. What brings you here?'\",\n",
    "        lambda: f\"'Ah, a new face!' {name} exclaims. 'I hope your journey was smoother than mine.'\",\n",
    "        lambda: f\"{name} pauses for a moment. 'I have much to share, but tell me, what is your business in this village?'\",\n",
    "        lambda: f\"The {name} nods slowly. 'Greetings. I am here to help, if you have the coin.'\"\n    ]\n",
    "    if category == 'greetings': return random.choice(templates)()\n",
    "    elif category == 'trade_related': return f\"{name} eyes your gear carefully. 'I deal in quality only. Are you buying or just looking?'\"\n",
    "    return f\"{name} considers your words deeply. 'Interesting... {player_input} is not something I hear every day.'\"\n",
    "\n",
    "dataset = []\n",
    "persona_list = list(personas.values())\n",
    "categories = list(utterances.keys())\n",
    "for _ in range(1200):\n",
    "    p = random.choice(persona_list)\n",
    "    c = random.choice(categories)\n",
    "    q = random.choice(utterances[c].get('en', ['Hello']))\n",
    "    a = generate_heuristic_response(p, c, q)\n",
    "    ctx = {'memories': [], 'current_emotion': {'description': 'neutral', 'valence': 0.0}, 'knowledge': [], 'npc_info': {'name': p.get('id', 'NPC'), 'persona': p.get('persona_en', '')}}\n",
    "    prompt = \"[INSTRUCTION] Respond strictly in English.\\n[CONTEXT]\\n\" + json.dumps(ctx, ensure_ascii=False) + \"\\n\\n[PLAYER] \" + q + \"\\n\\n[NPC] \"\n",
    "    dataset.append({'prompt': prompt, 'completion': a})\n",
    "\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f: json.dump(dataset, f, indent=1, ensure_ascii=False)\n",
    "print(f'‚úÖ Generated {len(dataset)} REFINED training samples at {OUTPUT_PATH}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "write_train_script",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Write Standalone Training Script (120 Steps)\n",
    "# ============================================================\n",
    "script_content = \"\"\"import torch, argparse, os, shutil, glob, gc\nimport torch.distributed as dist\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "import json\n",
    "\n",
    "def train(dataset_path, output_dir):\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = 'unsloth/llama-3-8b-instruct-bnb-4bit', max_seq_length = 2048, load_in_4bit = True\n",
    "    )\n",
    "    model = FastLanguageModel.get_peft_model(model, r = 16, target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'], lora_alpha = 16, lora_dropout = 0, bias = 'none')\n",
    "    \n",
    "    with open(dataset_path, 'r') as f: data = json.load(f)\n",
    "    dataset = Dataset.from_list([{'text': d['prompt'] + d['completion'] + '<|end|>'} for d in data])\n",
    "\n",
    "    resume = os.path.exists(output_dir) and len(os.listdir(output_dir)) > 0\n",
    "    trainer = SFTTrainer(\n",
    "        model = model, tokenizer = tokenizer, train_dataset = dataset, dataset_text_field = 'text',\n",
    "        max_seq_length = 2048, args = SFTConfig(\n",
    "            per_device_train_batch_size = 2, gradient_accumulation_steps = 4, warmup_steps = 10,\n",
    "            max_steps = 120, learning_rate = 2e-4, save_total_limit = 1, save_steps = 9999, logging_steps = 1, optim = 'adamw_8bit',\n",
    "            output_dir = output_dir, report_to = 'none', ddp_find_unused_parameters=False, gradient_checkpointing_kwargs={'use_reentrant': False}, fp16 = not torch.cuda.is_bf16_supported(), bf16 = torch.cuda.is_bf16_supported(),\n",
    "        ),\n",
    "    )\n",
    "    print('üöÄ Training starting (120 steps)...')\n",
    "    trainer.train(resume_from_checkpoint = resume)\n",
    "\n    # ============================================================\n",
    "    # ULTRA-ROBUST EXPORT (Rank 0 only)\n",
    "    # ============================================================\n",
    "    if dist.is_initialized(): dist.barrier()\n",
    "    if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "        print('üßπ Aggressive cleanup started...')\n",
    "        # A. Clear HF Cache (Reclaims 10GB+)\n",
    "        hf_cache = os.path.expanduser('~/.cache/huggingface')\n",
    "        if os.path.exists(hf_cache):\n",
    "            shutil.rmtree(hf_cache, ignore_errors=True)\n",
    "            print('‚úÖ HF Cache cleared.')\n",
    "\n",
    "        # B. Clear Checkpoints\n",
    "        for ckpt in glob.glob(os.path.join(output_dir, 'checkpoint-*')):\n",
    "            shutil.rmtree(ckpt, ignore_errors=True)\n",
    "        print('‚úÖ Checkpoints cleared.')\n",
    "\n",
    "        # C. Clear VRAM and RAM\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print('üì¶ Exporting to GGUF (q4_k_m) in /tmp...')\n",
    "        temp_export = '/tmp/model_export'\n",
    "        os.makedirs(temp_export, exist_ok=True)\n",
    "        # maximum_memory_usage=0.55 helps prevent OOM during GGUF conversion\n",
    "        model.save_pretrained_gguf(temp_export, tokenizer, quantization_method=\"q4_k_m\", maximum_memory_usage=0.55)\n",
    "\n",
    "        print('üöö Moving GGUF back to working directory...')\n",
    "        # Find the GGUF whether it's inside the dir or uses it as a prefix\n",
    "        for f in glob.glob(f'{temp_export}*.gguf') + glob.glob(f'{temp_export}/**/*.gguf', recursive=True):\n",
    "            shutil.move(f, os.path.join('/kaggle/working', os.path.basename(f)))\n",
    "        print('‚úÖ GGUF move complete.')\n",
    "        \n",
    "        # Final verification\n",
    "        ggufs = glob.glob('/kaggle/working/*.gguf')\n",
    "        if ggufs:\n",
    "            print(f'üî• SUCCESS: Exported {os.path.basename(ggufs[0])}')\n",
    "\n    # Synchronize and Export (Rank 0 only)\n",
    "    if dist.is_initialized(): dist.barrier()\n",
    "    if not dist.is_initialized() or dist.get_rank() == 0:\n",
    "        for ckpt in glob.glob(os.path.join(output_dir, 'checkpoint-*')):\n",
    "            shutil.rmtree(ckpt, ignore_errors=True)\n",
    "\n",
    "        # Export to /tmp to avoid 20GB disk limit in /kaggle/working\n",
    "        os.makedirs(temp_export, exist_ok=True)\n",
    "\n",
    "        print('üöö Moving GGUF back to working directory...')\n",
    "        # Find the GGUF whether it's inside the dir or uses it as a prefix\n",
    "        for f in glob.glob(f'{temp_export}*.gguf') + glob.glob(f'{temp_export}/**/*.gguf', recursive=True):\n",
    "            shutil.move(f, os.path.join('/kaggle/working', os.path.basename(f)))\n",
    "        print(f'‚úÖ Export complete. Final model in /kaggle/working/')\n",
    "        print(f'‚úÖ Model saved to {output_dir}')\n",
    "        if dist.is_initialized(): dist.barrier()\n",
    "        print('‚úÖ All processes finished successfully.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', type=str, default='data/npc_training_v2.json')\n",
    "    parser.add_argument('--output_dir', type=str, default='outputs/npc_model')\n",
    "    args = parser.parse_args()\n",
    "    train(args.dataset, args.output_dir)\n",
    "\"\"\"\n",
    "os.makedirs('scripts', exist_ok=True)\n",
    "with open('scripts/train_unsloth.py', 'w') as f: f.write(script_content)\n",
    "print('‚úÖ Standalone training script written (120 steps)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7265a9",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. üöÄ QLoRA Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3d2e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Execute Fine-tuning (VRAM Safe)\n",
    "# ============================================================\n",
    "import subprocess, sys, os, torch, gc\n",
    "print('üöÄ Starting fine-tuning...')\n",
    "# Call standalone script to keep main notebook process clean\n",
    "subprocess.check_call(['accelerate', 'launch', '--num_processes', '2', 'scripts/train_unsloth.py', \n",
    "                    '--dataset', 'data/npc_training_v2.json',\n",
    "                    '--output_dir', 'outputs/npc_model'])\n",
    "\n",
    "# VRAM cleanup after subprocess exits\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "print('‚úÖ Fine-tuning complete and VRAM cleared.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71391431",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. üì¶ GGUF Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: GGUF Export Status\n",
    "# ============================================================\n",
    "import os, glob\n",
    "gguf_files = glob.glob('*.gguf') + glob.glob('**/*.gguf', recursive=True)\n",
    "if gguf_files:\n",
    "    trained_model_path = gguf_files[0]\n",
    "    print(f'‚úÖ GGUF found: {trained_model_path}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è GGUF not found. Check training logs.')\n",
    "    trained_model_path = 'unsloth/llama-3-8b-instruct-gguf'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d57862b",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ü§ñ Ollama Serving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d2a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Ollama Serving (Safe Register & Stop Tokens)\n",
    "# ============================================================\n",
    "import subprocess, time, requests, os, glob\n",
    "print('üöÄ Starting Ollama server...')\n",
    "try:\n",
    "    if requests.get('http://localhost:11434/api/tags', timeout=1).status_code == 200:\n",
    "        print('‚úÖ Ollama is ALREADY running.')\n",
    "    else: raise Exception('Not running')\n",
    "except:\n",
    "    subprocess.Popen(['ollama', 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    time.sleep(5)\n",
    "\n",
    "tm_path = globals().get('trained_model_path')\n",
    "if not tm_path or not os.path.exists(tm_path):\n",
    "    candidates = glob.glob('model_gguf/*.gguf') + glob.glob('*.gguf') + glob.glob('outputs/*.gguf')\n",
    "    if candidates: tm_path = candidates[0]\n",
    "\n",
    "if tm_path and os.path.exists(tm_path):\n",
    "    lines = [\n",
    "        f'FROM {tm_path}', 'PARAMETER temperature 0.7',\n",
    "        'PARAMETER stop \"[PLAYER]\"', 'PARAMETER stop \"[INSTRUCTION]\"', 'PARAMETER stop \"[CONTEXT]\"', 'PARAMETER stop \"<|end|>\"',\n",
    "        'SYSTEM \"You are an NPC. Always respond strictly in English as the [NPC] speaker. Do not repeat the prompt.\"'\n",
    "    ]\n",
    "    with open('Modelfile', 'w') as f: f.write('\\n'.join(lines))\n",
    "    print(f'üì¶ Registering model npc-ai from {tm_path}...')\n",
    "    subprocess.run(['ollama', 'create', 'npc-ai', '-f', 'Modelfile'])\n",
    "else: print('‚ùå Model file NOT FOUND.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29ce908",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. üéÆ Integrated Demo (Enhanced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a3c4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Integrated Demo (Clean Turns)\n",
    "# ============================================================\n",
    "import json, requests\n",
    "def query_npc(player_input):\n",
    "    ctx = {'memories': [], 'current_emotion': {'description': 'neutral', 'valence': 0.0}, 'npc_info': {'name': 'Blacksmith', 'persona': 'A friendly blacksmith.'}}\n",
    "    prompt = \"[INSTRUCTION] Respond strictly in English.\\n[CONTEXT]\\n\" + json.dumps(ctx) + \"\\n\\n[PLAYER] \" + player_input + \"\\n\\n[NPC] \"\n",
    "    try:\n",
    "        payload = {\"model\": \"npc-ai\", \"prompt\": prompt, \"stream\": False, \"options\": {\"stop\": [\"[PLAYER]\", \"[INSTRUCTION]\", \"<|end|>\"]}}\n",
    "        res = requests.post(\"http://localhost:11434/api/generate\", json=payload, timeout=60)\n",
    "        if res.status_code == 200:\n",
    "            text = res.json().get('response', '[No response]')\n",
    "            return text.split('[NPC]')[-1].strip()\n",
    "        return f\"[Error {res.status_code}]\"\n",
    "    except Exception as e: return f\"[Error: {e}]\"\n",
    "\n",
    "for inp in [\"Hello! I am new here.\", \"What is the curse?\"]:\n",
    "    print(f\"üë§ Player: {inp}\\nü§ñ NPC: {query_npc(inp)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289aff3",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. üìä Quality Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üìä Evaluating responses...')\n",
    "# Simplified evaluation loop\n",
    "test_queries = [\"Hello!\", \"Who are you?\", \"Tell me a story.\"]\n",
    "for q in test_queries:\n",
    "    resp = query_npc(q)\n",
    "    print(f\"Q: {q}\\nA: {resp[:50]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73687f02",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. üõ†Ô∏è C++ Engine Compilation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: C++ Engine Compilation (Optimized)\n",
    "# ============================================================\n",
    "import os, subprocess\n",
    "if os.path.exists('cpp'):\n",
    "    os.makedirs('cpp/build', exist_ok=True)\n",
    "    try:\n",
    "        subprocess.check_call(['cmake', '..'], cwd='cpp/build')\n",
    "        nproc = subprocess.check_output(['nproc']).decode().strip()\n",
    "        subprocess.check_call(['make', f'-j{nproc}'], cwd='cpp/build')\n",
    "        print('‚úÖ Compilation successful!')\n",
    "    except subprocess.CalledProcessError as e: print(f'‚ùå Failed: {e}')\n",
    "else: print('‚ö†Ô∏è cpp/ not found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmarks_header",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. üìà Performance Benchmarking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "benchmarks_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: C++ Engine Benchmarks\n",
    "# ============================================================\n",
    "import os, subprocess\n",
    "\n",
    "if os.path.exists('cpp/build'):\n",
    "    print(\"üöÄ Running C++ Engine Benchmarks...\")\n",
    "    benchmarks = ['bench_engine', 'bench_memory', 'bench_retrieval', 'ablation_suite']\n",
    "    \n",
    "    for bench in benchmarks:\n",
    "        path = f'cpp/build/{bench}'\n",
    "        if os.path.exists(path):\n",
    "            print(f\"\\nüìä Executing {bench}...\")\n",
    "            print('-'*40)\n",
    "            try:\n",
    "                res = subprocess.run([path], capture_output=True, text=True, timeout=300)\n",
    "                print(res.stdout)\n",
    "                if res.stderr: print(f\"‚ö†Ô∏è Stderr: {res.stderr}\")\n",
    "            except subprocess.TimeoutExpired:\n",
    "                print(f\"‚ùå {bench} timed out after 5 minutes.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to run {bench}: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Benchmark binary not found: {path}\")\n",
    "else:\n",
    "    print(\"‚ùå C++ build directory (cpp/build) not found! Please run Cell 10 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. üìä Ablation Study Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Visualize Ablation Results\n",
    "# ============================================================\n",
    "import os, json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "results_path = 'cpp/build/ablation_results.json'\n",
    "if os.path.exists(results_path):\n",
    "    print(f\"üìà Loading ablation results from {results_path}...\")\n",
    "    with open(results_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    records = []\n",
    "    for config, metrics in data.items():\n",
    "        records.append({\n",
    "            'Configuration': config,\n",
    "            'Latency p95 (ms)': metrics.get('latency_p95_ms', 0),\n",
    "            'Throughput (tok/s)': metrics.get('throughput_tok_s', 0),\n",
    "            'Memory (MB)': metrics.get('peak_memory_mb', 0)\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    # display(df) # Commented out for standalone script robustness\n",
    "    print(df)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    df.plot(x='Configuration', y='Latency p95 (ms)', kind='bar', ax=axes[0], color='salmon', legend=False)\n",
    "    axes[0].set_title('95th Percentile Latency (Lower is Better)')\n",
    "    axes[0].set_ylabel('Milliseconds (ms)')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    df.plot(x='Configuration', y='Throughput (tok/s)', kind='bar', ax=axes[1], color='skyblue', legend=False)\n",
    "    axes[1].set_title('Generation Throughput (Higher is Better)')\n",
    "    axes[1].set_ylabel('Tokens per Second')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    df.plot(x='Configuration', y='Memory (MB)', kind='bar', ax=axes[2], color='lightgreen', legend=False)\n",
    "    axes[2].set_title('Peak Memory Usage (Lower is Better)')\n",
    "    axes[2].set_ylabel('Megabytes (MB)')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Ablation results not found at {results_path}. Make sure Cell 11 ran successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}