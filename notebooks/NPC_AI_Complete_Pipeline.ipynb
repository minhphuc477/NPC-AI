{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# \ud83e\udde0 NPC AI \u2014 Complete Training & Integration Pipeline\n",
                "\n",
                "**BD-NSCA: Behavior-Driven Neuro-Symbolic Cognitive Architecture**\n",
                "\n",
                "| Step | Description |\n",
                "|------|-------------|\n",
                "| 1 | Environment Setup |\n",
                "| 2 | Training Data Generation |\n",
                "| 3 | QLoRA Fine-Tuning (checkpoint/resume) |\n",
                "| 4 | GGUF Export |\n",
                "| 5 | Ollama Serving |\n",
                "| 6 | Integrated Demo |\n",
                "| 7 | Quality Evaluation |\n",
                "| 8 | C++ Engine Compilation |\n",
                "\n",
                "> **Checkpoint/Resume**: Training auto-detects and resumes from existing checkpoints.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. \ud83d\udd27 Environment Setup & Dependencies\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 1: Environment Setup\n",
                "# ============================================================\n",
                "import os, sys, subprocess\n",
                "\n",
                "IN_KAGGLE = os.path.exists('/kaggle')\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "ENV_NAME = 'Kaggle' if IN_KAGGLE else ('Colab' if IN_COLAB else 'Local')\n",
                "print(f'\ud83c\udf0d Environment: {ENV_NAME}')\n",
                "\n",
                "if IN_KAGGLE or IN_COLAB:\n",
                "    print('\ud83d\udce6 Installing Unsloth and dependencies...')\n",
                "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n",
                "        'unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git'])\n",
                "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q',\n",
                "        'trl>=0.12.0', 'transformers>=4.45.0', 'datasets', 'accelerate',\n",
                "        'bitsandbytes', 'sentencepiece', 'protobuf'])\n",
                "    print('\ud83d\udce6 Installing Ollama...')\n",
                "    os.system('curl -fsSL https://ollama.com/install.sh | sh')\n",
                "    print('\u2705 All dependencies installed!')\n",
                "else:\n",
                "    print('\u2139\ufe0f  Local env \u2014 assuming deps pre-installed.')\n",
                "\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    gpu_name = torch.cuda.get_device_name(0)\n",
                "    gpu_mem = torch.cuda.get_device_properties(0).total_mem / 1e9\n",
                "    print(f'\ud83c\udfae GPU: {gpu_name} ({gpu_mem:.1f} GB)')\n",
                "    print(f'   BF16: {torch.cuda.is_bf16_supported()}')\n",
                "else:\n",
                "    print('\u26a0\ufe0f  No GPU detected!')\n",
                "print('\u2705 Setup complete!')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. \ud83d\udcca Training Data Generation\n",
                "\n",
                "Generates diverse NPC dialogue data from **24 personas \u00d7 6 emotions \u00d7 6 memory states \u00d7 22 player inputs**.\n",
                "Uses the Phi-3 chat template with `[CONTEXT]` blocks matching the BD-NSCA architecture.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 2: Training Data Generation\n",
                "# ============================================================\n",
                "import json, random, os\n",
                "\n",
                "# --- Load Personas ---\n",
                "PERSONAS_PATH = 'data/personas.json'\n",
                "if os.path.exists(PERSONAS_PATH):\n",
                "    with open(PERSONAS_PATH, 'r', encoding='utf-8') as f:\n",
                "        personas = json.load(f)\n",
                "else:\n",
                "    personas = {\n",
                "        'merchant': {'persona_en': 'You are the Old Merchant, a cunning trader who has traveled across the kingdom.', 'traits': ['shrewd','friendly']},\n",
                "        'gatekeeper': {'persona_en': 'You are the Gatekeeper, an old warrior, former royal guard. You are strict but fair.', 'traits': ['stern','brave','suspicious']},\n",
                "        'healer': {'persona_en': 'You are the Village Healer, a kind middle-aged woman knowledgeable about herbs.', 'traits': ['caring','wise']},\n",
                "    }\n",
                "print(f'\ud83d\udccb {len(personas)} personas loaded')\n",
                "\n",
                "# --- Scenario Building Blocks ---\n",
                "EMOTIONS = [\n",
                "    {'description': 'neutral', 'valence': 0.0},\n",
                "    {'description': 'joyful', 'valence': 0.8},\n",
                "    {'description': 'angry', 'valence': -0.7},\n",
                "    {'description': 'fearful', 'valence': -0.5},\n",
                "    {'description': 'trusting', 'valence': 0.6},\n",
                "    {'description': 'surprised', 'valence': 0.3},\n",
                "]\n",
                "MEMORY_SETS = [\n",
                "    [],\n",
                "    [{'content': 'Player bought a health potion for 50 gold', 'timestamp': '2 days ago', 'importance': 0.6}],\n",
                "    [{'content': 'Player saved the village from bandits', 'timestamp': '1 week ago', 'importance': 0.9}],\n",
                "    [{'content': 'Player lied about their identity', 'timestamp': '3 days ago', 'importance': 0.7},\n",
                "     {'content': 'Player returned a stolen item', 'timestamp': '1 day ago', 'importance': 0.8}],\n",
                "    [{'content': 'Player asked about the forbidden forest', 'timestamp': 'yesterday', 'importance': 0.5}],\n",
                "    [{'content': 'Player completed the herb gathering quest', 'timestamp': '4 days ago', 'importance': 0.7}],\n",
                "]\n",
                "RELATIONSHIPS = [\n",
                "    [],\n",
                "    [{'entity': 'Marcus', 'relation': 'rival', 'trust': -0.3}],\n",
                "    [{'entity': 'Elder', 'relation': 'mentor', 'trust': 0.9}],\n",
                "    [{'entity': 'Player', 'relation': 'acquaintance', 'trust': 0.2}],\n",
                "    [{'entity': 'Player', 'relation': 'trusted_friend', 'trust': 0.8}],\n",
                "]\n",
                "PLAYER_INPUTS = [\n",
                "    'Xin ch\u00e0o! B\u1ea1n kh\u1ecfe kh\u00f4ng?',\n",
                "    \"Lovely weather we're having, isn't it?\",\n",
                "    'Do you trust Marcus?',\n",
                "    'B\u1ea1n c\u00f3 bi\u1ebft g\u00ec v\u1ec1 khu r\u1eebng c\u1ea5m kh\u00f4ng?',\n",
                "    'What happened to the old king?',\n",
                "    \"I'd like to buy some supplies.\",\n",
                "    'Can you heal my wounds?',\n",
                "    'What are you going to do about it?',\n",
                "    'Give me what I want or else!',\n",
                "    'How do you feel now?',\n",
                "    'Do you remember what I bought last time?',\n",
                "    'Tell me about your past.',\n",
                "    \"I heard there's a dragon in the mountains.\",\n",
                "    'The dark mage is threatening the village!',\n",
                "    'T\u00f4i m\u1edbi \u0111\u1ebfn l\u00e0ng n\u00e0y.',\n",
                "    'B\u1ea1n c\u00f3 th\u1ec3 r\u00e8n cho t\u00f4i m\u1ed9t thanh ki\u1ebfm kh\u00f4ng?',\n",
                "    'Ai l\u00e0 ng\u01b0\u1eddi m\u1ea1nh nh\u1ea5t \u1edf \u0111\u00e2y?',\n",
                "    'T\u00f4i kh\u00f4ng tin b\u1ea1n n\u00f3i th\u1eadt.',\n",
                "    'C\u00f3 ph\u1ea3i c\u00f3 kho b\u00e1u \u1ea9n gi\u1ea5u \u1edf \u0111\u00e2y kh\u00f4ng?',\n",
                "    'T\u00f4i mu\u1ed1n tham gia v\u00e0o h\u1ed9i v\u1ec7 binh.',\n",
                "    'B\u1ea1n c\u00f3 nh\u1edb l\u1ea7n cu\u1ed1i ch\u00fang ta g\u1eb7p nhau kh\u00f4ng?',\n",
                "    'T\u00f4i c\u1ea7n gi\u00fap \u0111\u1ee1 v\u1edbi m\u1ed9t nhi\u1ec7m v\u1ee5.',\n",
                "]\n",
                "AMBIENT = [\n",
                "    {},\n",
                "    {'time_of_day': 'morning', 'weather': 'sunny'},\n",
                "    {'time_of_day': 'night', 'weather': 'rainy'},\n",
                "    {'time_of_day': 'evening', 'weather': 'foggy', 'nearby_event': 'festival'},\n",
                "]\n",
                "PLAYER_BEHAVIORS = [\n",
                "    {},\n",
                "    {'stance': 'friendly', 'visit_count': 3},\n",
                "    {'stance': 'aggressive', 'visit_count': 1},\n",
                "    {'stance': 'neutral', 'visit_count': 5},\n",
                "]\n",
                "\n",
                "def generate_training_data(num_samples=1500, seed=42):\n",
                "    random.seed(seed)\n",
                "    samples = []\n",
                "    persona_keys = list(personas.keys())\n",
                "    \n",
                "    # Use simpler string concatenation instead of complex f-strings to avoid quoting issues\n",
                "    sys_tag = \"<|system|>\"\n",
                "    end_tag = \"<|end|>\"\n",
                "    user_tag = \"<|user|>\"\n",
                "    asst_tag = \"<|assistant|>\"\n",
                "\n",
                "    for i in range(num_samples):\n",
                "        pk = random.choice(persona_keys)\n",
                "        p = personas[pk]\n",
                "        emo = random.choice(EMOTIONS)\n",
                "        mems = random.choice(MEMORY_SETS)\n",
                "        rels = random.choice(RELATIONSHIPS)\n",
                "        inp = random.choice(PLAYER_INPUTS)\n",
                "        amb = random.choice(AMBIENT)\n",
                "        pb = random.choice(PLAYER_BEHAVIORS)\n",
                "\n",
                "        use_vi = random.random() < 0.5\n",
                "        persona_text = p.get('persona_vi', p['persona_en']) if use_vi else p['persona_en']\n",
                "\n",
                "        ctx = json.dumps({'memories': mems, 'current_emotion': emo,\n",
                "                          'relationships': rels, 'player_behavior': pb,\n",
                "                          'ambient_awareness': amb})\n",
                "\n",
                "        prompt = (f\"{sys_tag}\\n{persona_text}\\n{end_tag}\\n\"\n",
                "                  f\"{user_tag}\\n[CONTEXT]\\n{ctx}\\n\\n[PLAYER] {inp}{end_tag}\\n\"\n",
                "                  f\"{asst_tag}\\n\")\n",
                "\n",
                "        # Completion is a placeholder \u2014 will be replaced by real LLM outputs\n",
                "        # or teacher model during actual training\n",
                "        completion = f\"[NPC responds in character]{end_tag}\"\n",
                "\n",
                "        samples.append({'prompt': prompt, 'completion': completion})\n",
                "\n",
                "    return samples\n",
                "\n",
                "# --- Generate & Save ---\n",
                "OUTPUT_PATH = 'data/npc_training_v2.jsonl'\n",
                "os.makedirs('data', exist_ok=True)\n",
                "\n",
                "samples = generate_training_data(num_samples=1500)\n",
                "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
                "    for s in samples:\n",
                "        f.write(json.dumps(s, ensure_ascii=False) + '\\n')\n",
                "\n",
                "print(f'\u2705 Generated {len(samples)} training samples -> {OUTPUT_PATH}')\n",
                "\n",
                "# Stats\n",
                "unique_prompts = len(set(s['prompt'] for s in samples))\n",
                "print(f'   Unique prompts: {unique_prompts} / {len(samples)}')\n",
                "print(f'   Sample:\\n{json.dumps(samples[0], indent=2, ensure_ascii=False)[:300]}...')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. \ud83c\udfaf QLoRA Fine-Tuning with Checkpoint/Resume\n",
                "\n",
                "Fine-tunes Phi-3-mini using Unsloth + QLoRA. Key features:\n",
                "- **Auto checkpoint detection**: resumes from last saved checkpoint\n",
                "- **Gradient checkpointing**: fits in 15GB VRAM\n",
                "- **LoRA r=16** on all attention + MLP projections\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 3: QLoRA Fine-Tuning with Checkpoint/Resume\n",
                "# ============================================================\n",
                "from unsloth import FastLanguageModel\n",
                "import torch, os, glob\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from datasets import load_dataset\n",
                "\n",
                "# --- Configuration ---\n",
                "MAX_SEQ_LENGTH = 2048\n",
                "DTYPE = None  # Auto-detect (float16 for T4/V100, bfloat16 for Ampere+)\n",
                "LOAD_IN_4BIT = True\n",
                "MODEL_NAME = \"unsloth/Phi-3-mini-4k-instruct\"\n",
                "OUTPUT_DIR = \"npc_training_output\"\n",
                "DATASET_PATH = \"data/npc_training_v2.jsonl\"\n",
                "if not os.path.exists(DATASET_PATH):\n",
                "    DATASET_PATH = \"data/npc_training.jsonl\"  # Fallback to original\n",
                "\n",
                "# --- Load Model ---\n",
                "print(f\"\ud83d\udce5 Loading model: {MODEL_NAME}\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=MODEL_NAME,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dtype=DTYPE,\n",
                "    load_in_4bit=LOAD_IN_4BIT,\n",
                ")\n",
                "\n",
                "# --- Apply LoRA ---\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=16,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",\n",
                "    random_state=3407,\n",
                "    use_rslora=False,\n",
                "    loftq_config=None,\n",
                ")\n",
                "print(\"\u2705 LoRA applied (r=16, all projections)\")\n",
                "\n",
                "# --- Load Dataset ---\n",
                "print(f\"\ud83d\udcc2 Loading dataset: {DATASET_PATH}\")\n",
                "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
                "print(f\"   {len(dataset)} samples loaded\")\n",
                "\n",
                "# --- Format for SFTTrainer ---\n",
                "def formatting_prompts_func(examples):\n",
                "    texts = []\n",
                "    for p, c in zip(examples[\"prompt\"], examples[\"completion\"]):\n",
                "        texts.append(f\"{p}{c}\")\n",
                "    return {\"text\": texts}\n",
                "\n",
                "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
                "\n",
                "# --- Checkpoint/Resume Detection ---\n",
                "resume_ckpt = None\n",
                "if os.path.isdir(OUTPUT_DIR):\n",
                "    checkpoints = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint-*\")))\n",
                "    if checkpoints:\n",
                "        resume_ckpt = checkpoints[-1]\n",
                "        print(f\"\ud83d\udd04 Resuming from checkpoint: {resume_ckpt}\")\n",
                "    else:\n",
                "        print(\"\ud83d\udcc1 Output dir exists but no checkpoints found, training from scratch.\")\n",
                "else:\n",
                "    print(\"\ud83c\udd95 No previous training found, starting fresh.\")\n",
                "\n",
                "# --- Training ---\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dataset_num_proc=2,\n",
                "    packing=False,\n",
                "    args=SFTConfig(\n",
                "        per_device_train_batch_size=2,\n",
                "        gradient_accumulation_steps=4,\n",
                "        warmup_steps=5,\n",
                "        max_steps=60,\n",
                "        learning_rate=2e-4,\n",
                "        fp16=not torch.cuda.is_bf16_supported(),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        logging_steps=1,\n",
                "        optim=\"adamw_8bit\",\n",
                "        weight_decay=0.01,\n",
                "        lr_scheduler_type=\"linear\",\n",
                "        seed=3407,\n",
                "        output_dir=OUTPUT_DIR,\n",
                "        report_to=\"none\",\n",
                "        save_strategy=\"steps\",\n",
                "        save_steps=20,\n",
                "        save_total_limit=3,\n",
                "    ),\n",
                ")\n",
                "\n",
                "print(\"\ud83d\ude80 Starting training...\")\n",
                "trainer.train(resume_from_checkpoint=resume_ckpt)\n",
                "\n",
                "# --- Save Final Model ---\n",
                "print(f\"\ud83d\udcbe Saving model to {OUTPUT_DIR}\")\n",
                "model.save_pretrained(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "print(\"\u2705 Training complete!\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. \ud83d\udce6 GGUF Export\n",
                "\n",
                "Exports the fine-tuned model to GGUF format for efficient inference via Ollama/llama.cpp.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 4: GGUF Export\n",
                "# ============================================================\n",
                "import glob, os\n",
                "\n",
                "GGUF_OUTPUT = \"model\"  # Unsloth appends _gguf -> \"model_gguf/\"\n",
                "\n",
                "print(\"\ud83d\udce6 Exporting to GGUF (f16)...\")\n",
                "# Note: Unsloth appends \"_gguf\" to the directory name\n",
                "model.save_pretrained_gguf(GGUF_OUTPUT, tokenizer, quantization_method=\"f16\")\n",
                "\n",
                "# --- Find the exported GGUF file ---\n",
                "search_dirs = [\"model_gguf/\", \"model/\", \"model_gguf_gguf/\"]\n",
                "gguf_file = None\n",
                "for d in search_dirs:\n",
                "    matches = glob.glob(os.path.join(d, \"*.gguf\"))\n",
                "    if matches:\n",
                "        gguf_file = matches[0]\n",
                "        break\n",
                "\n",
                "if gguf_file:\n",
                "    size_mb = os.path.getsize(gguf_file) / (1024 * 1024)\n",
                "    print(f\"\u2705 GGUF exported: {gguf_file} ({size_mb:.0f} MB)\")\n",
                "else:\n",
                "    # Last resort: search everywhere\n",
                "    all_gguf = glob.glob(\"**/*.gguf\", recursive=True)\n",
                "    if all_gguf:\n",
                "        gguf_file = all_gguf[0]\n",
                "        print(f\"\u2705 GGUF found at: {gguf_file}\")\n",
                "    else:\n",
                "        print(\"\u274c GGUF file not found! Check export logs above.\")\n",
                "        gguf_file = None\n",
                "\n",
                "trained_model_path = gguf_file\n",
                "print(f\"   trained_model_path = {trained_model_path}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. \ud83d\ude80 Ollama Server Setup & Model Registration\n",
                "\n",
                "Starts the Ollama server and registers the fine-tuned model.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 5: Ollama Serving\n",
                "# ============================================================\n",
                "import subprocess, time, requests, os\n",
                "\n",
                "# --- Start Ollama Server ---\n",
                "print(\"\ud83d\ude80 Starting Ollama server...\")\n",
                "ollama_process = subprocess.Popen(\n",
                "    [\"ollama\", \"serve\"],\n",
                "    stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
                ")\n",
                "time.sleep(5)  # Wait for server startup\n",
                "\n",
                "# --- Health Check ---\n",
                "max_retries = 10\n",
                "for i in range(max_retries):\n",
                "    try:\n",
                "        r = requests.get(\"http://localhost:11434/api/tags\", timeout=3)\n",
                "        if r.status_code == 200:\n",
                "            print(\"\u2705 Ollama server is running!\")\n",
                "            break\n",
                "    except Exception:\n",
                "        pass\n",
                "    print(f\"   Waiting for server... ({i+1}/{max_retries})\")\n",
                "    time.sleep(3)\n",
                "else:\n",
                "    print(\"\u274c Ollama server failed to start!\")\n",
                "\n",
                "# --- Create Modelfile ---\n",
                "if trained_model_path and os.path.exists(trained_model_path):\n",
                "    # Use triple quotes for Modelfile content\n",
                "    modelfile_content = f'''FROM {trained_model_path}\n",
                "PARAMETER temperature 0.7\n",
                "PARAMETER top_p 0.9\n",
                "PARAMETER repeat_penalty 1.1\n",
                "SYSTEM You are an NPC in a fantasy RPG world. Respond in character, considering your memories, emotions, and relationships with the player.\n",
                "'''\n",
                "    with open(\"Modelfile\", \"w\") as f:\n",
                "        f.write(modelfile_content)\n",
                "\n",
                "    print(\"\ud83d\udcdd Modelfile created. Registering model...\")\n",
                "    result = subprocess.run(\n",
                "        [\"ollama\", \"create\", \"npc-ai\", \"-f\", \"Modelfile\"],\n",
                "        capture_output=True, text=True, timeout=300\n",
                "    )\n",
                "    if result.returncode == 0:\n",
                "        print(\"\u2705 Model 'npc-ai' registered with Ollama!\")\n",
                "    else:\n",
                "        print(f\"\u274c Registration failed: {result.stderr}\")\n",
                "else:\n",
                "    print(\"\u26a0\ufe0f  No GGUF file found \u2014 skipping Ollama model registration.\")\n",
                "    print(\"   You can still use the LoRA adapter from the training output.\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. \ud83c\udfae Integrated Demo \u2014 Full Cognitive Pipeline\n",
                "\n",
                "Demonstrates the complete BD-NSCA system:\n",
                "- **Emotional State Machine** (5-axis emotion with decay)\n",
                "- **Conversation Memory** (sliding window + summarization)\n",
                "- **Knowledge Graph** (NPC relationships & world facts)\n",
                "- **Prompt Builder** (V3 format with full context)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 6: Integrated Demo\n",
                "# ============================================================\n",
                "import json, requests, sys, os\n",
                "\n",
                "# Add project root to path for core imports\n",
                "sys.path.insert(0, '.')\n",
                "\n",
                "# --- Initialize Cognitive Components ---\n",
                "try:\n",
                "    from core.emotional_state import EmotionalStateMachine\n",
                "    from core.conversation_memory import ConversationMemory\n",
                "    emotional_sm = EmotionalStateMachine()\n",
                "    conv_memory = ConversationMemory(max_turns=20)\n",
                "    print(\"\u2705 Core cognitive components loaded\")\n",
                "    USE_CORE = True\n",
                "except ImportError as e:\n",
                "    print(f\"\u26a0\ufe0f  Core modules not available ({e})\")\n",
                "    print(\"   Using simplified standalone implementation\")\n",
                "    USE_CORE = False\n",
                "\n",
                "    # Simplified standalone emotion tracker\n",
                "    class SimpleEmotionTracker:\n",
                "        def __init__(self):\n",
                "            self.state = {'joy': 0.0, 'anger': 0.0, 'fear': 0.0, 'trust': 0.5, 'surprise': 0.0}\n",
                "        def update(self, sentiment):\n",
                "            if sentiment > 0:\n",
                "                self.state['joy'] = min(1.0, self.state['joy'] + sentiment * 0.2)\n",
                "                self.state['trust'] = min(1.0, self.state['trust'] + sentiment * 0.1)\n",
                "            else:\n",
                "                self.state['anger'] = min(1.0, self.state['anger'] - sentiment * 0.15)\n",
                "                self.state['trust'] = max(0.0, self.state['trust'] + sentiment * 0.1)\n",
                "            # Decay\n",
                "            for k in self.state:\n",
                "                if k != 'trust':\n",
                "                    self.state[k] *= 0.9\n",
                "        def dominant_emotion(self):\n",
                "            return max(self.state, key=self.state.get)\n",
                "        def summary(self):\n",
                "            return {k: round(v, 2) for k, v in self.state.items()}\n",
                "\n",
                "    emotional_sm = SimpleEmotionTracker()\n",
                "    conv_memory_log = []\n",
                "\n",
                "# --- Knowledge Graph (seed data) ---\n",
                "knowledge_graph = {\n",
                "    'npcs': {\n",
                "        'Merchant': {'location': 'Market Square', 'sells': ['potions', 'weapons', 'armor']},\n",
                "        'Gatekeeper': {'location': 'Main Gate', 'guards': 'entrance'},\n",
                "        'Elder': {'location': 'Village Hall', 'role': 'village leader'},\n",
                "    },\n",
                "    'world_facts': [\n",
                "        'The Dark Forest is forbidden \u2014 cursed by an ancient mage.',\n",
                "        'The old king disappeared 10 years ago.',\n",
                "        'Dragons were last seen in the Northern Mountains.',\n",
                "        'The annual harvest festival is next week.',\n",
                "    ],\n",
                "    'relationships': {\n",
                "        'Merchant-Gatekeeper': 'business_partners',\n",
                "        'Elder-Merchant': 'old_friends',\n",
                "        'Gatekeeper-Elder': 'respectful',\n",
                "    }\n",
                "}\n",
                "print(f\"\ud83d\uddfa\ufe0f Knowledge Graph: {len(knowledge_graph['npcs'])} NPCs, \"\n",
                "      f\"{len(knowledge_graph['world_facts'])} world facts\")\n",
                "\n",
                "# --- Ollama Query Function ---\n",
                "def query_npc(player_input, npc_name=\"Merchant\", memories=None, emotion_state=None):\n",
                "    # Build context\n",
                "    ctx = {\n",
                "        'memories': memories or [],\n",
                "        'current_emotion': emotion_state or {'description': 'neutral', 'valence': 0.0},\n",
                "        'knowledge': knowledge_graph.get('world_facts', [])[:3],\n",
                "        'npc_info': knowledge_graph.get('npcs', {}).get(npc_name, {}),\n",
                "    }\n",
                "    ctx_str = json.dumps(ctx)\n",
                "\n",
                "    full_prompt = f\"[CONTEXT]\\n{ctx_str}\\n\\n[PLAYER] {player_input}\"\n",
                "\n",
                "    try:\n",
                "        response = requests.post(\n",
                "            \"http://localhost:11434/api/generate\",\n",
                "            json={\n",
                "                \"model\": \"npc-ai\",\n",
                "                \"prompt\": full_prompt,\n",
                "                \"stream\": False,\n",
                "                \"options\": {\"temperature\": 0.7, \"top_p\": 0.9}\n",
                "            },\n",
                "            timeout=60\n",
                "        )\n",
                "        if response.status_code == 200:\n",
                "            return response.json().get(\"response\", \"[No response]\")\n",
                "        else:\n",
                "            return f\"[Error {response.status_code}]\"\n",
                "    except Exception as e:\n",
                "        return f\"[Ollama not available: {e}]\"\n",
                "\n",
                "# --- Interactive Demo ---\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"\ud83c\udfae NPC AI INTEGRATED DEMO\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "demo_conversations = [\n",
                "    (\"Hello! I'm new to this village.\", 0.3),\n",
                "    (\"What can you tell me about the forbidden forest?\", 0.0),\n",
                "    (\"I want to buy a health potion.\", 0.2),\n",
                "    (\"Do you trust the Gatekeeper?\", -0.1),\n",
                "    (\"The dark mage sent me to spy on you!\", -0.8),\n",
                "]\n",
                "\n",
                "memories = []\n",
                "for player_input, sentiment in demo_conversations:\n",
                "    print(f\"\\n\ud83d\udc64 Player: {player_input}\")\n",
                "\n",
                "    # Update emotion\n",
                "    if USE_CORE:\n",
                "        emotional_sm.process_interaction({'type': 'dialogue', 'sentiment': sentiment})\n",
                "        emo_state = emotional_sm.get_state_dict()\n",
                "    else:\n",
                "        emotional_sm.update(sentiment)\n",
                "        emo_state = {'description': emotional_sm.dominant_emotion(),\n",
                "                     'valence': sentiment}\n",
                "\n",
                "    # Query NPC\n",
                "    response = query_npc(player_input, memories=memories, emotion_state=emo_state)\n",
                "    print(f\"\ud83e\udd16 NPC: {response}\")\n",
                "\n",
                "    # Store memory\n",
                "    memories.append({\n",
                "        'content': f'Player said: {player_input}',\n",
                "        'timestamp': 'just now',\n",
                "        'importance': abs(sentiment) + 0.3\n",
                "    })\n",
                "    if len(memories) > 10:\n",
                "        memories = memories[-10:]\n",
                "\n",
                "    # Show emotion state\n",
                "    if USE_CORE:\n",
                "        print(f\"   \ud83d\udcad Emotion: {emo_state}\")\n",
                "    else:\n",
                "        print(f\"   \ud83d\udcad Emotion: {emotional_sm.summary()}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"\u2705 Demo complete!\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. \ud83d\udcc8 Quality Evaluation\n",
                "\n",
                "Runs automated quality metrics on NPC responses.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 7: Quality Evaluation\n",
                "# ============================================================\n",
                "import re, math\n",
                "from collections import Counter\n",
                "\n",
                "def evaluate_response_quality(responses):\n",
                "    if not responses:\n",
                "        print(\"\u26a0\ufe0f  No responses to evaluate.\")\n",
                "        return\n",
                "\n",
                "    metrics = {}\n",
                "\n",
                "    # 1. Average Length\n",
                "    lengths = [len(r.split()) for r in responses]\n",
                "    metrics['avg_word_count'] = sum(lengths) / len(lengths)\n",
                "\n",
                "    # 2. Diversity (unique n-gram ratio)\n",
                "    all_bigrams = []\n",
                "    for r in responses:\n",
                "        words = r.lower().split()\n",
                "        all_bigrams.extend(zip(words, words[1:]))\n",
                "    if all_bigrams:\n",
                "        metrics['bigram_diversity'] = len(set(all_bigrams)) / len(all_bigrams)\n",
                "    else:\n",
                "        metrics['bigram_diversity'] = 0.0\n",
                "\n",
                "    # 3. Repetition Score (lower = more repetitive)\n",
                "    unique_responses = len(set(responses))\n",
                "    metrics['uniqueness_ratio'] = unique_responses / len(responses)\n",
                "\n",
                "    # 4. Length Variance (consistent response lengths is unnatural)\n",
                "    if len(lengths) > 1:\n",
                "        mean_len = sum(lengths) / len(lengths)\n",
                "        variance = sum((l - mean_len) ** 2 for l in lengths) / len(lengths)\n",
                "        metrics['length_std'] = math.sqrt(variance)\n",
                "    else:\n",
                "        metrics['length_std'] = 0.0\n",
                "\n",
                "    # 5. Non-empty responses\n",
                "    non_empty = sum(1 for r in responses if len(r.strip()) > 5)\n",
                "    metrics['non_empty_ratio'] = non_empty / len(responses)\n",
                "\n",
                "    # Display\n",
                "    print(\"\ud83d\udcca Quality Metrics:\")\n",
                "    print(f\"   \ud83d\udcdd Avg Word Count:    {metrics['avg_word_count']:.1f}\")\n",
                "    print(f\"   \ud83d\udd24 Bigram Diversity:  {metrics['bigram_diversity']:.3f}\")\n",
                "    print(f\"   \ud83c\udfaf Uniqueness Ratio:  {metrics['uniqueness_ratio']:.3f}\")\n",
                "    print(f\"   \ud83d\udccf Length Std Dev:     {metrics['length_std']:.1f}\")\n",
                "    print(f\"   \u2705 Non-empty Ratio:   {metrics['non_empty_ratio']:.3f}\")\n",
                "\n",
                "    return metrics\n",
                "\n",
                "# Collect responses from demo (or generate test queries)\n",
                "test_responses = []\n",
                "test_inputs = [\n",
                "    \"Hello there!\",\n",
                "    \"What do you sell?\",\n",
                "    \"Tell me about the dark forest.\",\n",
                "    \"Can I trust you?\",\n",
                "    \"I need help with a quest.\",\n",
                "]\n",
                "\n",
                "# Only query if NPC server is running (checking query_npc existence)\n",
                "if 'query_npc' in globals():\n",
                "    print(\"\ud83d\udd0d Generating test responses for evaluation...\")\n",
                "    for inp in test_inputs:\n",
                "        resp = query_npc(inp)\n",
                "        test_responses.append(resp)\n",
                "        print(f\"  Q: {inp}\")\n",
                "        print(f\"  A: {resp[:100]}...\")\n",
                "\n",
                "    print(\"\\n\")\n",
                "    evaluate_response_quality(test_responses)\n",
                "else:\n",
                "    print(\"\u26a0\ufe0f  Skipping evaluation (Ollama server not active)\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. \ud83d\udd27 C++ Engine Compilation (Linux/Kaggle)\n",
                "\n",
                "Builds the native BD-NSCA inference engine from the `cpp/` directory.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 8: C++ Engine Compilation\n",
                "# ============================================================\n",
                "import subprocess, os, shutil\n",
                "\n",
                "CPP_DIR = \"cpp\"\n",
                "\n",
                "if not os.path.isdir(CPP_DIR):\n",
                "    print(\"\u26a0\ufe0f  cpp/ directory not found \u2014 skipping C++ build.\")\n",
                "    print(\"   This is expected if running in a cloud environment without the full repo.\")\n",
                "else:\n",
                "    print(\"\ud83d\udd28 Building C++ BD-NSCA Engine...\")\n",
                "\n",
                "    build_dir = os.path.join(CPP_DIR, \"build\")\n",
                "    os.makedirs(build_dir, exist_ok=True)\n",
                "\n",
                "    # Configure\n",
                "    print(\"   [1/3] CMake Configure...\")\n",
                "    result = subprocess.run(\n",
                "        [\"cmake\", \"..\"],\n",
                "        cwd=build_dir,\n",
                "        capture_output=True, text=True\n",
                "    )\n",
                "    if result.returncode != 0:\n",
                "        print(f\"   \u274c Configure failed:\\n{result.stderr[:500]}\")\n",
                "    else:\n",
                "        print(\"   \u2705 Configure OK\")\n",
                "\n",
                "        # Build\n",
                "        print(\"   [2/3] CMake Build...\")\n",
                "        result = subprocess.run(\n",
                "            [\"cmake\", \"--build\", \".\", \"--config\", \"Release\"],\n",
                "            cwd=build_dir,\n",
                "            capture_output=True, text=True\n",
                "        )\n",
                "        if result.returncode != 0:\n",
                "            print(f\"   \u274c Build failed:\\n{result.stderr[:500]}\")\n",
                "        else:\n",
                "            print(\"   \u2705 Build OK\")\n",
                "\n",
                "            # Run tests\n",
                "            print(\"   [3/3] Running tests...\")\n",
                "            test_exes = [\"test_fix\", \"test_grammar\", \"test_integration\"]\n",
                "            for test in test_exes:\n",
                "                test_path = os.path.join(build_dir, \"Release\", test)\n",
                "                if not os.path.exists(test_path):\n",
                "                    test_path = os.path.join(build_dir, test)\n",
                "                if os.path.exists(test_path):\n",
                "                    r = subprocess.run([test_path], capture_output=True, text=True, timeout=60)\n",
                "                    status = \"\u2705\" if r.returncode == 0 else \"\u274c\"\n",
                "                    print(f\"   {status} {test}: returncode={r.returncode}\")\n",
                "                else:\n",
                "                    print(f\"   \u23ed\ufe0f  {test}: not found\")\n",
                "\n",
                "    print(\"\\n\u2705 C++ build phase complete!\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}