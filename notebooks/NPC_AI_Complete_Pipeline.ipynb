{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üß† NPC AI ‚Äî Complete Training & Integration Pipeline\n",
                "\n",
                "**BD-NSCA: Behavior-Driven Neuro-Symbolic Cognitive Architecture**\n",
                "\n",
                "| Step | Description |\n",
                "|------|-------------|\n",
                "| 1 | Environment Setup |\n",
                "| 2 | Training Data Generation |\n",
                "| 3 | QLoRA Fine-Tuning (checkpoint/resume) |\n",
                "| 4 | GGUF Export |\n",
                "| 5 | Ollama Serving |\n",
                "| 6 | Integrated Demo |\n",
                "| 7 | Quality Evaluation |\n",
                "| 8 | C++ Engine Compilation |\n",
                "\n",
                "> **Checkpoint/Resume**: Training auto-detects and resumes from existing checkpoints.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 1. üîß Environment Setup & Dependencies\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "471d0674",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 1: Environment Setup\n",
                "# ============================================================\n",
                "import os, sys, subprocess, shutil\n",
                "\n",
                "IN_KAGGLE = os.path.exists('/kaggle')\n",
                "IN_COLAB = 'google.colab' in sys.modules\n",
                "ENV_NAME = 'Kaggle' if IN_KAGGLE else ('Colab' if IN_COLAB else 'Local')\n",
                "print(f'üåç Environment: {ENV_NAME}')\n",
                "\n",
                "if IN_KAGGLE or IN_COLAB:\n",
                "    print('üì¶ Installing Unsloth and dependencies...')\n",
                "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git'])\n",
                "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'trl>=0.12.0', 'transformers>=4.45.0', 'datasets', 'accelerate', 'bitsandbytes', 'sentencepiece', 'protobuf'])\n",
                "    print('üì¶ Installing Ollama...')\n",
                "    try:\n",
                "        subprocess.run(['apt-get', 'update'], check=True, capture_output=True)\n",
                "        subprocess.run(['apt-get', 'install', '-y', 'zstd'], check=True, capture_output=True)\n",
                "        os.system('curl -fsSL https://ollama.com/install.sh | sh')\n",
                "        if shutil.which('ollama'): print('‚úÖ Ollama installed successfully!')\n",
                "    except Exception as e: print(f'‚ùå Failed to install Ollama: {e}')\n",
                "else: print('‚ÑπÔ∏è  Local env - assuming deps pre-installed.')\n",
                "\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    print(f'üéÆ GPU: {torch.cuda.get_device_name(0)}')\n",
                "else: print('‚ö†Ô∏è  No GPU detected!')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "97efb198",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. üìù Training Data Generation (Enhanced)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bd29124a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 2: Training Data Generation (Enhanced)\n",
                "# ============================================================\n",
                "import json, random, os\n",
                "PERSONAS_PATH = 'data/personas.json'\n",
                "UTTERANCES_PATH = 'data/player_utterances.json'\n",
                "if os.path.exists(PERSONAS_PATH):\n",
                "    with open(PERSONAS_PATH, 'r', encoding='utf-8') as f: personas = json.load(f)\n",
                "else: personas = {'merchant': {'persona_en': 'You are a Merchant.', 'traits': ['friendly'], 'name': 'Merchant'}}\n",
                "if os.path.exists(UTTERANCES_PATH):\n",
                "    with open(UTTERANCES_PATH, 'r', encoding='utf-8') as f: utterances = json.load(f)\n",
                "else: utterances = {'greetings': {'en': ['Hello!']}}\n",
                "def generate_heuristic_response(persona, category, player_input):\n",
                "    name = persona.get('name', 'NPC'); traits = \", \".join(persona.get('traits', []))\n",
                "    if category == 'greetings': return f\"{name} nods. 'Greetings, traveler. I am but a humble {name.lower()} with {traits} traits.'\"\n",
                "    elif category == 'trade_related': return f\"{name} eyes your gold. 'I have exactly what you need, but the price is firm.'\"\n",
                "    elif category == 'lore_questions': return f\"{name} looks distant. 'Ancient secrets are best left buried, though many whisper of the curse.'\"\n",
                "    return f\"{name} considers your words. 'I have much to think about regarding {player_input[:20]}...'\"\n",
                "def generate_training_data(num_samples=1500, seed=42):\n",
                "    random.seed(seed); samples = []\n",
                "    p_keys = list(personas.keys()); u_cats = list(utterances.keys())\n",
                "    for _ in range(num_samples):\n",
                "        pk = random.choice(p_keys); p = personas[pk]; cat = random.choice(u_cats)\n",
                "        inp = random.choice(utterances[cat].get('en', utterances[cat].get('vi', ['...'])))\n",
                "        ctx = json.dumps({'memories': [], 'current_emotion': {'description': 'neutral', 'valence': 0.0}})\n",
                "        prompt = f\"<|system|>\\n{p['persona_en']}\\n<|end|>\\n<|user|>\\n[CONTEXT]\\n{ctx}\\n\\n[PLAYER] {inp}<|end|>\\n<|assistant|>\\n\"\n",
                "        completion = f\"{generate_heuristic_response(p, cat, inp)}<|end|>\"\n",
                "        samples.append({'prompt': prompt, 'completion': completion})\n",
                "    return samples\n",
                "OUTPUT_PATH = 'data/npc_training_v2.jsonl'\n",
                "os.makedirs('data', exist_ok=True)\n",
                "samples = generate_training_data(num_samples=1500)\n",
                "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
                "    for s in samples: f.write(json.dumps(s, ensure_ascii=False) + '\\n')\n",
                "print(f'‚úÖ Generated {len(samples)} training samples -> {OUTPUT_PATH}')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "dc7265a9",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. üöÄ QLoRA Fine-Tuning\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "write_script_cell",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 3: Write Standalone Training Script\n",
                "# ============================================================\n",
                "import os\n",
                "os.makedirs('scripts', exist_ok=True)\n",
                "\n",
                "script_content = \"\"\"\n",
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from transformers import TrainingArguments\n",
                "from datasets import load_dataset\n",
                "import os\n",
                "import argparse\n",
                "\n",
                "def train(dataset_path, output_dir):\n",
                "    max_seq_length = 2048\n",
                "    dtype = None\n",
                "    load_in_4bit = True\n",
                "\n",
                "    model_name = \"unsloth/Phi-3-mini-4k-instruct\"\n",
                "    \n",
                "    print(f\"üöÄ Loading Unsloth model: {model_name}\")\n",
                "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "        model_name = model_name,\n",
                "        max_seq_length = max_seq_length,\n",
                "        dtype = dtype,\n",
                "        load_in_4bit = load_in_4bit,\n",
                "    )\n",
                "\n",
                "    model = FastLanguageModel.get_peft_model(\n",
                "        model,\n",
                "        r = 16,\n",
                "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
                "        lora_alpha = 16,\n",
                "        lora_dropout = 0,\n",
                "        bias = \"none\",\n",
                "        use_gradient_checkpointing = \"unsloth\",\n",
                "        random_state = 3407,\n",
                "        use_rslora = False,\n",
                "        loftq_config = None,\n",
                "    )\n",
                "\n",
                "    print(f\"üìä Loading dataset: {dataset_path}\")\n",
                "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
                "\n",
                "    def formatting_prompts_func(examples):\n",
                "        prompts = examples[\"prompt\"]\n",
                "        completions = examples[\"completion\"]\n",
                "        texts = [f\"{p}{c}\" for p, c in zip(prompts, completions)]\n",
                "        return { \"text\" : texts, }\n",
                "\n",
                "    dataset = dataset.map(formatting_prompts_func, batched = True)\n",
                "\n",
                "    print(\"üöÑ Starting training...\")\n",
                "    \n",
                "    resume_from_checkpoint = False\n",
                "    if os.path.exists(output_dir) and len(os.listdir(output_dir)) > 0:\n",
                "        print(f\"üîÑ Checkpoints detected in {output_dir}. Resuming...\")\n",
                "        resume_from_checkpoint = True\n",
                "\n",
                "    trainer = SFTTrainer(\n",
                "        model = model,\n",
                "        tokenizer = tokenizer,\n",
                "        train_dataset = dataset,\n",
                "        dataset_text_field = \"text\",\n",
                "        max_seq_length = max_seq_length,\n",
                "        dataset_num_proc = 2,\n",
                "        packing = False,\n",
                "        args = SFTConfig(\n",
                "            per_device_train_batch_size = 2,\n",
                "            gradient_accumulation_steps = 4,\n",
                "            warmup_steps = 5,\n",
                "            max_steps = 60,\n",
                "            learning_rate = 2e-4,\n",
                "            fp16 = not torch.cuda.is_bf16_supported(),\n",
                "            bf16 = torch.cuda.is_bf16_supported(),\n",
                "            logging_steps = 1,\n",
                "            optim = \"adamw_8bit\",\n",
                "            weight_decay = 0.01,\n",
                "            lr_scheduler_type = \"linear\",\n",
                "            seed = 3407,\n",
                "            output_dir = output_dir,\n",
                "            report_to = \"none\",\n",
                "            push_to_hub = False,\n",
                "        ),\n",
                "    )\n",
                "\n",
                "    trainer.train(resume_from_checkpoint = resume_from_checkpoint)\n",
                "    \n",
                "    print(f\"üíæ Saving fine-tuned model to {output_dir}\")\n",
                "    model.save_pretrained(output_dir)\n",
                "    tokenizer.save_pretrained(output_dir)\n",
                "    print(\"‚úÖ Training complete!\")\n",
                "\n",
                "if __name__ == \\\"__main__\\\":\n",
                "    parser = argparse.ArgumentParser(description=\\\"Unsloth Fine-tuning Script\\\")\n",
                "    parser.add_argument(\\\"--dataset\\\", type=str, required=True, help=\\\"Path to the training dataset (.jsonl)\\\")\n",
                "    parser.add_argument(\\\"--output_dir\\\", type=str, default=\\\"outputs/npc_model\\\", help=\\\"Directory to save the model\\\")\n",
                "    \n",
                "    args = parser.parse_args()\n",
                "    train(args.dataset, args.output_dir)\n",
                "\"\"\"\n",
                "\n",
                "with open('scripts/train_unsloth.py', 'w') as f:\n",
                "    f.write(script_content)\n",
                "print('‚úÖ Standalone training script written to scripts/train_unsloth.py')\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7a3d2e1b",
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess, sys, os\n",
                "print('üöÄ Starting fine-tuning...')\n",
                "# Call the standalone script to avoid memory fragmentation in notebooks\n",
                "subprocess.check_call([sys.executable, 'scripts/train_unsloth.py', \n",
                "                    '--dataset', 'data/npc_training_v2.jsonl',\n",
                "                    '--output_dir', 'outputs/npc_model'])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "71391431",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. üì¶ GGUF Export\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b1c2d3e",
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import os\n",
                "model_name = \"outputs/npc_model\"\n",
                "save_path = \"model_gguf\"\n",
                "if os.path.exists(model_name):\n",
                "    model, tokenizer = FastLanguageModel.from_pretrained(model_name, load_in_4bit=True)\n",
                "    print('üì¶ Exporting to F16 GGUF...')\n",
                "    model.save_pretrained_gguf(save_path, tokenizer, quantization_method = \"f16\")\n",
                "    trained_model_path = os.path.join(save_path, \"phi-3-mini-4k-instruct.F16.gguf\")\n",
                "    print(f'‚úÖ GGUF exported: {trained_model_path}')\n",
                "else:\n",
                "    print('‚ö†Ô∏è Trained model not found. Using pre-trained for demo.')\n",
                "    trained_model_path = \"unsloth/Phi-3-mini-4k-instruct-gguf\"\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0d57862b",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. ü§ñ Ollama Serving\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e4d2a1b3",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 5: Ollama Serving\n",
                "# ============================================================\n",
                "import subprocess, time, requests, os, shutil\n",
                "if shutil.which('ollama'):\n",
                "    print(\"üöÄ Starting Ollama server...\")\n",
                "    ollama_process = subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
                "    time.sleep(5)\n",
                "else:\n",
                "    print(\"‚ùå Ollama binary not found!\"); ollama_process = None\n",
                "if ollama_process:\n",
                "    for i in range(12):\n",
                "        try:\n",
                "            if requests.get(\"http://localhost:11434/api/tags\", timeout=3).status_code == 200:\n",
                "                print(\"‚úÖ Ollama server is running!\"); break\n",
                "        except Exception: pass\n",
                "        print(f\"   Waiting for server... ({i+1}/12)\"); time.sleep(5)\n",
                "    if 'trained_model_path' in globals() and trained_model_path and os.path.exists(trained_model_path):\n",
                "        modelfile = f'FROM {trained_model_path}\\nPARAMETER temperature 0.7\\nSYSTEM You are an NPC.'\n",
                "        with open(\"Modelfile\", \"w\") as f: f.write(modelfile)\n",
                "        res = subprocess.run([\"ollama\", \"create\", \"npc-ai\", \"-f\", \"Modelfile\"], capture_output=True, text=True)\n",
                "        if res.returncode == 0: print(\"‚úÖ Model registered!\")\n",
                "        else: print(f\"‚ùå Registration failed: {res.stderr}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c29ce908",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. üéÆ Integrated Demo (Enhanced)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2a3c4b1",
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============================================================\n",
                "# Cell 6: Integrated Demo (Enhanced)\n",
                "# ============================================================\n",
                "import json, requests, sys, os, time\n",
                "def query_npc(player_input, timeout=120):\n",
                "    ctx = {'memories': [], 'current_emotion': {'description': 'neutral', 'valence': 0.0}, 'knowledge': [], 'npc_info': {}}\n",
                "    prompt = f\"[CONTEXT]\\n{json.dumps(ctx)}\\n\\n[PLAYER] {player_input}\"\n",
                "    try:\n",
                "        res = requests.post(\"http://localhost:11434/api/generate\", json={\"model\": \"npc-ai\", \"prompt\": prompt, \"stream\": False, \"options\": {\"temperature\": 0.7}}, timeout=timeout)\n",
                "        if res.status_code == 200: return res.json().get(\"response\", \"[No response]\")\n",
                "        return f\"[Error {res.status_code}]\"\n",
                "    except Exception as e: return f\"[Ollama error: {e}]\"\n",
                "print(\"üîç Warming up model...\")\n",
                "print(f\"   Warmup: {query_npc('Warmup request', timeout=180)[:20]}...\")\n",
                "print(\"\\n\" + \"=\"*60 + \"\\nüéÆ NPC AI INTEGRATED DEMO\\n\" + \"=\"*60)\n",
                "for inp in [\"Hello! I am new here.\", \"What is the curse?\"]:\n",
                "    print(f\"\\nüë§ Player: {inp}\\nü§ñ NPC: {query_npc(inp)}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b289aff3",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. üìä Quality Evaluation\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a1b2c3d4",
            "metadata": {},
            "outputs": [],
            "source": [
                "print('üìä Evaluating responses...')\n",
                "# Simplified evaluation loop\n",
                "test_queries = [\"Hello!\", \"Who are you?\", \"Tell me a story.\"]\n",
                "for q in test_queries:\n",
                "    resp = query_npc(q)\n",
                "    print(f\"Q: {q}\\nA: {resp[:50]}...\\n\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "73687f02",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. üõ†Ô∏è C++ Engine Compilation\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c3d4e5f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "if os.path.exists('cpp'):\n",
                "    print('üõ†Ô∏è Compiling C++ engine...')\n",
                "    os.system('mkdir -p cpp/build && cd cpp/build && cmake .. && make')\n",
                "else:\n",
                "    print('‚ö†Ô∏è cpp/ directory not found ‚Äî skipping C++ build.')\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}