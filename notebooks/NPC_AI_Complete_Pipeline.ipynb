{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† NPC AI ‚Äî Complete Training & Integration Pipeline\n",
        "\n",
        "**BD-NSCA: Behavior-Driven Neuro-Symbolic Cognitive Architecture**\n",
        "\n",
        "| Step | Description |\n",
        "|------|-------------|\n",
        "| 1 | Environment Setup |\n",
        "| 2 | Training Data Generation |\n",
        "| 3 | QLoRA Fine-Tuning (checkpoint/resume) |\n",
        "| 4 | GGUF Export |\n",
        "| 5 | Ollama Serving |\n",
        "| 6 | Integrated Demo |\n",
        "| 7 | Quality Evaluation |\n",
        "| 8 | C++ Engine Compilation |\n",
        "\n",
        "> **Checkpoint/Resume**: Training auto-detects and resumes from existing checkpoints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. üîß Environment Setup & Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "471d0674",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 1: Environment Setup (accelerator-aware: TPU/GPU/CPU)\n",
        "# ============================================================\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_KAGGLE = Path('/kaggle').exists()\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "ENV_NAME = 'Kaggle' if IN_KAGGLE else ('Colab' if IN_COLAB else 'Local')\n",
        "\n",
        "\n",
        "def run_cmd(cmd, allow_fail=False):\n",
        "    try:\n",
        "        subprocess.check_call(cmd)\n",
        "        return True\n",
        "    except Exception as exc:\n",
        "        if not allow_fail:\n",
        "            raise\n",
        "        print(f'Warning: command failed: {cmd}')\n",
        "        print(f'  -> {exc}')\n",
        "        return False\n",
        "\n",
        "\n",
        "def pip_install(packages, allow_fail=False, extra_args=None):\n",
        "    extra_args = extra_args or []\n",
        "    cmd = [sys.executable, '-m', 'pip', 'install', '-q'] + list(extra_args) + list(packages)\n",
        "    return run_cmd(cmd, allow_fail=allow_fail)\n",
        "\n",
        "\n",
        "def detect_runtime():\n",
        "    forced = os.environ.get('NPC_ACCELERATOR', 'auto').strip().lower()\n",
        "    if forced in {'tpu', 'cuda', 'cpu'}:\n",
        "        return forced\n",
        "\n",
        "    if os.environ.get('PJRT_DEVICE', '').strip().upper() == 'TPU':\n",
        "        return 'tpu'\n",
        "\n",
        "    kaggle_accel = os.environ.get('KAGGLE_ACCELERATOR_TYPE', '').strip().upper()\n",
        "    if kaggle_accel.startswith('TPU'):\n",
        "        return 'tpu'\n",
        "\n",
        "    tpu_hints = ('TPU_NAME', 'COLAB_TPU_ADDR', 'TPU_WORKER_ID')\n",
        "    if any(os.environ.get(k) for k in tpu_hints):\n",
        "        try:\n",
        "            import torch_xla.core.xla_model as xm  # type: ignore\n",
        "            _ = xm.xla_device()\n",
        "            return 'tpu'\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm  # type: ignore\n",
        "        dev = str(xm.xla_device()).lower()\n",
        "        if 'xla' in dev:\n",
        "            return 'tpu'\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        import torch\n",
        "        if torch.cuda.is_available():\n",
        "            return 'cuda'\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return 'cpu'\n",
        "\n",
        "\n",
        "TRAIN_ACCELERATOR = detect_runtime()\n",
        "if TRAIN_ACCELERATOR == 'tpu':\n",
        "    os.environ.setdefault('PJRT_DEVICE', 'TPU')\n",
        "\n",
        "print(f'Environment: {ENV_NAME}')\n",
        "print(f'Training accelerator: {TRAIN_ACCELERATOR}')\n",
        "\n",
        "base_deps = [\n",
        "    'transformers>=4.45.0',\n",
        "    'datasets>=2.20.0',\n",
        "    'peft>=0.11.0',\n",
        "    'accelerate>=0.30.0',\n",
        "    'sentencepiece',\n",
        "    'protobuf',\n",
        "    'requests',\n",
        "]\n",
        "pip_install(base_deps, allow_fail=False)\n",
        "\n",
        "if TRAIN_ACCELERATOR == 'cuda':\n",
        "    pip_install(['bitsandbytes>=0.43.0'], allow_fail=True)\n",
        "    # Optional, only for legacy Unsloth path.\n",
        "    pip_install(['unsloth[kaggle-new] @ git+https://github.com/unslothai/unsloth.git'], allow_fail=True)\n",
        "elif TRAIN_ACCELERATOR == 'tpu':\n",
        "    try:\n",
        "        import torch_xla  # type: ignore # noqa: F401\n",
        "        print('torch_xla already available.')\n",
        "    except Exception:\n",
        "        print('torch_xla not found. Attempting install for TPU runtime...')\n",
        "        pip_install(\n",
        "            ['torch_xla[tpu]>=2.2'],\n",
        "            allow_fail=True,\n",
        "            extra_args=['-f', 'https://storage.googleapis.com/libtpu-releases/index.html'],\n",
        "        )\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f'CUDA GPU count: {torch.cuda.device_count()}')\n",
        "        print(f'Primary GPU: {torch.cuda.get_device_name(0)}')\n",
        "except Exception as exc:\n",
        "    print(f'Warning: torch check failed: {exc}')\n",
        "\n",
        "if TRAIN_ACCELERATOR == 'tpu':\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm  # type: ignore\n",
        "        print(f'TPU device: {xm.xla_device()}')\n",
        "    except Exception as exc:\n",
        "        print(f'Warning: TPU selected but torch_xla check failed: {exc}')\n",
        "\n",
        "print('Environment setup done.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cpp_patch_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Step 1.5: Validate C++ Engine Layout (Non-destructive)\n",
        "# ============================================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print('Validating C++ engine files for Kaggle build...')\n",
        "\n",
        "required_files = [\n",
        "    'cpp/CMakeLists.txt',\n",
        "    'cpp/src/NPCInference.cpp',\n",
        "    'cpp/src/PromptBuilder.cpp',\n",
        "    'cpp/include/ModelLoader.h',\n",
        "]\n",
        "missing = [p for p in required_files if not os.path.exists(p)]\n",
        "\n",
        "if missing:\n",
        "    print('Warning: missing required C++ files:')\n",
        "    for m in missing:\n",
        "        print(f'  - {m}')\n",
        "else:\n",
        "    print('All required C++ files are present.')\n",
        "\n",
        "pb_path = Path('cpp/src/PromptBuilder.cpp')\n",
        "if pb_path.exists():\n",
        "    pb_text = pb_path.read_text(encoding='utf-8', errors='ignore')\n",
        "    if 'BuildAdvanced' in pb_text and '[CONTEXT]' in pb_text and '[PLAYER]' in pb_text:\n",
        "        print('PromptBuilder format check passed.')\n",
        "    else:\n",
        "        print('Warning: PromptBuilder format markers were not detected.')\n",
        "else:\n",
        "    print('Warning: PromptBuilder.cpp not found.')\n",
        "\n",
        "print('C++ validation complete (source files left unchanged).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97efb198",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. üìù Training Data Generation (Enhanced)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd29124a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 2: Training Data Generation (Refined English)\n",
        "# ============================================================\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "os.makedirs('data', exist_ok=True)\n",
        "PERSONAS_PATH = 'data/personas.json'\n",
        "UTTERANCES_PATH = 'data/player_utterances.json'\n",
        "OUTPUT_PATH = 'data/npc_training_v2.json'\n",
        "\n",
        "if os.path.exists(PERSONAS_PATH):\n",
        "    with open(PERSONAS_PATH, 'r', encoding='utf-8') as f:\n",
        "        personas = json.load(f)\n",
        "else:\n",
        "    personas = {'merchant': {'persona_en': 'You are a Merchant.', 'traits': ['friendly'], 'id': 'merchant'}}\n",
        "\n",
        "if os.path.exists(UTTERANCES_PATH):\n",
        "    with open(UTTERANCES_PATH, 'r', encoding='utf-8') as f:\n",
        "        utterances = json.load(f)\n",
        "else:\n",
        "    utterances = {'greetings': {'en': ['Hello!']}}\n",
        "\n",
        "\n",
        "def generate_heuristic_response(persona, category, player_input):\n",
        "    name = persona.get('id', 'NPC').replace('npc_', '').capitalize()\n",
        "    traits = persona.get('traits', [])\n",
        "    trait_str = random.choice(traits) if traits else 'friendly'\n",
        "    templates = [\n",
        "        lambda: f\"{name} looks at you with a {trait_str} expression. 'Welcome, traveler. What brings you here?'\",\n",
        "        lambda: f\"'Ah, a new face!' {name} exclaims. 'I hope your journey was smoother than mine.'\",\n",
        "        lambda: f\"{name} pauses for a moment. 'I have much to share, but tell me, what is your business in this village?'\",\n",
        "        lambda: f\"The {name} nods slowly. 'Greetings. I am here to help, if you have the coin.'\",\n",
        "    ]\n",
        "    if category == 'greetings':\n",
        "        return random.choice(templates)()\n",
        "    if category == 'trade_related':\n",
        "        return f\"{name} eyes your gear carefully. 'I deal in quality only. Are you buying or just looking?'\"\n",
        "    return f\"{name} considers your words deeply. 'Interesting... {player_input} is not something I hear every day.'\"\n",
        "\n",
        "\n",
        "if isinstance(personas, dict):\n",
        "    persona_list = list(personas.values())\n",
        "elif isinstance(personas, list):\n",
        "    persona_list = personas\n",
        "else:\n",
        "    persona_list = []\n",
        "\n",
        "if not persona_list:\n",
        "    persona_list = [{'persona_en': 'You are an NPC.', 'traits': ['friendly'], 'id': 'npc_default'}]\n",
        "\n",
        "categories = [\n",
        "    key for key, value in utterances.items()\n",
        "    if isinstance(value, dict) and isinstance(value.get('en', []), list) and value.get('en')\n",
        "]\n",
        "if not categories:\n",
        "    utterances = {'greetings': {'en': ['Hello!']}}\n",
        "    categories = ['greetings']\n",
        "\n",
        "\n",
        "dataset = []\n",
        "for _ in range(1200):\n",
        "    p = random.choice(persona_list)\n",
        "    c = random.choice(categories)\n",
        "    q = random.choice(utterances[c].get('en', ['Hello']))\n",
        "    a = generate_heuristic_response(p, c, q)\n",
        "    ctx = {\n",
        "        'memories': [],\n",
        "        'current_emotion': {'description': 'neutral', 'valence': 0.0},\n",
        "        'knowledge': [],\n",
        "        'npc_info': {'name': p.get('id', 'NPC'), 'persona': p.get('persona_en', '')},\n",
        "    }\n",
        "    prompt = (\n",
        "        '[INSTRUCTION] Respond strictly in English.\\n[CONTEXT]\\n'\n",
        "        + json.dumps(ctx, ensure_ascii=False)\n",
        "        + '\\n\\n[PLAYER] '\n",
        "        + q\n",
        "        + '\\n\\n[NPC] '\n",
        "    )\n",
        "    dataset.append({'prompt': prompt, 'completion': a})\n",
        "\n",
        "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
        "    json.dump(dataset, f, indent=1, ensure_ascii=False)\n",
        "\n",
        "print(f'Generated {len(dataset)} training samples at {OUTPUT_PATH}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "write_train_script",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 3: Trainer selection and dry-run\n",
        "# ============================================================\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "TRAIN_SCRIPT = 'scripts/train_qlora.py'\n",
        "if not os.path.exists(TRAIN_SCRIPT):\n",
        "    raise FileNotFoundError(f'{TRAIN_SCRIPT} not found. Ensure repository files are present.')\n",
        "\n",
        "candidates = [\n",
        "    'data/npc_training_v2.json',\n",
        "    'data/npc_training_v2.jsonl',\n",
        "    'data/npc_training.json',\n",
        "    'data/npc_training.jsonl',\n",
        "]\n",
        "TRAIN_DATASET = next((p for p in candidates if os.path.exists(p)), None)\n",
        "if TRAIN_DATASET is None:\n",
        "    raise FileNotFoundError('No training dataset found in expected paths.')\n",
        "\n",
        "print(f'Using train script: {TRAIN_SCRIPT}')\n",
        "print(f'Using dataset: {TRAIN_DATASET}')\n",
        "print(f'Accelerator target: {globals().get(\"TRAIN_ACCELERATOR\", \"auto\")}')\n",
        "\n",
        "# Dry-run to validate config and dataset before long training.\n",
        "dry_cmd = [\n",
        "    sys.executable,\n",
        "    TRAIN_SCRIPT,\n",
        "    '--data',\n",
        "    TRAIN_DATASET,\n",
        "    '--output-dir',\n",
        "    'outputs/npc_model',\n",
        "    '--accelerator',\n",
        "    globals().get('TRAIN_ACCELERATOR', 'auto'),\n",
        "    '--dry-run',\n",
        "]\n",
        "subprocess.check_call(dry_cmd)\n",
        "print('Dry-run validation completed.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7265a9",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. üöÄ QLoRA Fine-Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a3d2e1b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 4: Execute fine-tuning (TPU/GPU aware) and optional GGUF export\n",
        "# ============================================================\n",
        "import glob\n",
        "import importlib.util\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "WORK_DIR = '/kaggle/working' if os.path.exists('/kaggle/working') else os.getcwd()\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "\n",
        "TRAINING_SUCCESS = False\n",
        "GGUF_EXPORT_SUCCESS = False\n",
        "\n",
        "train_script = 'scripts/train_qlora.py'\n",
        "train_data = globals().get('TRAIN_DATASET', 'data/npc_training_v2.json')\n",
        "accelerator = globals().get('TRAIN_ACCELERATOR', 'auto')\n",
        "output_dir = 'outputs/npc_model'\n",
        "\n",
        "if accelerator == 'tpu':\n",
        "    os.environ.setdefault('PJRT_DEVICE', 'TPU')\n",
        "\n",
        "train_args = [\n",
        "    '--data',\n",
        "    train_data,\n",
        "    '--output-dir',\n",
        "    output_dir,\n",
        "    '--accelerator',\n",
        "    accelerator,\n",
        "    '--epochs',\n",
        "    '1',\n",
        "    '--max-seq-length',\n",
        "    '1024',\n",
        "    '--learning-rate',\n",
        "    '2e-4',\n",
        "    '--gradient-checkpointing',\n",
        "]\n",
        "\n",
        "if accelerator == 'cuda':\n",
        "    train_args += ['--use-4bit', '--batch-size', '2', '--gradient-accumulation-steps', '4']\n",
        "elif accelerator == 'tpu':\n",
        "    # TPU cannot use bitsandbytes 4-bit. Increase grad accumulation for stable global batch.\n",
        "    train_args += ['--no-4bit', '--batch-size', '1', '--gradient-accumulation-steps', '16']\n",
        "else:\n",
        "    train_args += ['--no-4bit', '--batch-size', '1', '--gradient-accumulation-steps', '8']\n",
        "\n",
        "if accelerator == 'tpu' and importlib.util.find_spec('torch_xla.distributed.xla_run') is not None:\n",
        "    tpu_cores = os.environ.get('NPC_TPU_CORES', '8')\n",
        "    train_cmd = [\n",
        "        sys.executable,\n",
        "        '-m',\n",
        "        'torch_xla.distributed.xla_run',\n",
        "        '--num_cores',\n",
        "        str(tpu_cores),\n",
        "        train_script,\n",
        "    ] + train_args\n",
        "else:\n",
        "    if accelerator == 'tpu':\n",
        "        print('torch_xla xla_run launcher not found; using single-process TPU execution.')\n",
        "    train_cmd = [sys.executable, train_script] + train_args\n",
        "\n",
        "print('Running training command:')\n",
        "print(' '.join(train_cmd))\n",
        "\n",
        "try:\n",
        "    subprocess.check_call(train_cmd)\n",
        "    TRAINING_SUCCESS = True\n",
        "    print('Fine-tuning completed.')\n",
        "except Exception as exc:\n",
        "    print(f'Warning: fine-tuning failed: {exc}')\n",
        "\n",
        "if TRAINING_SUCCESS and accelerator != 'tpu' and os.path.exists('scripts/export_gguf.py'):\n",
        "    # Optional export path. On TPU we skip by default to avoid long CPU merge/convert.\n",
        "    gguf_out = os.path.join(WORK_DIR, 'npc-phi3.gguf')\n",
        "    export_cmd = [\n",
        "        sys.executable,\n",
        "        'scripts/export_gguf.py',\n",
        "        '--adapter',\n",
        "        output_dir,\n",
        "        '--base-model',\n",
        "        'microsoft/Phi-3-mini-4k-instruct',\n",
        "        '--output',\n",
        "        gguf_out,\n",
        "        '--merged-dir',\n",
        "        'outputs/merged',\n",
        "    ]\n",
        "    print('Attempting GGUF export...')\n",
        "    try:\n",
        "        subprocess.check_call(export_cmd)\n",
        "        GGUF_EXPORT_SUCCESS = True\n",
        "    except Exception as exc:\n",
        "        print(f'Warning: GGUF export failed: {exc}')\n",
        "\n",
        "# Ensure any generated GGUF is copied to work dir.\n",
        "for src in glob.glob('**/*.gguf', recursive=True):\n",
        "    dst = os.path.join(WORK_DIR, os.path.basename(src))\n",
        "    if os.path.abspath(src) != os.path.abspath(dst) and not os.path.exists(dst):\n",
        "        shutil.copy2(src, dst)\n",
        "\n",
        "if accelerator == 'tpu':\n",
        "    print('TPU training complete. To export GGUF, run export_gguf.py later on a GPU/CPU runtime.')\n",
        "print(f'TRAINING_SUCCESS={TRAINING_SUCCESS}, GGUF_EXPORT_SUCCESS={GGUF_EXPORT_SUCCESS}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71391431",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. üì¶ GGUF Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b1c2d3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 5: GGUF Export Status\n",
        "# ============================================================\n",
        "import glob\n",
        "import os\n",
        "\n",
        "WORK_DIR = '/kaggle/working' if os.path.exists('/kaggle/working') else os.getcwd()\n",
        "\n",
        "all_ggufs = (\n",
        "    glob.glob(os.path.join(WORK_DIR, '*.gguf'))\n",
        "    + glob.glob('/tmp/model_export*.gguf')\n",
        "    + glob.glob('/tmp/model_export*/**/*.gguf')\n",
        "    + glob.glob('/tmp/model_export_gguf/*.gguf')\n",
        "    + glob.glob('outputs/npc_model/*.gguf')\n",
        "    + glob.glob('*.gguf')\n",
        "    + glob.glob('**/*.gguf', recursive=True)\n",
        ")\n",
        "\n",
        "candidates = [\n",
        "    f\n",
        "    for f in all_ggufs\n",
        "    if os.path.exists(f)\n",
        "    and os.path.getsize(f) > 200 * 1024 * 1024\n",
        "    and not any(x in f.lower() for x in ['vocab', 'embedding', 'bge', 'bert'])\n",
        "]\n",
        "\n",
        "if candidates:\n",
        "    trained_model_path = candidates[0]\n",
        "    print(f'GGUF found: {trained_model_path}')\n",
        "else:\n",
        "    trained_model_path = None\n",
        "    print('Warning: GGUF model not found yet. Ollama registration will be skipped.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d57862b",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. ü§ñ Ollama Serving\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d2a1b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 6: Ollama Serving (Safe Register)\n",
        "# ============================================================\n",
        "import glob\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "import requests\n",
        "\n",
        "OLLAMA_READY = False\n",
        "ollama_bin = shutil.which('ollama')\n",
        "\n",
        "if not ollama_bin:\n",
        "    print('Warning: ollama binary not found. Skipping model registration.')\n",
        "else:\n",
        "    print('Starting or checking Ollama server...')\n",
        "    server_ok = False\n",
        "    try:\n",
        "        if requests.get('http://localhost:11434/api/tags', timeout=2).status_code == 200:\n",
        "            server_ok = True\n",
        "            print('Ollama is already running.')\n",
        "    except Exception as exc:\n",
        "        print(f'Warning: initial Ollama health check failed: {exc}')\n",
        "\n",
        "    if not server_ok:\n",
        "        try:\n",
        "            subprocess.Popen([ollama_bin, 'serve'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            time.sleep(5)\n",
        "            server_ok = requests.get('http://localhost:11434/api/tags', timeout=5).status_code == 200\n",
        "        except Exception as exc:\n",
        "            print(f'Warning: failed to start Ollama: {exc}')\n",
        "\n",
        "    tm_path = globals().get('trained_model_path')\n",
        "    if not tm_path or not os.path.exists(tm_path):\n",
        "        all_ggufs = (\n",
        "            glob.glob('model_gguf/*.gguf')\n",
        "            + glob.glob('*.gguf')\n",
        "            + glob.glob('outputs/*.gguf')\n",
        "            + glob.glob('/kaggle/working/*.gguf')\n",
        "        )\n",
        "        candidates = [\n",
        "            f\n",
        "            for f in all_ggufs\n",
        "            if os.path.exists(f)\n",
        "            and os.path.getsize(f) > 200 * 1024 * 1024\n",
        "            and not any(x in f.lower() for x in ['vocab', 'embedding', 'bge', 'bert'])\n",
        "        ]\n",
        "        if candidates:\n",
        "            tm_path = candidates[0]\n",
        "\n",
        "    if server_ok and tm_path and os.path.exists(tm_path):\n",
        "        lines = [\n",
        "            f'FROM {tm_path}',\n",
        "            'PARAMETER temperature 0.7',\n",
        "            'PARAMETER stop \"[PLAYER]\"',\n",
        "            'PARAMETER stop \"[INSTRUCTION]\"',\n",
        "            'PARAMETER stop \"[CONTEXT]\"',\n",
        "            'PARAMETER stop \"<|end|>\"',\n",
        "            'SYSTEM \"You are an NPC. Always respond strictly in English as the [NPC] speaker. Do not repeat the prompt.\"',\n",
        "        ]\n",
        "        with open('Modelfile', 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(lines))\n",
        "\n",
        "        print(f'Registering model npc-ai from: {tm_path}')\n",
        "        result = subprocess.run(\n",
        "            [ollama_bin, 'create', 'npc-ai', '-f', 'Modelfile'],\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "        )\n",
        "        if result.returncode == 0:\n",
        "            OLLAMA_READY = True\n",
        "            print('Ollama model registration succeeded.')\n",
        "        else:\n",
        "            print('Warning: Ollama model registration failed:')\n",
        "            print(result.stderr[-500:])\n",
        "    else:\n",
        "        print('Warning: Ollama server/model prerequisites not satisfied; skipping registration.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29ce908",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. üéÆ Integrated Demo (Enhanced)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2a3c4b1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 7: Integrated Demo (Clean Turns)\n",
        "# ============================================================\n",
        "import json\n",
        "\n",
        "import requests\n",
        "\n",
        "\n",
        "def query_npc(player_input):\n",
        "    if not globals().get('OLLAMA_READY', False):\n",
        "        return '[Ollama model is not ready]'\n",
        "\n",
        "    ctx = {\n",
        "        'memories': [],\n",
        "        'current_emotion': {'description': 'neutral', 'valence': 0.0},\n",
        "        'npc_info': {'name': 'Blacksmith', 'persona': 'A friendly blacksmith.'},\n",
        "    }\n",
        "    prompt = (\n",
        "        '[INSTRUCTION] Respond strictly in English.\\n[CONTEXT]\\n'\n",
        "        + json.dumps(ctx)\n",
        "        + '\\n\\n[PLAYER] '\n",
        "        + player_input\n",
        "        + '\\n\\n[NPC] '\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        payload = {\n",
        "            'model': 'npc-ai',\n",
        "            'prompt': prompt,\n",
        "            'stream': False,\n",
        "            'options': {'stop': ['[PLAYER]', '[INSTRUCTION]', '<|end|>']},\n",
        "        }\n",
        "        res = requests.post('http://localhost:11434/api/generate', json=payload, timeout=60)\n",
        "        if res.status_code == 200:\n",
        "            text = res.json().get('response', '[No response]')\n",
        "            return text.split('[NPC]')[-1].strip()\n",
        "        return f'[Error {res.status_code}]'\n",
        "    except Exception as exc:\n",
        "        return f'[Error: {exc}]'\n",
        "\n",
        "\n",
        "for inp in ['Hello! I am new here.', 'What is the curse?']:\n",
        "    print(f'Player: {inp}\\nNPC: {query_npc(inp)}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b289aff3",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. üìä Quality Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b2c3d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Evaluating responses...')\n",
        "# Simplified evaluation loop\n",
        "if 'query_npc' not in globals():\n",
        "    print('query_npc is unavailable; skipping evaluation.')\n",
        "else:\n",
        "    test_queries = ['Hello!', 'Who are you?', 'Tell me a story.']\n",
        "    for q in test_queries:\n",
        "        resp = query_npc(q)\n",
        "        print(f'Q: {q}\\nA: {resp[:80]}...\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73687f02",
      "metadata": {},
      "source": [
        "---\n",
        "## 8. üõ†Ô∏è C++ Engine Compilation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3d4e5f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 10: C++ Engine Compilation (Optimized)\n",
        "# ============================================================\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "if os.path.exists('cpp'):\n",
        "    os.makedirs('cpp/build', exist_ok=True)\n",
        "    try:\n",
        "        subprocess.check_call(['cmake', '..'], cwd='cpp/build')\n",
        "        jobs = max(1, os.cpu_count() or 1)\n",
        "        subprocess.check_call(['cmake', '--build', '.', f'-j{jobs}'], cwd='cpp/build')\n",
        "        print('Compilation successful.')\n",
        "    except Exception as exc:\n",
        "        print(f'Warning: C++ compilation failed: {exc}')\n",
        "else:\n",
        "    print('Warning: cpp/ not found.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "benchmarks_header",
      "metadata": {},
      "source": [
        "---\n",
        "## 9. üìà Performance Benchmarking\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "benchmarks_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 11: C++ Engine Benchmarks\n",
        "# ============================================================\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "if os.path.exists('cpp/build'):\n",
        "    print('Running C++ engine benchmarks...')\n",
        "    benchmarks = ['bench_engine', 'bench_memory', 'bench_retrieval', 'ablation_suite']\n",
        "\n",
        "    for bench in benchmarks:\n",
        "        path = f'cpp/build/{bench}'\n",
        "        if os.path.exists(path):\n",
        "            print(f'\\nExecuting {bench}...')\n",
        "            print('-' * 40)\n",
        "            try:\n",
        "                res = subprocess.run([path], capture_output=True, text=True, timeout=300)\n",
        "                print(res.stdout)\n",
        "                if res.stderr:\n",
        "                    print(f'Stderr: {res.stderr}')\n",
        "            except subprocess.TimeoutExpired:\n",
        "                print(f'Warning: {bench} timed out after 5 minutes.')\n",
        "            except Exception as exc:\n",
        "                print(f'Warning: failed to run {bench}: {exc}')\n",
        "        else:\n",
        "            print(f'Warning: benchmark binary not found: {path}')\n",
        "else:\n",
        "    print('Warning: cpp/build not found. Run the compilation cell first.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8690d71f",
      "metadata": {},
      "source": [
        "---\n",
        "## 10. üìä Ablation Study Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed68e5d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Cell 12: Visualize Ablation Results\n",
        "# ============================================================\n",
        "import os, json\n",
        "\n",
        "try:\n",
        "    import pandas as pd\n",
        "    import matplotlib.pyplot as plt\n",
        "except Exception as exc:\n",
        "    pd = None\n",
        "    plt = None\n",
        "    print(f'Warning: plotting libraries unavailable: {exc}')\n",
        "\n",
        "results_path = 'cpp/build/ablation_results.json'\n",
        "if pd is None or plt is None:\n",
        "    print('Skipping ablation visualization because pandas/matplotlib are missing.')\n",
        "elif os.path.exists(results_path):\n",
        "    print(f\"üìà Loading ablation results from {results_path}...\")\n",
        "    with open(results_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    records = []\n",
        "    for config, metrics in data.items():\n",
        "        records.append({\n",
        "            'Configuration': config,\n",
        "            'Latency p95 (ms)': metrics.get('latency_p95_ms', 0),\n",
        "            'Throughput (tok/s)': metrics.get('throughput_tok_s', 0),\n",
        "            'Memory (MB)': metrics.get('peak_memory_mb', 0)\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(records)\n",
        "    # display(df) # Commented out for standalone script robustness\n",
        "    print(df)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "    \n",
        "    df.plot(x='Configuration', y='Latency p95 (ms)', kind='bar', ax=axes[0], color='salmon', legend=False)\n",
        "    axes[0].set_title('95th Percentile Latency (Lower is Better)')\n",
        "    axes[0].set_ylabel('Milliseconds (ms)')\n",
        "    axes[0].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    df.plot(x='Configuration', y='Throughput (tok/s)', kind='bar', ax=axes[1], color='skyblue', legend=False)\n",
        "    axes[1].set_title('Generation Throughput (Higher is Better)')\n",
        "    axes[1].set_ylabel('Tokens per Second')\n",
        "    axes[1].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    df.plot(x='Configuration', y='Memory (MB)', kind='bar', ax=axes[2], color='lightgreen', legend=False)\n",
        "    axes[2].set_title('Peak Memory Usage (Lower is Better)')\n",
        "    axes[2].set_ylabel('Megabytes (MB)')\n",
        "    axes[2].tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Ablation results not found at {results_path}. Make sure Cell 11 ran successfully.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c10b6a32",
      "metadata": {},
      "source": [
        "## Proposal Evaluation (Batched)\n",
        "Generate expanded scenarios and run proposal evaluation in batches for Kaggle stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53676ba6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "hf_cache = \"/kaggle/working/hf_cache\" if os.path.exists(\"/kaggle/working\") else os.path.abspath(\"hf_cache\")\n",
        "os.makedirs(hf_cache, exist_ok=True)\n",
        "\n",
        "subprocess.check_call(\n",
        "    [\n",
        "        sys.executable,\n",
        "        \"scripts/generate_proposal_scenarios_large.py\",\n",
        "        \"--variants-per-base\",\n",
        "        \"14\",\n",
        "        \"--output\",\n",
        "        \"data/proposal_eval_scenarios_large.jsonl\",\n",
        "    ]\n",
        ")\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    \"scripts/run_proposal_alignment_eval_batched.py\",\n",
        "    \"--scenarios\",\n",
        "    \"data/proposal_eval_scenarios_large.jsonl\",\n",
        "    \"--batch-size\",\n",
        "    \"28\",\n",
        "    \"--repeats\",\n",
        "    \"1\",\n",
        "    \"--max-tokens\",\n",
        "    \"80\",\n",
        "    \"--temperature\",\n",
        "    \"0.2\",\n",
        "    \"--baseline-models\",\n",
        "    \"phi3:latest\",\n",
        "    \"--bertscore-model-type\",\n",
        "    \"roberta-large\",\n",
        "    \"--bertscore-batch-size\",\n",
        "    \"16\",\n",
        "    \"--bertscore-cache-dir\",\n",
        "    hf_cache,\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.check_call(cmd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e493e77d",
      "metadata": {},
      "source": [
        "## Human Evaluation Pack (Optional)\n",
        "Build blind multi-rater annotation files from the latest proposal run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5cb34e0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "proposal_root = pathlib.Path(\"artifacts/proposal\")\n",
        "run_dirs = sorted([p for p in proposal_root.iterdir() if p.is_dir()]) if proposal_root.exists() else []\n",
        "if not run_dirs:\n",
        "    raise RuntimeError(\"No proposal runs found under artifacts/proposal. Run proposal eval first.\")\n",
        "latest_run = run_dirs[-1]\n",
        "\n",
        "subprocess.check_call(\n",
        "    [\n",
        "        sys.executable,\n",
        "        \"scripts/build_human_eval_pack.py\",\n",
        "        \"--run-dir\",\n",
        "        str(latest_run),\n",
        "        \"--annotators\",\n",
        "        \"annotator_1,annotator_2,annotator_3\",\n",
        "        \"--shared-ratio\",\n",
        "        \"0.35\",\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bed0cb8b",
      "metadata": {},
      "source": [
        "## Publication Benchmark Suite\n",
        "Run non-mock benchmark suite with retrieval security checks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2e7b33",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    \"scripts/run_publication_benchmark_suite.py\",\n",
        "    \"--repeats\",\n",
        "    \"1\",\n",
        "    \"--max-tokens\",\n",
        "    \"64\",\n",
        "    \"--temperature\",\n",
        "    \"0.2\",\n",
        "    \"--run-security-benchmark\",\n",
        "    \"--run-security-spoofed-benchmark\",\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.check_call(cmd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edf9ce70",
      "metadata": {},
      "source": [
        "## Proposal Quality Gate\n",
        "Evaluate whether latest proposal/publication artifacts satisfy the quality bar.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ee50b24",
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    \"scripts/proposal_quality_gate.py\",\n",
        "    \"--proposal-run\",\n",
        "    \"latest\",\n",
        "    \"--publication-run\",\n",
        "    \"latest\",\n",
        "    \"--require-security-benchmark\",\n",
        "]\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.check_call(cmd)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Artifact Checkout (Recommended)\n",
        "Run the complete proposal/publication pipeline and emit a single manifest with all output paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "# Option: set True to skip keyword/random ablation baselines in publication retrieval metrics.\n",
        "SKIP_ABLATION_BASELINES = False\n",
        "\n",
        "cmd = [\n",
        "    sys.executable,\n",
        "    \"scripts/run_kaggle_full_results.py\",\n",
        "    \"--host\",\n",
        "    \"http://127.0.0.1:11434\",\n",
        "]\n",
        "if SKIP_ABLATION_BASELINES:\n",
        "    cmd.append(\"--skip-ablation-baselines\")\n",
        "\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.check_call(cmd)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}