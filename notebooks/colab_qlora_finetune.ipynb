{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eeb96db",
   "metadata": {},
   "source": [
    "This notebook shows a minimal, safe, and runnable QLoRA finetuning workflow for the BD-NSCA dataset (gatekeeper CSV). It is intentionally conservative for smoke tests (short steps, low steps for training). Citations: Dettmers et al., 2023 (QLoRA); Zhang et al., 2020 (BD-NSCA dataset description)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8603fe4d",
   "metadata": {},
   "source": [
    "Commented installs for Colab or local: pip install transformers accelerate bitsandbytes peft datasets\n",
    "(These are commented to avoid running heavy installs during tests.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d14fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads a local CSV and writes train.jsonl using scripts/colab_helpers.convert_csv_to_jsonl\n",
    "from pathlib import Path\n",
    "import json\n",
    "from scripts import colab_helpers\n",
    "root = Path(\"../\")  # notebook parent\n",
    "csv_path = root / \"annotation_pipeline\" / \"data\" / \"gatekeeper_dataset.csv\"\n",
    "out_dir = Path(\"../data\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "if csv_path.exists():\n",
    "    out_path = out_dir / \"train.jsonl\"\n",
    "    colab_helpers.convert_csv_to_jsonl(str(csv_path), str(out_path))\n",
    "    print(\"Wrote\", out_path)\n",
    "else:\n",
    "    print(\"CSV not found; place gatekeeper_dataset.csv at\", csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9be5c17",
   "metadata": {},
   "source": [
    "This cell shows a minimal training loop using PEFT/QLoRA. For smoke tests, use max_steps=60 or 1. Replace with your own compute settings for full runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef551846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mock_train(max_steps=1):\n",
    "    print(f\"Mock QLoRA training for {max_steps} steps (no-op)\")\n",
    "\n",
    "mock_train(max_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c2d701",
   "metadata": {},
   "source": [
    "After training save adapter and optionally export to gguf/ggml for Ollama or local inference. See IMPLEMENTATION_CHECKLIST.md for commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2083ef2",
   "metadata": {},
   "source": [
    "Two Vietnamese prompts from BD-NSCA report:\n",
    "- \"Hãy đóng vai một NPC...\"\n",
    "- \"Sau khi người chơi nói, hãy trả lời...\"\n",
    "Include examples and expected behavior; keep prompts short for tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b832fe",
   "metadata": {},
   "source": [
    "Groq can be used as an alternative \"teacher\" to generate prompts/responses. In Colab set your API key via an environment variable (do not hard-code secrets):\n",
    "\n",
    "```\n",
    "import os\n",
    "os.environ['GROQ_API_KEY'] = 'YOUR_GROQ_API_KEY'  # set this in Colab securely\n",
    "```\n",
    "\n",
    "Example usage (do not execute network calls in tests):\n",
    "\n",
    "```\n",
    "from scripts.colab_helpers import call_groq_api\n",
    "resp = call_groq_api(\"A short NPC prompt\", model_id='llama-3.1-8b', max_tokens=64)\n",
    "print(resp)\n",
    "```\n",
    "\n",
    "Be mindful of rate limits and cost; use `batch_generate_with_groq()` with caching for larger generation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4903cbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# resp = requests.post(\"http://localhost:11434/api/generate\", json={\"model\": \"your-adapter-model\", \"prompt\": \"Xin chào\"})\n",
    "# print(resp.json())\n",
    "print(\"Inference demo placeholder\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
