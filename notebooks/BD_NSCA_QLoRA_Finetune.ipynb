{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# BD-NSCA QLoRA Fine-tuning for NPC Dialogue\n",
                "\n",
                "This notebook fine-tunes Phi-3 Mini for Vietnamese NPC dialogue using QLoRA.\n",
                "\n",
                "## Requirements\n",
                "- Google Colab with GPU (T4 or better)\n",
                "- Training data in JSONL format"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers==4.40.0 peft==0.10.0 bitsandbytes==0.43.0\n",
                "!pip install -q accelerate==0.29.0 trl==0.8.0 datasets==2.18.0"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Upload your training data\n",
                "from google.colab import files\n",
                "uploaded = files.upload()  # Upload train.jsonl"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import json\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments,\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "from trl import SFTTrainer\n",
                "from datasets import Dataset\n",
                "\n",
                "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
                "DATA_FILE = \"train.jsonl\"  # Your uploaded file\n",
                "OUTPUT_DIR = \"./npc-phi3-adapter\"\n",
                "\n",
                "# Training params\n",
                "EPOCHS = 3\n",
                "BATCH_SIZE = 4\n",
                "LEARNING_RATE = 2e-4\n",
                "MAX_SEQ_LENGTH = 1024\n",
                "\n",
                "# LoRA params\n",
                "LORA_R = 16\n",
                "LORA_ALPHA = 32\n",
                "LORA_DROPOUT = 0.05"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset\n",
                "def load_jsonl(path):\n",
                "    samples = []\n",
                "    with open(path, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            if line.strip():\n",
                "                data = json.loads(line)\n",
                "                samples.append({'text': data['prompt'] + data['completion']})\n",
                "    return samples\n",
                "\n",
                "samples = load_jsonl(DATA_FILE)\n",
                "dataset = Dataset.from_list(samples)\n",
                "print(f\"Loaded {len(dataset)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup 4-bit quantization\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16,\n",
                "    bnb_4bit_use_double_quant=False,\n",
                ")\n",
                "\n",
                "# Load model\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    BASE_MODEL,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True,\n",
                ")\n",
                "model.config.use_cache = False\n",
                "\n",
                "# Load tokenizer\n",
                "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "print(\"Model loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare for training\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "# LoRA config\n",
                "peft_config = LoraConfig(\n",
                "    lora_alpha=LORA_ALPHA,\n",
                "    lora_dropout=LORA_DROPOUT,\n",
                "    r=LORA_R,\n",
                "    bias=\"none\",\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                ")\n",
                "\n",
                "# Training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    num_train_epochs=EPOCHS,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    gradient_accumulation_steps=4,\n",
                "    optim=\"paged_adamw_32bit\",\n",
                "    save_steps=100,\n",
                "    logging_steps=10,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    weight_decay=0.001,\n",
                "    fp16=True,\n",
                "    max_grad_norm=0.3,\n",
                "    warmup_ratio=0.03,\n",
                "    group_by_length=True,\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    report_to=\"none\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize trainer\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=peft_config,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    tokenizer=tokenizer,\n",
                "    args=training_args,\n",
                "    packing=False,\n",
                ")\n",
                "\n",
                "# Train!\n",
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save adapter\n",
                "trainer.model.save_pretrained(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "print(f\"Adapter saved to {OUTPUT_DIR}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test inference\n",
                "from peft import PeftModel\n",
                "\n",
                "test_prompt = \"\"\"[SYSTEM]\n",
                "Ban la Than giu cong, mot chien binh gia nghiem khac nhung cong bang.\n",
                "\n",
                "Cot truyen: Nguoi choi den cong lang trong luc lang dang cang thang.\n",
                "\n",
                "Ngu canh hien tai:\n",
                "- Trang thai: Dang canh gac\n",
                "- Tam trang: Nghi ngo\n",
                "[/SYSTEM]\n",
                "\n",
                "[USER]\n",
                "Xin chao, toi muon vao lang.\n",
                "[/USER]\n",
                "\n",
                "[ASSISTANT]\n",
                "\"\"\"\n",
                "\n",
                "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.7)\n",
                "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Download adapter\n",
                "!zip -r npc-phi3-adapter.zip {OUTPUT_DIR}\n",
                "files.download('npc-phi3-adapter.zip')"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}