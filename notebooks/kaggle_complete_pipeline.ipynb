{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "062741d5",
            "metadata": {},
            "source": [
                "# NPC AI: Advanced Neuro-Symbolic Research Pipeline\n",
                "**Author:** Le Tran Minh Phuc\n",
                "\n",
                "This notebook implements the **State-of-the-Art (SOTA) Neuro-Symbolic Architecture** for NPC logic. Unlike basic RAG comparisons, this pipeline implements:\n",
                "1.  **Dynamic Knowledge Graph Construction**: Using LLM-based Open Information Extraction (OIE) to build a graph from raw lore text at runtime.\n",
                "2.  **Fine-Tuning on PersonaChat**: Using the `proj-persona/PersonaChat` dataset for high-fidelity roleplay adaptation.\n",
                "3.  **Advanced RAG**: Recursive Semantic Chunking for optimal context retrieval.\n",
                "4.  **Robust Training**: Includes Checkpoint Resume logic for long runs.\n",
                "\n",
                "### Hardware Requirements:\n",
                "- **Accelerator**: GPU T4 x2 (Turn on in Kaggle Settings)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b494c15e",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SECTION 1: SETUP & INSTALLATION ===\n",
                "# Install Unsloth, Llama-cpp, and Advanced NLP Tools\n",
                "%%capture\n",
                "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
                "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
                "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -q llama-cpp-python\n",
                "!pip install -q sentence-transformers faiss-cpu networkx bert_score huggingface_hub pandas psutil langchain-text-splitters datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1457e090",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SECTION 2: FINE-TUNING ON REAL DATASET (PersonaChat) ===\n",
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "from datasets import load_dataset\n",
                "\n",
                "max_seq_length = 2048\n",
                "dtype = None \n",
                "load_in_4bit = True\n",
                "\n",
                "print(\"Loading Phi-3 Mini...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name = \"unsloth/Phi-3-mini-4k-instruct\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dtype = dtype,\n",
                "    load_in_4bit = load_in_4bit,\n",
                ")\n",
                "\n",
                "print(\"Adding LoRA adapters...\")\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\", \n",
                "    random_state = 3407,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6e5a8848",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.1 Load and Format PersonaChat Dataset\n",
                "# Transforming PersonaChat (Dialogue) into Instruction Format for NPC training\n",
                "dataset = load_dataset(\"proj-persona/PersonaChat\", split=\"train[:5000]\") # subset for speed\n",
                "\n",
                "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
                "\n",
                "### Instruction:\n",
                "You are an NPC with these traits: {}. \\nUser says: \"{}\"\n",
                "\n",
                "### Response:\n",
                "{}\"\"\"\n",
                "\n",
                "def format_persona_chat(examples):\n",
                "    personalities = examples['personality']\n",
                "    histories = examples['history']\n",
                "    candidates = examples['candidates']\n",
                "    \n",
                "    texts = []\n",
                "    for i in range(len(personalities)):\n",
                "        # Extract last turn \n",
                "        user_input = histories[i][-1] if histories[i] else \"Hello\"\n",
                "        npc_response = candidates[i][-1] # Gold response\n",
                "        persona_desc = \"; \".join(personalities[i])\n",
                "        \n",
                "        text = alpaca_prompt.format(persona_desc, user_input, npc_response) + tokenizer.eos_token\n",
                "        texts.append(text)\n",
                "    return { \"text\" : texts }\n",
                "\n",
                "print(\"Formatting PersonaChat Dataset...\")\n",
                "dataset = dataset.map(format_persona_chat, batched = True)\n",
                "print(f\"Training on {len(dataset)} real dialogue examples.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8b48d85a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2.2 Train with Checkpoint & Resume Logic\n",
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "from transformers.trainer_utils import get_last_checkpoint\n",
                "import os\n",
                "\n",
                "output_dir = \"outputs\"\n",
                "\n",
                "# Resume Checkpoint Logic\n",
                "last_checkpoint = None\n",
                "if os.path.isdir(output_dir):\n",
                "    last_checkpoint = get_last_checkpoint(output_dir)\n",
                "    if last_checkpoint:\n",
                "        print(f\"Found checkpoint: {last_checkpoint}. Resuming training...\")\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = dataset,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dataset_num_proc = 2,\n",
                "    packing = False,\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 10,\n",
                "        max_steps = 100, \n",
                "        learning_rate = 2e-4,\n",
                "        fp16 = not torch.cuda.is_bf16_supported(),\n",
                "        bf16 = torch.cuda.is_bf16_supported(),\n",
                "        logging_steps = 1,\n",
                "        optim = \"adamw_8bit\",\n",
                "        weight_decay = 0.01,\n",
                "        lr_scheduler_type = \"linear\",\n",
                "        seed = 3407,\n",
                "        output_dir = output_dir,\n",
                "        # Checkpoint Settings\n",
                "        save_strategy = \"steps\",\n",
                "        save_steps = 20, # Save every 20 steps\n",
                "        save_total_limit = 2, # Keep only last 2 checkpoints\n",
                "    ),\n",
                ")\n",
                "\n",
                "print(\"Starting Fine-Tuning on PersonaChat...\")\n",
                "trainer.train(resume_from_checkpoint=last_checkpoint)\n",
                "\n",
                "# Save trained model\n",
                "model.save_pretrained_gguf(\"model_finetuned\", tokenizer, quantization_method = \"q4_k_m\")\n",
                "trained_model_path = \"model_finetuned/unsloth.Q4_K_M.gguf\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9a641f6d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SECTION 3: ADVANCED NEURO-SYMBOLIC INFERENCE ===\n",
                "# NOTE: We import the pipeline from the repository to ensure reproducibility\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# 1. Setup Repo Access\n",
                "if not os.path.exists(\"NPC-AI\"):\n",
                "    print(\"Cloning repository...\")\n",
                "    !git clone https://github.com/minhphuc477/NPC-AI.git\n",
                "else:\n",
                "    print(\"Repository already exists.\")\n",
                "\n",
                "# 2. Add to Path\n",
                "sys.path.append(os.path.abspath(\"NPC-AI\"))\n",
                "\n",
                "# 3. Import from Core\n",
                "try:\n",
                "    from core.neuro_symbolic_pipeline import NeuroSymbolicPipeline\n",
                "    print(\"Successfully imported NeuroSymbolicPipeline from repo!\")\n",
                "except ImportError as e:\n",
                "    print(f\"Error importing from repo: {e}\")\n",
                "    print(\"Ensure you are running this in a Kaggle environment where the repo is cloned or mounted.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f2135e7a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# === SECTION 4: SCIENTIFIC BENCHMARK ===\n",
                "\n",
                "# Using the sophisticated engine\n",
                "print(\"Initializing Neuro-Symbolic Engine...\")\n",
                "# Use the trained model from previous step\n",
                "engine = NeuroSymbolicPipeline(trained_model_path)\n",
                "\n",
                "configs = {\n",
                "    \"Neuro-Symbolic (Full)\": {\"enable_rag\": True, \"enable_graph\": True},\n",
                "    \"RAG Only\": {\"enable_rag\": True, \"enable_graph\": False},\n",
                "    \"Graph Only\": {\"enable_rag\": False, \"enable_graph\": True}\n",
                "}\n",
                "\n",
                "prompts = [\n",
                "    \"Where can I find finding the Elder Stone?\", \n",
                "    \"Why did Duke Varen betray the King?\",\n",
                "    \"How do I make a healing potion?\"\n",
                "]\n",
                "\n",
                "results = []\n",
                "print(\"Running Experiments...\")\n",
                "\n",
                "for name, cfg in configs.items():\n",
                "    for p in prompts:\n",
                "        res = engine.generate(p, cfg)\n",
                "        results.append({\n",
                "            \"Config\": name,\n",
                "            \"Prompt\": p,\n",
                "            \"Latency\": res['latency_ms'],\n",
                "            \"Throughput\": res['tps'],\n",
                "            \"Response\": res['text'][:50] + \"...\"\n",
                "        })\n",
                "\n",
                "import pandas as pd\n",
                "df = pd.DataFrame(results)\n",
                "print(df.groupby(\"Config\")[[\"Latency\", \"Throughput\"]].mean().to_markdown())\n",
                "df.to_csv(\"final_results.csv\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}