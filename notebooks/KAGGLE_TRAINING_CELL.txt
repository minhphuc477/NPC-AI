from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer, SFTConfig
from transformers import TrainingArguments
from datasets import load_dataset
from transformers.trainer_utils import get_last_checkpoint
import os

# Config
max_seq_length = 2048 
model_name = "unsloth/Phi-3-mini-4k-instruct"

# Load Model
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    dtype = None, 
    load_in_4bit = True,
)

# Add LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, 
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_alpha = 16,
    lora_dropout = 0, 
    bias = "none",    
    use_gradient_checkpointing = "unsloth", 
    random_state = 3407,
)

# Prepare Data
alpaca_prompt = """
### Instruction:
{}

### Response:
{}"""
EOS_TOKEN = tokenizer.eos_token
def formatting_prompts_func(examples):
    texts = []
    for instruction, output in zip(examples["prompt"], examples["completion"]):
        text = alpaca_prompt.format(instruction, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }

data_path = f"{REPO_DIR}/data/npc_training.jsonl"
if os.path.exists(data_path):
    dataset = load_dataset("json", data_files = data_path, split = "train")
    dataset = dataset.map(formatting_prompts_func, batched = True)
    
    # Train
    output_dir = "outputs"
    last_checkpoint = None
    if os.path.exists(output_dir) and os.path.isdir(output_dir):
        last_checkpoint = get_last_checkpoint(output_dir)
        if last_checkpoint:
            print(f"Resuming from checkpoint: {last_checkpoint}")

    trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = dataset,
        dataset_text_field = "text",
        max_seq_length = max_seq_length,
        dataset_num_proc = 2,
        packing = False,
        args = SFTConfig(
            per_device_train_batch_size = 2,  # CRITICAL: Must be >= 2 for proper tensor handling
            gradient_accumulation_steps = 4,
            warmup_steps = 5,
            max_steps = 60, 
            learning_rate = 2e-4,
            fp16 = not torch.cuda.is_bf16_supported(),
            bf16 = torch.cuda.is_bf16_supported(),
            logging_steps = 1,
            optim = "adamw_8bit",
            weight_decay = 0.01,
            lr_scheduler_type = "linear",
            seed = 3407,
            output_dir = output_dir,
            report_to = "none",  # Disable wandb to avoid auth issues in Kaggle
        ),
    )
    trainer.train(resume_from_checkpoint=last_checkpoint)
    
    # Export GGUF
    model.save_pretrained_gguf("model_gguf", tokenizer, quantization_method = "f16")
    
    # Find the actual GGUF file created
    import glob
    gguf_files = glob.glob("model_gguf/*.gguf")
    if not gguf_files:
        raise FileNotFoundError("No GGUF file found after training!")
    trained_model_path = gguf_files[0]
    print(f"âœ“ Found GGUF model: {trained_model_path}")
    print("Training Complete & Model Saved to model_gguf/model-unsloth.f16.gguf")
else:
    print("Data file not found, skipping training.")
